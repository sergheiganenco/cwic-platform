

========================================================================================================================
  CWIC PLATFORM - BACKEND SERVICES CODE
========================================================================================================================

Generated on: 09/06/2025 13:49:11
Backend services only - excluding frontend


========================================================================================================================
  ROOT CONFIGURATION FILES
========================================================================================================================


--------------------------------------------------------------------------------
FILE: docker-compose.yml
--------------------------------------------------------------------------------
name: cwic-platform

services:
  # ---------- FRONTEND ----------
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    command: npm run dev
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      VITE_API_URL: http://localhost:8000
    depends_on:
      api-gateway:
        condition: service_started
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:5173/',r=>{process.exit(r.statusCode>=200 && r.statusCode<500?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------- API GATEWAY ----------
  api-gateway:
    build:
      context: ./backend/api-gateway
      dockerfile: Dockerfile.dev
    command: ["npm","run","dev"]
    ports:
      - "8000:8000"
    environment:
      NODE_ENV: development
      HOST: 0.0.0.0
      PORT: 8000
      DATA_SERVICE_URL: http://data-service:3002   # no trailing /api
      AUTH_SERVICE_URL: http://auth-service:8001
      AI_SERVICE_URL: http://ai-service:3003
      INTEGRATION_SERVICE_URL: http://integration-service:3006
      NOTIFICATION_SERVICE_URL: http://notification-service:3005
      PIPELINE_SERVICE_URL: http://pipeline-service:3004
      CORS_ORIGIN: http://localhost:3000,http://localhost:5173
      JWT_SECRET: ${JWT_SECRET:-devsecret}
      DEBUG: http-proxy-middleware:*
      DEV_BEARER: ${DEV_BEARER}
      # helpful if a host proxy is set
      NO_PROXY: "localhost,127.0.0.1,api-gateway,data-service,ai-service,auth-service,pipeline-service,notification-service,integration-service,db,redis,host.docker.internal"
      no_proxy: "localhost,127.0.0.1,api-gateway,data-service,ai-service,auth-service,pipeline-service,notification-service,integration-service,db,redis,host.docker.internal"

    volumes:
      - ./backend/api-gateway:/app
      - /app/node_modules
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      data-service:
        condition: service_healthy     # wait until healthy
      ai-service:
        condition: service_healthy     # wait until healthy
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:8000/health',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 10s
      timeout: 3s
      start_period: 15s
      retries: 3

  # ---------- AUTH SERVICE ----------
  auth-service:
    build:
      context: ./backend/auth-service
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      NODE_ENV: production
      HOST: 0.0.0.0
      PORT: 8001
      DATABASE_URL: postgresql://${DB_USER:-cwic_user}:${DB_PASSWORD:-cwic_secure_pass}@db:5432/${DB_NAME:-cwic_platform}
      REDIS_URL: redis://redis:6379/0
      JWT_SECRET: devsecret
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:8001/health',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------- DATA SERVICE ----------
  data-service:
    build:
      context: ./backend/data-service
      dockerfile: Dockerfile
    # NOTE: no volume bind here — it runs built JS from the image
    environment:
      NODE_ENV: development
      HOST: 0.0.0.0
      PORT: 3002
      JWT_SECRET: ${JWT_SECRET:-devsecret}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:-change-me}
    env_file:
      - ./backend/data-service/.env.docker
    ports:
      - "3002:3002"
    command: ["node","dist/server.js"]
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:3002/health',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 10s
      timeout: 3s
      start_period: 30s
      retries: 3

  # ---------- AI SERVICE ----------
  ai-service:
    build:
      context: ./backend/ai-service
      dockerfile: Dockerfile.dev
    command: ["npm","run","dev"]
    environment:
      NODE_ENV: development
      HOST: 0.0.0.0          # critical for avoiding ECONNREFUSED
      PORT: 3003
      CORS_ORIGIN: http://localhost:5173,http://localhost:3000
    ports:
      - "3003:3003"
    volumes:
      - ./backend/ai-service:/app
      - /app/node_modules
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:3003/health',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------- PIPELINE SERVICE ----------
  pipeline-service:
    build:
      context: ./backend/pipeline-service
      dockerfile: Dockerfile.dev
    command: ["npm","run","dev"]
    environment:
      NODE_ENV: development
      HOST: 0.0.0.0
      PORT: 3004
      CORS_ORIGIN: http://localhost:5173,http://localhost:3000
    ports:
      - "3004:3004"
    volumes:
      - ./backend/pipeline-service:/app
      - /app/node_modules
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:3004/health',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------- NOTIFICATION SERVICE ----------
  notification-service:
    build:
      context: ./backend/notification-service
      dockerfile: Dockerfile.dev
    command: ["npm","run","dev"]
    environment:
      NODE_ENV: development
      HOST: 0.0.0.0
      PORT: 3005
      CORS_ORIGIN: http://localhost:5173,http://localhost:3000
    ports:
      - "3005:3005"
    volumes:
      - ./backend/notification-service:/app
      - /app/node_modules
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:3005/health',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------- INTEGRATION SERVICE ----------
  integration-service:
    build:
      context: ./backend/integration-service
      dockerfile: Dockerfile.dev
    command: ["npm","run","dev"]
    environment:
      NODE_ENV: development
      HOST: 0.0.0.0
      PORT: 3006
      CORS_ORIGIN: http://localhost:5173,http://localhost:3000
    ports:
      - "3006:3006"
    volumes:
      - ./backend/integration-service:/app
      - /app/node_modules
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:3006/health',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------- POSTGRES ----------
  db:
    image: postgres:15
    ports:
      - "${DB_PORT:-5432}:5432"
    environment:
      POSTGRES_DB: ${DB_NAME:-cwic_platform}
      POSTGRES_USER: ${DB_USER:-cwic_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-cwic_secure_pass}
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-cwic_user} -d ${DB_NAME:-cwic_platform} -h localhost || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - cwic-network
    restart: unless-stopped

  # ---------- REDIS ----------
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redisdata:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - cwic-network
    restart: unless-stopped

  # ---------- OPTIONAL: ELASTICSEARCH ----------
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    profiles: ["search"]
    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      ES_JAVA_OPTS: -Xms512m -Xmx512m
    ports:
      - "9200:9200"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - cwic-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://127.0.0.1:9200/_cluster/health?wait_for_status=yellow&timeout=1s',r=>{process.exit(r.statusCode===200?0:1)}).on('error',()=>process.exit(1))\""
        ]
      interval: 30s
      timeout: 10s
      retries: 10

networks:
  cwic-network:
    driver: bridge

volumes:
  pgdata:
  redisdata:



--------------------------------------------------------------------------------
FILE: .env
--------------------------------------------------------------------------------
JWT_SECRET=devsecret
ENCRYPTION_KEY=dev_encryption_key_please_change
DEV_BEARER=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1MSIsImVtYWlsIjoiZGV2QGN3aWMubG9jYWwiLCJyb2xlIjoiYWRtaW4iLCJwZXJtaXNzaW9ucyI6WyIqIl0sImlhdCI6MTc1NzE1ODU4OSwiZXhwIjoxNzU3NzYzMzg5LCJhdWQiOiJjd2ljLXVpIiwiaXNzIjoiY3dpYyJ9.Gqtm-Ll77DVwUOHQ0GEmTKZ8r2PuB3C5T32w1toU7GQ

# Database configuration
POSTGRES_DB=cwic_platform
POSTGRES_USER=cwic_user
POSTGRES_PASSWORD=cwic_secure_pass
POSTGRES_PORT=5432

# Network configuration
CORS_ORIGIN=http://localhost:5173,http://localhost:3000

# Docker configuration
COMPOSE_BAKE=false


--------------------------------------------------------------------------------
FILE: .env.example
--------------------------------------------------------------------------------
# ==============================================
# CWIC Platform Environment Configuration
# ==============================================

# Application
NODE_ENV=development
APP_NAME=CWIC Platform
APP_VERSION=1.0.0
APP_URL=http://localhost:3000
API_URL=http://localhost:8000

# Database Configuration
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=postgres
DATABASE_PASSWORD=password
DATABASE_NAME=cwic_platform

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# JWT Configuration
JWT_SECRET=your-super-secret-jwt-key-change-in-production
JWT_EXPIRES_IN=24h
JWT_REFRESH_EXPIRES_IN=7d

# AI Service Configuration
OPENAI_API_KEY=your-openai-api-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-azure-openai-key
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4

# External Integrations
SERVICENOW_INSTANCE=your-instance.service-now.com
SERVICENOW_USERNAME=your-username
SERVICENOW_PASSWORD=your-password

JIRA_URL=https://your-company.atlassian.net
JIRA_USERNAME=your-email@company.com
JIRA_API_TOKEN=your-jira-api-token

AZURE_DEVOPS_ORGANIZATION=your-organization
AZURE_DEVOPS_PROJECT=your-project
AZURE_DEVOPS_TOKEN=your-personal-access-token

# GitHub Integration
GITHUB_TOKEN=your-github-token
GITHUB_WEBHOOK_SECRET=your-webhook-secret

# Email Configuration
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your-email@gmail.com
SMTP_PASSWORD=your-app-password

# Monitoring & Logging
LOG_LEVEL=info
ENABLE_REQUEST_LOGGING=true
SENTRY_DSN=your-sentry-dsn

# Security
CORS_ORIGIN=http://localhost:3000
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100

# Storage
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_BUCKET=cwic-storage

# Search
ELASTICSEARCH_HOST=localhost:9200
ELASTICSEARCH_USERNAME=
ELASTICSEARCH_PASSWORD=

# Message Queue
KAFKA_BROKERS=localhost:9092
KAFKA_CLIENT_ID=cwic-platform


--------------------------------------------------------------------------------
FILE: package.json
--------------------------------------------------------------------------------
{
  "name": "cwic-platform",
  "version": "1.0.0",
  "description": "Complete Workflow Intelligence & Control Platform",
  "private": true,
  "workspaces": [
    "frontend",
    "backend/*"
  ],
  "scripts": {
    "dev": "./scripts/development/start-dev.sh",
    "build": "./scripts/development/build.sh",
    "test": "./scripts/development/test.sh",
    "lint": "./scripts/development/lint.sh",
    "setup": "./scripts/setup/init-project.sh",
    "deploy:staging": "./scripts/deployment/deploy-staging.sh",
    "deploy:prod": "./scripts/deployment/deploy-production.sh",
    "docker:up": "docker-compose up -d",
    "docker:down": "docker-compose down",
    "db:migrate": "./scripts/database/migrate.sh",
    "db:seed": "./scripts/database/seed.sh",
    "clean": "./scripts/maintenance/cleanup.sh"
  },
  "keywords": [
    "data-governance",
    "ai-assistant",
    "ci-cd",
    "workflow-automation",
    "data-platform"
  ],
  "author": "Your Company",
  "license": "MIT",
  "devDependencies": {
    "@types/express": "^4.17.23",
    "@types/express-serve-static-core": "^4.19.6",
    "concurrently": "^8.2.0",
    "cross-env": "^7.0.3",
    "cross-env-shell": "^7.0.3"
  }
}



========================================================================================================================
  BACKEND SERVICE: API-GATEWAY
========================================================================================================================


--------------------------------------------------------------------------------
FILE: backend\api-gateway\.env
--------------------------------------------------------------------------------
# backend/api-gateway/.env  (DOCKER COMPOSE)
SERVICE_NAME=api-gateway
PORT=8000
NODE_ENV=development

CORS_ORIGIN=http://localhost:3000,http://localhost:5173,http://localhost:4173

# Internal DNS via compose service names
DATA_SERVICE_URL=http://data-service:3002/api
AI_SERVICE_URL=http://ai-service:3003
AUTH_SERVICE_URL=http://auth-service:3001
PIPELINE_SERVICE_URL=http://pipeline-service:3004
INTEGRATION_SERVICE_URL=http://integration-service:3006
NOTIFICATION_SERVICE_URL=http://notification-service:3007
# Security
DEV_BEARER=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1MSIsImVtYWlsIjoiZGV2QGN3aWMubG9jYWwiLCJyb2xlIjoiYWRtaW4iLCJwZXJtaXNzaW9ucyI6WyIqIl0sImlhdCI6MTc1NzE1ODU4OSwiZXhwIjoxNzU3NzYzMzg5LCJhdWQiOiJjd2ljLXVpIiwiaXNzIjoiY3dpYyJ9.Gqtm-Ll77DVwUOHQ0GEmTKZ8r2PuB3C5T32w1toU7GQ
JWT_SECRET=devsecret
TRUST_PROXY=1
BODY_LIMIT=10mb


--------------------------------------------------------------------------------
FILE: backend\api-gateway\.env.docker
--------------------------------------------------------------------------------
SERVICE_NAME=api-gateway
PORT=8000
TRUST_PROXY=1
JWT_SECRET=devsecret
BODY_LIMIT=10mb

# Docker DNS names (matches compose services)
DATA_SERVICE_URL=http://data-service:3002
AI_SERVICE_URL=http://ai-service:3003
AUTH_SERVICE_URL=http://auth-service:3001
PIPELINE_SERVICE_URL=http://pipeline-service:3004
INTEGRATION_SERVICE_URL=http://integration-service:3006
NOTIFICATION_SERVICE_URL=http://notification-service:3007

# Frontend dev origins
CORS_ORIGIN=http://localhost:3000,http://localhost:5173,http://localhost:4173

# Dev token (optional)
DEV_BEARER=



--------------------------------------------------------------------------------
FILE: backend\api-gateway\Dockerfile
--------------------------------------------------------------------------------
# backend/api-gateway/Dockerfile
FROM node:18-alpine AS deps
WORKDIR /app
COPY package*.json ./
RUN npm ci

FROM node:18-alpine AS build
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY tsconfig.json ./
COPY src ./src
RUN npm run build

FROM node:18-alpine AS runtime
WORKDIR /app
ENV NODE_ENV=production
COPY --from=build /app/dist ./dist
COPY package*.json ./
RUN npm ci --omit=dev && npm cache clean --force
EXPOSE 8000
CMD ["node","dist/server.js"]



--------------------------------------------------------------------------------
FILE: backend\api-gateway\Dockerfile.dev
--------------------------------------------------------------------------------
# ---------- deps ----------
FROM node:20-alpine AS deps
WORKDIR /app
COPY package*.json ./
# Use ci if lockfile exists; otherwise fallback to install
RUN if [ -f package-lock.json ]; then npm ci --no-audit --no-fund; else npm i --no-audit --no-fund; fi

# ---------- dev runtime ----------
FROM node:20-alpine
WORKDIR /app
ENV NODE_ENV=development
# keep node_modules from deps, code will be bind-mounted by compose
COPY --from=deps /app/node_modules ./node_modules
COPY package*.json ./
EXPOSE 8000
CMD ["npm","run","dev"]



--------------------------------------------------------------------------------
FILE: backend\api-gateway\package.json
--------------------------------------------------------------------------------
{
  "name": "cwic-api-gateway",
  "version": "1.0.0",
  "type": "module",
  "main": "dist/server.js",
  "scripts": {
    "dev": "nodemon --watch src --ext ts --exec \"npx tsx src/server.ts\"",
    "build": "tsc",
    "start": "node dist/server.js",
    "typecheck": "tsc --noEmit"
  },
  "engines": {
    "node": ">=18"
  },
  "dependencies": {
    "compression": "^1.8.1",
    "cors": "^2.8.5",
    "dotenv": "^16.4.5",
    "express": "^4.21.2",
    "express-rate-limit": "^8.0.1",
    "helmet": "^7.2.0",
    "http-proxy-middleware": "^3.0.5",
    "jsonwebtoken": "^9.0.2",
    "morgan": "^1.10.1"
  },
  "devDependencies": {
    "@types/express": "^4.17.23",
    "@types/node": "^20.19.11",
    "nodemon": "^3.1.0",
    "tsx": "^4.20.4",
    "typescript": "^5.9.2"
  }
}



--------------------------------------------------------------------------------
FILE: backend\api-gateway\package-lock.json
--------------------------------------------------------------------------------
{
  "name": "cwic-api-gateway",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "cwic-api-gateway",
      "version": "1.0.0",
      "dependencies": {
        "compression": "^1.7.4",
        "cors": "^2.8.5",
        "dotenv": "^16.4.5",
        "express": "^4.19.2",
        "express-rate-limit": "^8.0.1",
        "helmet": "^7.1.0",
        "http-proxy-middleware": "^3.0.5",
        "jsonwebtoken": "^9.0.2",
        "morgan": "^1.10.0"
      },
      "devDependencies": {
        "@types/express": "^4.17.21",
        "@types/node": "^20.12.7",
        "nodemon": "^3.1.0",
        "tsx": "^4.20.4",
        "typescript": "^5.4.5"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-x64": {
      "version": "0.25.9",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@types/body-parser": {
      "version": "1.19.6",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/connect": "*",
        "@types/node": "*"
      }
    },
    "node_modules/@types/connect": {
      "version": "3.4.38",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/@types/express": {
      "version": "4.17.23",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/body-parser": "*",
        "@types/express-serve-static-core": "^4.17.33",
        "@types/qs": "*",
        "@types/serve-static": "*"
      }
    },
    "node_modules/@types/express-serve-static-core": {
      "version": "4.19.6",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/node": "*",
        "@types/qs": "*",
        "@types/range-parser": "*",
        "@types/send": "*"
      }
    },
    "node_modules/@types/http-errors": {
      "version": "2.0.5",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/http-proxy": {
      "version": "1.17.16",
      "license": "MIT",
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/@types/mime": {
      "version": "1.3.5",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/node": {
      "version": "20.19.11",
      "license": "MIT",
      "dependencies": {
        "undici-types": "~6.21.0"
      }
    },
    "node_modules/@types/qs": {
      "version": "6.14.0",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/range-parser": {
      "version": "1.2.7",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/send": {
      "version": "0.17.5",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/mime": "^1",
        "@types/node": "*"
      }
    },
    "node_modules/@types/serve-static": {
      "version": "1.15.8",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/http-errors": "*",
        "@types/node": "*",
        "@types/send": "*"
      }
    },
    "node_modules/accepts": {
      "version": "1.3.8",
      "license": "MIT",
      "dependencies": {
        "mime-types": "~2.1.34",
        "negotiator": "0.6.3"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/accepts/node_modules/negotiator": {
      "version": "0.6.3",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/anymatch": {
      "version": "3.1.3",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "normalize-path": "^3.0.0",
        "picomatch": "^2.0.4"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/array-flatten": {
      "version": "1.1.1",
      "license": "MIT"
    },
    "node_modules/balanced-match": {
      "version": "1.0.2",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/basic-auth": {
      "version": "2.0.1",
      "license": "MIT",
      "dependencies": {
        "safe-buffer": "5.1.2"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/basic-auth/node_modules/safe-buffer": {
      "version": "5.1.2",
      "license": "MIT"
    },
    "node_modules/binary-extensions": {
      "version": "2.3.0",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/body-parser": {
      "version": "1.20.3",
      "license": "MIT",
      "dependencies": {
        "bytes": "3.1.2",
        "content-type": "~1.0.5",
        "debug": "2.6.9",
        "depd": "2.0.0",
        "destroy": "1.2.0",
        "http-errors": "2.0.0",
        "iconv-lite": "0.4.24",
        "on-finished": "2.4.1",
        "qs": "6.13.0",
        "raw-body": "2.5.2",
        "type-is": "~1.6.18",
        "unpipe": "1.0.0"
      },
      "engines": {
        "node": ">= 0.8",
        "npm": "1.2.8000 || >= 1.4.16"
      }
    },
    "node_modules/brace-expansion": {
      "version": "1.1.12",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "balanced-match": "^1.0.0",
        "concat-map": "0.0.1"
      }
    },
    "node_modules/braces": {
      "version": "3.0.3",
      "license": "MIT",
      "dependencies": {
        "fill-range": "^7.1.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/buffer-equal-constant-time": {
      "version": "1.0.1",
      "license": "BSD-3-Clause"
    },
    "node_modules/bytes": {
      "version": "3.1.2",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/call-bind-apply-helpers": {
      "version": "1.0.2",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/call-bound": {
      "version": "1.0.4",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.2",
        "get-intrinsic": "^1.3.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/chokidar": {
      "version": "3.6.0",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "anymatch": "~3.1.2",
        "braces": "~3.0.2",
        "glob-parent": "~5.1.2",
        "is-binary-path": "~2.1.0",
        "is-glob": "~4.0.1",
        "normalize-path": "~3.0.0",
        "readdirp": "~3.6.0"
      },
      "engines": {
        "node": ">= 8.10.0"
      },
      "funding": {
        "url": "https://paulmillr.com/funding/"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.2"
      }
    },
    "node_modules/compressible": {
      "version": "2.0.18",
      "license": "MIT",
      "dependencies": {
        "mime-db": ">= 1.43.0 < 2"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/compression": {
      "version": "1.8.1",
      "license": "MIT",
      "dependencies": {
        "bytes": "3.1.2",
        "compressible": "~2.0.18",
        "debug": "2.6.9",
        "negotiator": "~0.6.4",
        "on-headers": "~1.1.0",
        "safe-buffer": "5.2.1",
        "vary": "~1.1.2"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/concat-map": {
      "version": "0.0.1",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/content-disposition": {
      "version": "0.5.4",
      "license": "MIT",
      "dependencies": {
        "safe-buffer": "5.2.1"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/content-type": {
      "version": "1.0.5",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/cookie": {
      "version": "0.7.1",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/cookie-signature": {
      "version": "1.0.6",
      "license": "MIT"
    },
    "node_modules/cors": {
      "version": "2.8.5",
      "license": "MIT",
      "dependencies": {
        "object-assign": "^4",
        "vary": "^1"
      },
      "engines": {
        "node": ">= 0.10"
      }
    },
    "node_modules/debug": {
      "version": "2.6.9",
      "license": "MIT",
      "dependencies": {
        "ms": "2.0.0"
      }
    },
    "node_modules/depd": {
      "version": "2.0.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/destroy": {
      "version": "1.2.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8",
        "npm": "1.2.8000 || >= 1.4.16"
      }
    },
    "node_modules/dotenv": {
      "version": "16.6.1",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://dotenvx.com"
      }
    },
    "node_modules/dunder-proto": {
      "version": "1.0.1",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.1",
        "es-errors": "^1.3.0",
        "gopd": "^1.2.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/ecdsa-sig-formatter": {
      "version": "1.0.11",
      "license": "Apache-2.0",
      "dependencies": {
        "safe-buffer": "^5.0.1"
      }
    },
    "node_modules/ee-first": {
      "version": "1.1.1",
      "license": "MIT"
    },
    "node_modules/encodeurl": {
      "version": "2.0.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/es-define-property": {
      "version": "1.0.1",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-errors": {
      "version": "1.3.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-object-atoms": {
      "version": "1.1.1",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/esbuild": {
      "version": "0.25.9",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "bin": {
        "esbuild": "bin/esbuild"
      },
      "engines": {
        "node": ">=18"
      },
      "optionalDependencies": {
        "@esbuild/aix-ppc64": "0.25.9",
        "@esbuild/android-arm": "0.25.9",
        "@esbuild/android-arm64": "0.25.9",
        "@esbuild/android-x64": "0.25.9",
        "@esbuild/darwin-arm64": "0.25.9",
        "@esbuild/darwin-x64": "0.25.9",
        "@esbuild/freebsd-arm64": "0.25.9",
        "@esbuild/freebsd-x64": "0.25.9",
        "@esbuild/linux-arm": "0.25.9",
        "@esbuild/linux-arm64": "0.25.9",
        "@esbuild/linux-ia32": "0.25.9",
        "@esbuild/linux-loong64": "0.25.9",
        "@esbuild/linux-mips64el": "0.25.9",
        "@esbuild/linux-ppc64": "0.25.9",
        "@esbuild/linux-riscv64": "0.25.9",
        "@esbuild/linux-s390x": "0.25.9",
        "@esbuild/linux-x64": "0.25.9",
        "@esbuild/netbsd-arm64": "0.25.9",
        "@esbuild/netbsd-x64": "0.25.9",
        "@esbuild/openbsd-arm64": "0.25.9",
        "@esbuild/openbsd-x64": "0.25.9",
        "@esbuild/openharmony-arm64": "0.25.9",
        "@esbuild/sunos-x64": "0.25.9",
        "@esbuild/win32-arm64": "0.25.9",
        "@esbuild/win32-ia32": "0.25.9",
        "@esbuild/win32-x64": "0.25.9"
      }
    },
    "node_modules/escape-html": {
      "version": "1.0.3",
      "license": "MIT"
    },
    "node_modules/etag": {
      "version": "1.8.1",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/eventemitter3": {
      "version": "4.0.7",
      "license": "MIT"
    },
    "node_modules/express": {
      "version": "4.21.2",
      "license": "MIT",
      "dependencies": {
        "accepts": "~1.3.8",
        "array-flatten": "1.1.1",
        "body-parser": "1.20.3",
        "content-disposition": "0.5.4",
        "content-type": "~1.0.4",
        "cookie": "0.7.1",
        "cookie-signature": "1.0.6",
        "debug": "2.6.9",
        "depd": "2.0.0",
        "encodeurl": "~2.0.0",
        "escape-html": "~1.0.3",
        "etag": "~1.8.1",
        "finalhandler": "1.3.1",
        "fresh": "0.5.2",
        "http-errors": "2.0.0",
        "merge-descriptors": "1.0.3",
        "methods": "~1.1.2",
        "on-finished": "2.4.1",
        "parseurl": "~1.3.3",
        "path-to-regexp": "0.1.12",
        "proxy-addr": "~2.0.7",
        "qs": "6.13.0",
        "range-parser": "~1.2.1",
        "safe-buffer": "5.2.1",
        "send": "0.19.0",
        "serve-static": "1.16.2",
        "setprototypeof": "1.2.0",
        "statuses": "2.0.1",
        "type-is": "~1.6.18",
        "utils-merge": "1.0.1",
        "vary": "~1.1.2"
      },
      "engines": {
        "node": ">= 0.10.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/express"
      }
    },
    "node_modules/express-rate-limit": {
      "version": "8.0.1",
      "resolved": "https://registry.npmjs.org/express-rate-limit/-/express-rate-limit-8.0.1.tgz",
      "integrity": "sha512-aZVCnybn7TVmxO4BtlmnvX+nuz8qHW124KKJ8dumsBsmv5ZLxE0pYu7S2nwyRBGHHCAzdmnGyrc5U/rksSPO7Q==",
      "license": "MIT",
      "dependencies": {
        "ip-address": "10.0.1"
      },
      "engines": {
        "node": ">= 16"
      },
      "funding": {
        "url": "https://github.com/sponsors/express-rate-limit"
      },
      "peerDependencies": {
        "express": ">= 4.11"
      }
    },
    "node_modules/fill-range": {
      "version": "7.1.1",
      "license": "MIT",
      "dependencies": {
        "to-regex-range": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/finalhandler": {
      "version": "1.3.1",
      "license": "MIT",
      "dependencies": {
        "debug": "2.6.9",
        "encodeurl": "~2.0.0",
        "escape-html": "~1.0.3",
        "on-finished": "2.4.1",
        "parseurl": "~1.3.3",
        "statuses": "2.0.1",
        "unpipe": "~1.0.0"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/follow-redirects": {
      "version": "1.15.11",
      "funding": [
        {
          "type": "individual",
          "url": "https://github.com/sponsors/RubenVerborgh"
        }
      ],
      "license": "MIT",
      "engines": {
        "node": ">=4.0"
      },
      "peerDependenciesMeta": {
        "debug": {
          "optional": true
        }
      }
    },
    "node_modules/forwarded": {
      "version": "0.2.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/fresh": {
      "version": "0.5.2",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/function-bind": {
      "version": "1.1.2",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-intrinsic": {
      "version": "1.3.0",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.2",
        "es-define-property": "^1.0.1",
        "es-errors": "^1.3.0",
        "es-object-atoms": "^1.1.1",
        "function-bind": "^1.1.2",
        "get-proto": "^1.0.1",
        "gopd": "^1.2.0",
        "has-symbols": "^1.1.0",
        "hasown": "^2.0.2",
        "math-intrinsics": "^1.1.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-proto": {
      "version": "1.0.1",
      "license": "MIT",
      "dependencies": {
        "dunder-proto": "^1.0.1",
        "es-object-atoms": "^1.0.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/get-tsconfig": {
      "version": "4.10.1",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "resolve-pkg-maps": "^1.0.0"
      },
      "funding": {
        "url": "https://github.com/privatenumber/get-tsconfig?sponsor=1"
      }
    },
    "node_modules/glob-parent": {
      "version": "5.1.2",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "is-glob": "^4.0.1"
      },
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/gopd": {
      "version": "1.2.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-flag": {
      "version": "3.0.0",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/has-symbols": {
      "version": "1.1.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/hasown": {
      "version": "2.0.2",
      "license": "MIT",
      "dependencies": {
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/helmet": {
      "version": "7.2.0",
      "license": "MIT",
      "engines": {
        "node": ">=16.0.0"
      }
    },
    "node_modules/http-errors": {
      "version": "2.0.0",
      "license": "MIT",
      "dependencies": {
        "depd": "2.0.0",
        "inherits": "2.0.4",
        "setprototypeof": "1.2.0",
        "statuses": "2.0.1",
        "toidentifier": "1.0.1"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/http-proxy": {
      "version": "1.18.1",
      "license": "MIT",
      "dependencies": {
        "eventemitter3": "^4.0.0",
        "follow-redirects": "^1.0.0",
        "requires-port": "^1.0.0"
      },
      "engines": {
        "node": ">=8.0.0"
      }
    },
    "node_modules/http-proxy-middleware": {
      "version": "3.0.5",
      "license": "MIT",
      "dependencies": {
        "@types/http-proxy": "^1.17.15",
        "debug": "^4.3.6",
        "http-proxy": "^1.18.1",
        "is-glob": "^4.0.3",
        "is-plain-object": "^5.0.0",
        "micromatch": "^4.0.8"
      },
      "engines": {
        "node": "^14.15.0 || ^16.10.0 || >=18.0.0"
      }
    },
    "node_modules/http-proxy-middleware/node_modules/debug": {
      "version": "4.4.1",
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/http-proxy-middleware/node_modules/ms": {
      "version": "2.1.3",
      "license": "MIT"
    },
    "node_modules/iconv-lite": {
      "version": "0.4.24",
      "license": "MIT",
      "dependencies": {
        "safer-buffer": ">= 2.1.2 < 3"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/ignore-by-default": {
      "version": "1.0.1",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/inherits": {
      "version": "2.0.4",
      "license": "ISC"
    },
    "node_modules/ip-address": {
      "version": "10.0.1",
      "resolved": "https://registry.npmjs.org/ip-address/-/ip-address-10.0.1.tgz",
      "integrity": "sha512-NWv9YLW4PoW2B7xtzaS3NCot75m6nK7Icdv0o3lfMceJVRfSoQwqD4wEH5rLwoKJwUiZ/rfpiVBhnaF0FK4HoA==",
      "license": "MIT",
      "engines": {
        "node": ">= 12"
      }
    },
    "node_modules/ipaddr.js": {
      "version": "1.9.1",
      "license": "MIT",
      "engines": {
        "node": ">= 0.10"
      }
    },
    "node_modules/is-binary-path": {
      "version": "2.1.0",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "binary-extensions": "^2.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/is-extglob": {
      "version": "2.1.1",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-glob": {
      "version": "4.0.3",
      "license": "MIT",
      "dependencies": {
        "is-extglob": "^2.1.1"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-number": {
      "version": "7.0.0",
      "license": "MIT",
      "engines": {
        "node": ">=0.12.0"
      }
    },
    "node_modules/is-plain-object": {
      "version": "5.0.0",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/jsonwebtoken": {
      "version": "9.0.2",
      "license": "MIT",
      "dependencies": {
        "jws": "^3.2.2",
        "lodash.includes": "^4.3.0",
        "lodash.isboolean": "^3.0.3",
        "lodash.isinteger": "^4.0.4",
        "lodash.isnumber": "^3.0.3",
        "lodash.isplainobject": "^4.0.6",
        "lodash.isstring": "^4.0.1",
        "lodash.once": "^4.0.0",
        "ms": "^2.1.1",
        "semver": "^7.5.4"
      },
      "engines": {
        "node": ">=12",
        "npm": ">=6"
      }
    },
    "node_modules/jsonwebtoken/node_modules/ms": {
      "version": "2.1.3",
      "license": "MIT"
    },
    "node_modules/jwa": {
      "version": "1.4.2",
      "license": "MIT",
      "dependencies": {
        "buffer-equal-constant-time": "^1.0.1",
        "ecdsa-sig-formatter": "1.0.11",
        "safe-buffer": "^5.0.1"
      }
    },
    "node_modules/jws": {
      "version": "3.2.2",
      "license": "MIT",
      "dependencies": {
        "jwa": "^1.4.1",
        "safe-buffer": "^5.0.1"
      }
    },
    "node_modules/lodash.includes": {
      "version": "4.3.0",
      "license": "MIT"
    },
    "node_modules/lodash.isboolean": {
      "version": "3.0.3",
      "license": "MIT"
    },
    "node_modules/lodash.isinteger": {
      "version": "4.0.4",
      "license": "MIT"
    },
    "node_modules/lodash.isnumber": {
      "version": "3.0.3",
      "license": "MIT"
    },
    "node_modules/lodash.isplainobject": {
      "version": "4.0.6",
      "license": "MIT"
    },
    "node_modules/lodash.isstring": {
      "version": "4.0.1",
      "license": "MIT"
    },
    "node_modules/lodash.once": {
      "version": "4.1.1",
      "license": "MIT"
    },
    "node_modules/math-intrinsics": {
      "version": "1.1.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/media-typer": {
      "version": "0.3.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/merge-descriptors": {
      "version": "1.0.3",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/methods": {
      "version": "1.1.2",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/micromatch": {
      "version": "4.0.8",
      "license": "MIT",
      "dependencies": {
        "braces": "^3.0.3",
        "picomatch": "^2.3.1"
      },
      "engines": {
        "node": ">=8.6"
      }
    },
    "node_modules/mime": {
      "version": "1.6.0",
      "license": "MIT",
      "bin": {
        "mime": "cli.js"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/mime-db": {
      "version": "1.54.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/mime-types": {
      "version": "2.1.35",
      "license": "MIT",
      "dependencies": {
        "mime-db": "1.52.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/mime-types/node_modules/mime-db": {
      "version": "1.52.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/minimatch": {
      "version": "3.1.2",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "brace-expansion": "^1.1.7"
      },
      "engines": {
        "node": "*"
      }
    },
    "node_modules/morgan": {
      "version": "1.10.1",
      "license": "MIT",
      "dependencies": {
        "basic-auth": "~2.0.1",
        "debug": "2.6.9",
        "depd": "~2.0.0",
        "on-finished": "~2.3.0",
        "on-headers": "~1.1.0"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/morgan/node_modules/on-finished": {
      "version": "2.3.0",
      "license": "MIT",
      "dependencies": {
        "ee-first": "1.1.1"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/ms": {
      "version": "2.0.0",
      "license": "MIT"
    },
    "node_modules/negotiator": {
      "version": "0.6.4",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/nodemon": {
      "version": "3.1.10",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "chokidar": "^3.5.2",
        "debug": "^4",
        "ignore-by-default": "^1.0.1",
        "minimatch": "^3.1.2",
        "pstree.remy": "^1.1.8",
        "semver": "^7.5.3",
        "simple-update-notifier": "^2.0.0",
        "supports-color": "^5.5.0",
        "touch": "^3.1.0",
        "undefsafe": "^2.0.5"
      },
      "bin": {
        "nodemon": "bin/nodemon.js"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/nodemon"
      }
    },
    "node_modules/nodemon/node_modules/debug": {
      "version": "4.4.1",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/nodemon/node_modules/ms": {
      "version": "2.1.3",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/normalize-path": {
      "version": "3.0.0",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/object-assign": {
      "version": "4.1.1",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/object-inspect": {
      "version": "1.13.4",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/on-finished": {
      "version": "2.4.1",
      "license": "MIT",
      "dependencies": {
        "ee-first": "1.1.1"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/on-headers": {
      "version": "1.1.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/parseurl": {
      "version": "1.3.3",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/path-to-regexp": {
      "version": "0.1.12",
      "license": "MIT"
    },
    "node_modules/picomatch": {
      "version": "2.3.1",
      "license": "MIT",
      "engines": {
        "node": ">=8.6"
      },
      "funding": {
        "url": "https://github.com/sponsors/jonschlinkert"
      }
    },
    "node_modules/proxy-addr": {
      "version": "2.0.7",
      "license": "MIT",
      "dependencies": {
        "forwarded": "0.2.0",
        "ipaddr.js": "1.9.1"
      },
      "engines": {
        "node": ">= 0.10"
      }
    },
    "node_modules/pstree.remy": {
      "version": "1.1.8",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/qs": {
      "version": "6.13.0",
      "license": "BSD-3-Clause",
      "dependencies": {
        "side-channel": "^1.0.6"
      },
      "engines": {
        "node": ">=0.6"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/range-parser": {
      "version": "1.2.1",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/raw-body": {
      "version": "2.5.2",
      "license": "MIT",
      "dependencies": {
        "bytes": "3.1.2",
        "http-errors": "2.0.0",
        "iconv-lite": "0.4.24",
        "unpipe": "1.0.0"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/readdirp": {
      "version": "3.6.0",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "picomatch": "^2.2.1"
      },
      "engines": {
        "node": ">=8.10.0"
      }
    },
    "node_modules/requires-port": {
      "version": "1.0.0",
      "license": "MIT"
    },
    "node_modules/resolve-pkg-maps": {
      "version": "1.0.0",
      "dev": true,
      "license": "MIT",
      "funding": {
        "url": "https://github.com/privatenumber/resolve-pkg-maps?sponsor=1"
      }
    },
    "node_modules/safe-buffer": {
      "version": "5.2.1",
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/feross"
        },
        {
          "type": "patreon",
          "url": "https://www.patreon.com/feross"
        },
        {
          "type": "consulting",
          "url": "https://feross.org/support"
        }
      ],
      "license": "MIT"
    },
    "node_modules/safer-buffer": {
      "version": "2.1.2",
      "license": "MIT"
    },
    "node_modules/semver": {
      "version": "7.7.2",
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/send": {
      "version": "0.19.0",
      "license": "MIT",
      "dependencies": {
        "debug": "2.6.9",
        "depd": "2.0.0",
        "destroy": "1.2.0",
        "encodeurl": "~1.0.2",
        "escape-html": "~1.0.3",
        "etag": "~1.8.1",
        "fresh": "0.5.2",
        "http-errors": "2.0.0",
        "mime": "1.6.0",
        "ms": "2.1.3",
        "on-finished": "2.4.1",
        "range-parser": "~1.2.1",
        "statuses": "2.0.1"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/send/node_modules/encodeurl": {
      "version": "1.0.2",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/send/node_modules/ms": {
      "version": "2.1.3",
      "license": "MIT"
    },
    "node_modules/serve-static": {
      "version": "1.16.2",
      "license": "MIT",
      "dependencies": {
        "encodeurl": "~2.0.0",
        "escape-html": "~1.0.3",
        "parseurl": "~1.3.3",
        "send": "0.19.0"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/setprototypeof": {
      "version": "1.2.0",
      "license": "ISC"
    },
    "node_modules/side-channel": {
      "version": "1.1.0",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "object-inspect": "^1.13.3",
        "side-channel-list": "^1.0.0",
        "side-channel-map": "^1.0.1",
        "side-channel-weakmap": "^1.0.2"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/side-channel-list": {
      "version": "1.0.0",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "object-inspect": "^1.13.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/side-channel-map": {
      "version": "1.0.1",
      "license": "MIT",
      "dependencies": {
        "call-bound": "^1.0.2",
        "es-errors": "^1.3.0",
        "get-intrinsic": "^1.2.5",
        "object-inspect": "^1.13.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/side-channel-weakmap": {
      "version": "1.0.2",
      "license": "MIT",
      "dependencies": {
        "call-bound": "^1.0.2",
        "es-errors": "^1.3.0",
        "get-intrinsic": "^1.2.5",
        "object-inspect": "^1.13.3",
        "side-channel-map": "^1.0.1"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/simple-update-notifier": {
      "version": "2.0.0",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "semver": "^7.5.3"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/statuses": {
      "version": "2.0.1",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/supports-color": {
      "version": "5.5.0",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "has-flag": "^3.0.0"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/to-regex-range": {
      "version": "5.0.1",
      "license": "MIT",
      "dependencies": {
        "is-number": "^7.0.0"
      },
      "engines": {
        "node": ">=8.0"
      }
    },
    "node_modules/toidentifier": {
      "version": "1.0.1",
      "license": "MIT",
      "engines": {
        "node": ">=0.6"
      }
    },
    "node_modules/touch": {
      "version": "3.1.1",
      "dev": true,
      "license": "ISC",
      "bin": {
        "nodetouch": "bin/nodetouch.js"
      }
    },
    "node_modules/tsx": {
      "version": "4.20.4",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "esbuild": "~0.25.0",
        "get-tsconfig": "^4.7.5"
      },
      "bin": {
        "tsx": "dist/cli.mjs"
      },
      "engines": {
        "node": ">=18.0.0"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.3"
      }
    },
    "node_modules/type-is": {
      "version": "1.6.18",
      "license": "MIT",
      "dependencies": {
        "media-typer": "0.3.0",
        "mime-types": "~2.1.24"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/typescript": {
      "version": "5.9.2",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=14.17"
      }
    },
    "node_modules/undefsafe": {
      "version": "2.0.5",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/undici-types": {
      "version": "6.21.0",
      "license": "MIT"
    },
    "node_modules/unpipe": {
      "version": "1.0.0",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/utils-merge": {
      "version": "1.0.1",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4.0"
      }
    },
    "node_modules/vary": {
      "version": "1.1.2",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    }
  }
}



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\app.ts
--------------------------------------------------------------------------------
// src/app.ts
import compression from "compression";
import cors from "cors";
import "dotenv/config";
import express, {
  type NextFunction,
  type Request,
  type RequestHandler,
  type Response,
} from "express";
import rateLimit from "express-rate-limit";
import helmet from "helmet";
import morgan from "morgan";
import { randomUUID } from "node:crypto";

import { resolveUpstreams } from "./config/upstreams.js";
import aiServiceProxy from "./proxy/aiServiceProxy.js";
import authServiceProxy from "./proxy/authServiceProxy.js";
import dataServiceProxy from "./proxy/dataServiceProxy.js";

const app = express();

const NODE_ENV = process.env.NODE_ENV || "development";
const isProd = NODE_ENV === "production";
const isDev = !isProd;

const bodyLimit = process.env.BODY_LIMIT || "1mb";
const DEV_BEARER = process.env.DEV_BEARER ?? "";

/** Parse comma-separated CORS origins from env */
function parseOrigins(val?: string): string[] {
  if (!val) return [];
  return val.split(",").map((s) => s.trim()).filter(Boolean);
}

// Allow explicit env origins; in dev, default to localhost ports if none provided
const origins = parseOrigins(
  process.env.CORS_ORIGIN ||
    (isDev
      ? "http://localhost:3000,http://localhost:5173,http://localhost:4173"
      : "")
);
/** If no origins provided or '*' present, reflect any origin */
const allowAll = origins.length === 0 || origins.includes("*");

// ────────────────────────── App hardening & basics ──────────────────────────
app.disable("x-powered-by");

// Accept a numeric hop count or boolean "true"
const trustProxy = process.env.TRUST_PROXY ?? (isProd ? "1" : "true");
app.set("trust proxy", trustProxy);

app.use(
  helmet({
    contentSecurityPolicy: false,
    crossOriginEmbedderPolicy: false,
    hsts: isProd ? undefined : false,
  })
);

// ──────────────────────── BULLETPROOF CORS PRE-FLIGHT ───────────────────────
app.use((req: Request, res: Response, next: NextFunction) => {
  if (req.method !== "OPTIONS") return next();

  const origin = (req.headers.origin as string) || "";
  const isAllowedOrigin = allowAll || origins.includes(origin);

  if (!isAllowedOrigin) return next();

  // Reflect origin (credentials-compatible)
  res.setHeader("Access-Control-Allow-Origin", origin);
  res.setHeader("Vary", "Origin");
  res.setHeader("Access-Control-Allow-Credentials", "true");

  // Reflect requested methods/headers so *any* custom headers are accepted
  const reqMethods =
    (req.headers["access-control-request-method"] as string) ||
    "GET,POST,PUT,PATCH,DELETE,OPTIONS";
  const reqHeaders =
    (req.headers["access-control-request-headers"] as string) ||
    "authorization,content-type";

  res.setHeader("Access-Control-Allow-Methods", reqMethods);
  res.setHeader("Access-Control-Allow-Headers", reqHeaders);
  res.setHeader("Access-Control-Max-Age", "86400"); // 24h

  return res.status(204).end();
});

// Standard CORS for non-OPTIONS requests
const corsOptions: cors.CorsOptions = {
  origin: allowAll ? true : origins,
  credentials: true,
  methods: ["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"],
  exposedHeaders: ["X-Request-Id"],
  maxAge: 86400,
  optionsSuccessStatus: 204,
};
app.use(cors(corsOptions));

// ─────────────────────────── Rate limit (prod only) ─────────────────────────
const limiter = rateLimit({
  windowMs: 60_000,
  max: 300,
  standardHeaders: true,
  legacyHeaders: false,
  skip: (req) => req.method === "OPTIONS" || !isProd,
}) as unknown as RequestHandler;
app.use(limiter);

// ─────────────────────────── Parsers & logging ──────────────────────────────
app.use(compression());
app.use(express.json({ limit: bodyLimit }));
app.use(express.urlencoded({ extended: true, limit: bodyLimit }));
app.use(morgan(isDev ? "dev" : "combined"));

// Request ID (echo/propagate downstream)
app.use((req: Request, res: Response, next: NextFunction) => {
  const rid = (req.headers["x-request-id"] as string) || randomUUID();
  req.headers["x-request-id"] = rid;
  res.setHeader("X-Request-Id", rid);
  next();
});

// ───────────────────────────── Health & debug ───────────────────────────────
app.get("/health", (_req, res) =>
  res.json({ ok: true, service: "api-gateway", env: NODE_ENV })
);
app.get("/ready", (_req, res) => res.json({ ready: true }));
app.get("/upstreams", (_req, res) =>
  res.json({ env: NODE_ENV, upstreams: resolveUpstreams() })
);

// ─────────────────────────── Dev stubs (before proxies) ─────────────────────
// NOTE: These must be registered BEFORE the proxies below.
if (isDev) {
  // Refresh endpoint stub for dev: UI posts here; we return a token
  app.post("/api/auth/refresh", (_req, res) => {
    if (!DEV_BEARER) return res.status(500).json({ error: "DEV_BEARER not set" });
    const raw = DEV_BEARER.startsWith("Bearer ") ? DEV_BEARER.slice(7) : DEV_BEARER;
    return res.json({ token: raw });
  });

  // Optional: AI health stub so UI stops 502’ing when ai-service is down
  app.get("/api/ai/health", (_req, res) => {
    res.json({ status: "ok", stub: true });
  });
}

// ───────────────────────── Dev bearer injection block ───────────────────────
// In dev, if no Authorization header, attach DEV_BEARER (do not overwrite a real one)
if (isDev) {
  app.use(["/api", "/api/ai", "/api/auth"], (req, _res, next) => {
    if (!req.headers.authorization && DEV_BEARER) {
      req.headers.authorization = DEV_BEARER.startsWith("Bearer ")
        ? DEV_BEARER
        : `Bearer ${DEV_BEARER}`;
    }
    next();
  });
}

// ──────────────────────────────── Proxies ───────────────────────────────────
// IMPORTANT: Do NOT strip the /api prefix when forwarding to the data-service.
// The data-service mounts its routes under /api/* (e.g., /api/data-sources).
// Order matters: specific first.
app.use("/api/ai", aiServiceProxy);     // rewrites ^/api/ai → /api on upstream (see file)
app.use("/api/auth", authServiceProxy); // rewrites ^/api/auth → /auth on upstream (see file)
app.use("/api", dataServiceProxy);      // forwards /api/* intact to data-service

// ───────────────────────────── 404 + error handling ─────────────────────────
app.use((req, res) => {
  res.status(404).json({
    success: false,
    error: "NOT_FOUND",
    message: `Route ${req.method} ${req.originalUrl} not found`,
  });
});

// Centralized JSON error handler
// eslint-disable-next-line @typescript-eslint/no-unused-vars
app.use((err: any, _req: Request, res: Response, _next: NextFunction) => {
  const status = err.status || err.statusCode || 500;
  const code = err.code || "INTERNAL_SERVER_ERROR";
  const publicMessage =
    isProd ? err.publicMessage || "Internal server error" : err.message || "Internal server error";

  if (!res.headersSent) {
    res.status(status).json({
      success: false,
      error: { code, message: publicMessage },
    });
  }
});

export default app;



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\config\upstreams.ts
--------------------------------------------------------------------------------
// backend/api-gateway/src/config/upstreams.ts
import fs from "node:fs";
import { URL } from "node:url";

export interface Upstreams {
  dataService: string;
  aiService: string;
  authService: string;
}

function inDocker(): boolean {
  try {
    if (fs.existsSync("/.dockerenv")) return true;
    if (process.env.KUBERNETES_SERVICE_HOST) return true;
  } catch {}
  return false;
}

function sanitizeBase(u: string): string {
  try {
    const url = new URL(u);
    url.pathname = url.pathname.replace(/\/+$/, "");
    return url.toString().replace(/\/+$/, "");
  } catch {
    return (u || "").replace(/\/+$/, "");
  }
}

function normalizeServiceBase(raw: string | undefined, fallback: string) {
  let url = (raw || fallback).trim().replace(/\/+$/, "");
  if (url.endsWith("/api")) url = url.slice(0, -4);
  return sanitizeBase(url);
}

function maybeLocalize(u: string, local: string): string {
  if (inDocker()) return u;
  try {
    const url = new URL(u);
    const dockerNames = new Set([
      "data-service",
      "ai-service",
      "auth-service",
      "pipeline-service",
      "notification-service",
      "integration-service",
    ]);
    if (dockerNames.has(url.hostname)) {
      return `${url.protocol}//localhost${url.port ? ":" + url.port : ""}`;
    }
    return u;
  } catch {
    return u;
  }
}

export function resolveUpstreams(): Upstreams {
  const dataService = maybeLocalize(
    normalizeServiceBase(process.env.DATA_SERVICE_URL, "http://data-service:3002"),
    "http://localhost:3002"
  );
  const aiService = maybeLocalize(
    normalizeServiceBase(process.env.AI_SERVICE_URL, "http://ai-service:3003"),
    "http://localhost:3003"
  );
  const authService = maybeLocalize(
    normalizeServiceBase(process.env.AUTH_SERVICE_URL, "http://auth-service:8001"),
    "http://localhost:8001"
  );
  return { dataService, aiService, authService };
}



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\middleware\healthCheck.ts
--------------------------------------------------------------------------------
import { Request, Response } from 'express';
// These clients come from @cwic/shared (weâ€™ll wire them as simple axios wrappers)
import { AIServiceClient, AuthServiceClient, DataServiceClient } from '@cwic/shared';

const serviceClients = {
  auth: new AuthServiceClient(),
  data: new DataServiceClient(),
  ai: new AIServiceClient()
};

export const healthCheck = async (req: Request, res: Response): Promise<void> => {
  const startTime = Date.now();
  try {
    const serviceChecks = await Promise.allSettled([
      checkServiceHealth('auth', serviceClients.auth),
      checkServiceHealth('data', serviceClients.data),
      checkServiceHealth('ai', serviceClients.ai)
    ]);

    const services = serviceChecks.reduce((acc, result, index) => {
      const serviceName = ['auth', 'data', 'ai'][index];
      acc[serviceName] = result.status === 'fulfilled'
        ? result.value
        : { status: 'unhealthy', error: (result as any).reason?.message };
      return acc;
    }, {} as any);

    const allHealthy = Object.values(services).every((s: any) => s.status === 'healthy');

    res.status(allHealthy ? 200 : 503).json({
      status: allHealthy ? 'healthy' : 'degraded',
      timestamp: new Date().toISOString(),
      uptime: process.uptime(),
      responseTime: Date.now() - startTime,
      version: process.env.npm_package_version || '1.0.0',
      environment: process.env.NODE_ENV || 'development',
      services
    });
  } catch (error) {
    res.status(503).json({
      status: 'unhealthy',
      timestamp: new Date().toISOString(),
      uptime: process.uptime(),
      responseTime: Date.now() - startTime,
      error: error instanceof Error ? error.message : 'Unknown error'
    });
  }
};

async function checkServiceHealth(_name: string, client: any): Promise<any> {
  try {
    const start = Date.now();
    await client.get('/health');
    return { status: 'healthy', responseTime: Date.now() - start };
  } catch (error) {
    return { status: 'unhealthy', error: error instanceof Error ? error.message : 'Unknown error' };
  }
}



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\proxy\aiServiceProxy.ts
--------------------------------------------------------------------------------
// backend/api-gateway/src/proxy/aiServiceProxy.ts
import type { Request, Response } from "express";
import { createProxyMiddleware, type Options } from "http-proxy-middleware";
import type { Socket } from "node:net";
import { resolveUpstreams } from "../config/upstreams.js";

const { aiService: target } = resolveUpstreams();
const isDev = process.env.NODE_ENV !== "production";
const devBearer = process.env.DEV_BEARER ?? "";

const options: Options<Request, Response> = {
  target,
  changeOrigin: true,
  xfwd: true,
  secure: isDev ? false : true,
  proxyTimeout: 30_000,
  timeout: 30_000,
  // AI service exposes /api/* â€” we mount at /api/ai â†’ rewrite to /api/*
  pathRewrite: { "^/api/ai": "/api" },

  on: {
    proxyReq: (proxyReq: any, req: Request) => {
      if (isDev && devBearer) {
        try { proxyReq.removeHeader?.("authorization"); } catch {}
        proxyReq.setHeader("authorization", devBearer.startsWith("Bearer ")
          ? devBearer
          : `Bearer ${devBearer}`);
        proxyReq.setHeader("x-dev-auth", "gateway");
      } else if (req.headers.authorization) {
        proxyReq.setHeader("authorization", req.headers.authorization);
      }
      const rid = (req.headers["x-request-id"] as string) || "";
      if (rid) proxyReq.setHeader("x-request-id", rid);
    },

    error: (err: Error, req: Request, res: Response | Socket) => {
      if (typeof (res as Socket).destroy === "function" && !(res as any).setHeader) {
        try { (res as Socket).destroy(); } catch {}
        return;
      }
      const r = res as Response;
      if (!r.headersSent) {
        r.status(502).json({
          success: false,
          error: "AI_SERVICE_UNAVAILABLE",
          message: "AI service is currently unavailable",
          details: err.message,
          upstream: target,
          timestamp: new Date().toISOString(),
        });
      }
    },
  },
};

export default createProxyMiddleware(options);



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\proxy\authServiceProxy.ts
--------------------------------------------------------------------------------
// backend/api-gateway/src/proxy/authServiceProxy.ts
import type { Request, Response } from "express";
import { createProxyMiddleware, type Options } from "http-proxy-middleware";
import type { Socket } from "node:net";
import { resolveUpstreams } from "../config/upstreams.js";

const { authService: target } = resolveUpstreams();
const isDev = process.env.NODE_ENV !== "production";
const devBearer = process.env.DEV_BEARER ?? "";

const options: Options<Request, Response> = {
  target,
  changeOrigin: true,
  xfwd: true,
  secure: isDev ? false : true,
  proxyTimeout: 30_000,
  timeout: 30_000,
  // Auth service exposes /auth/* â€” we mount at /api/auth â†’ rewrite to /auth/*
  pathRewrite: { "^/api/auth": "/auth" },

  on: {
    proxyReq: (proxyReq: any, req: Request) => {
      if (isDev && devBearer) {
        try { proxyReq.removeHeader?.("authorization"); } catch {}
        proxyReq.setHeader("authorization", devBearer.startsWith("Bearer ")
          ? devBearer
          : `Bearer ${devBearer}`);
        proxyReq.setHeader("x-dev-auth", "gateway");
      } else if (req.headers.authorization) {
        proxyReq.setHeader("authorization", req.headers.authorization);
      }
      const rid = (req.headers["x-request-id"] as string) || "";
      if (rid) proxyReq.setHeader("x-request-id", rid);
    },

    error: (err: Error, req: Request, res: Response | Socket) => {
      if (typeof (res as Socket).destroy === "function" && !(res as any).setHeader) {
        try { (res as Socket).destroy(); } catch {}
        return;
      }
      const r = res as Response;
      if (!r.headersSent) {
        r.status(502).json({
          success: false,
          error: "AUTH_SERVICE_UNAVAILABLE",
          message: "Auth service is currently unavailable",
          details: err.message,
          upstream: target,
          timestamp: new Date().toISOString(),
        });
      }
    },
  },
};

export default createProxyMiddleware(options);



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\proxy\dataServiceProxy.ts
--------------------------------------------------------------------------------
// backend/api-gateway/src/proxy/dataServiceProxy.ts
import type { Request, Response } from "express";
import { createProxyMiddleware, type Options } from "http-proxy-middleware";
import type { Socket } from "node:net";
import { resolveUpstreams } from "../config/upstreams.js";

const { dataService: target } = resolveUpstreams();
const isDev = process.env.NODE_ENV !== "production";
const devBearer = process.env.DEV_BEARER ?? "";

function attachBody(proxyReq: any, req: Request): void {
  if (!req.body || req.method === "GET" || req.method === "HEAD") return;
  const ct = String(
    proxyReq.getHeader("content-type") || proxyReq.getHeader("Content-Type") || ""
  ).toLowerCase();

  if (ct.includes("application/json")) {
    const body = JSON.stringify(req.body);
    proxyReq.setHeader("content-length", Buffer.byteLength(body));
    proxyReq.write(body);
  } else if (ct.includes("application/x-www-form-urlencoded")) {
    const body = new URLSearchParams(req.body as Record<string, string>).toString();
    proxyReq.setHeader("content-length", Buffer.byteLength(body));
    proxyReq.write(body);
  }
}

const options: Options<Request, Response> = {
  target,
  changeOrigin: true,
  xfwd: true,
  secure: isDev ? false : true,
  proxyTimeout: 30_000,
  timeout: 30_000,

  /**
   * IMPORTANT:
   * Mount path is `/api` in app.ts. Express passes req.url WITHOUT `/api`.
   * data-service expects `/api/*`, so we restore it using the originalUrl.
   * Example: originalUrl = /api/data-sources â†’ forward /api/data-sources
   */
  pathRewrite: (_path, req) => (req as Request).originalUrl,

  on: {
    proxyReq: (proxyReq: any, req: Request) => {
      if (isDev && devBearer) {
        try { proxyReq.removeHeader?.("authorization"); } catch {}
        proxyReq.setHeader("authorization", devBearer.startsWith("Bearer ")
          ? devBearer
          : `Bearer ${devBearer}`);
        proxyReq.setHeader("x-dev-auth", "gateway");
      } else if (req.headers.authorization) {
        proxyReq.setHeader("authorization", req.headers.authorization);
      }

      const rid = (req.headers["x-request-id"] as string) || "";
      if (rid) proxyReq.setHeader("x-request-id", rid);

      const cookie = req.headers["cookie"];
      if (cookie) proxyReq.setHeader("cookie", cookie);

      attachBody(proxyReq, req);

      if (isDev) {
        // http-proxy-middleware doesn't expose the final URL easily; show both sides
        console.debug(`[proxyâ†’data] ${req.method} ${req.originalUrl} â†’ ${target}${(proxyReq as any).path ?? ""}`);
      }
    },

    proxyRes: (proxyRes, req: Request) => {
      const origin = req.headers.origin;
      if (origin) {
        proxyRes.headers["access-control-allow-origin"] = origin;
        proxyRes.headers["access-control-allow-credentials"] = "true";
      }
      if (isDev) {
        console.debug(`[proxyâ†’data] ${req.method} ${req.originalUrl} â‡ ${proxyRes.statusCode}`);
      }
    },

    error: (err: Error, req: Request, res: Response | Socket) => {
      console.error(`[proxyâ†’data] Error for ${req.method} ${req.originalUrl}:`, err.message);

      if (typeof (res as Socket).destroy === "function" && !(res as any).setHeader) {
        try { (res as Socket).destroy(); } catch {}
        return;
      }

      const r = res as Response;
      if (!r.headersSent) {
        const origin = req.headers.origin;
        if (origin) {
          r.setHeader("Access-Control-Allow-Origin", origin);
          r.setHeader("Access-Control-Allow-Credentials", "true");
        }
        r.status(502).json({
          success: false,
          error: "DATA_SERVICE_UNAVAILABLE",
          message: "Data service is currently unavailable",
          details: err.message,
          upstream: target,
          timestamp: new Date().toISOString(),
        });
      }
    },
  },
};

export default createProxyMiddleware(options);



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\routes\data-proxy.ts
--------------------------------------------------------------------------------
import { Request, Response, Router } from 'express';
import { createProxyMiddleware, type Options } from 'http-proxy-middleware';

const router = Router();

// Compose sets DATA_SERVICE_URL=http://data-service:3002
const base = (process.env.DATA_SERVICE_URL || 'http://data-service:3002').replace(/\/+$/, '');
const target = `${base}/api`; // upstream base includes /api

function attachBody(proxyReq: any, req: Request): void {
  const ct = String(proxyReq.getHeader('Content-Type') || '');
  if (!req.body || req.method === 'GET' || req.method === 'HEAD') return;

  if (ct.includes('application/json')) {
    const body = JSON.stringify(req.body);
    proxyReq.setHeader('Content-Length', Buffer.byteLength(body));
    proxyReq.write(body);
  } else if (ct.includes('application/x-www-form-urlencoded')) {
    const body = new URLSearchParams(req.body as Record<string, string>).toString();
    proxyReq.setHeader('Content-Length', Buffer.byteLength(body));
    proxyReq.write(body);
  }
}

const options: Options<Request, Response> = {
  target,
  changeOrigin: true,
  on: {
    proxyReq: (proxyReq, req) => attachBody(proxyReq, req as Request),
    error: (err, _req, res) => {
      const r = res as Response;
      if (!r.headersSent) {
        r.status(502).json({
          success: false,
          error: 'BAD_GATEWAY',
          message: (err as Error).message,
          upstream: target,
        });
      }
    },
    proxyRes: (_proxyRes, req, _res) => {
      // lightweight trace in your logs
      (req as any).log?.debug?.(`Proxied ${req.method} ${req.originalUrl} -> ${target}${req.url}`);
    },
  },
};

// Handle both /api and /api/*
const proxyMw = createProxyMiddleware(options);
router.all('/', proxyMw as any);
router.all('/*', proxyMw as any);

export default router;



--------------------------------------------------------------------------------
FILE: backend\api-gateway\src\server.ts
--------------------------------------------------------------------------------
// src/server.ts
import "dotenv/config";
import app from "./app.js";

const PORT = Number(process.env.PORT ?? 8000);
const HOST = process.env.HOST ?? "0.0.0.0";

const DATA_SERVICE_URL = process.env.DATA_SERVICE_URL ?? "http://localhost:3002";
const AI_SERVICE_URL = process.env.AI_SERVICE_URL ?? "http://localhost:8003";

const server = app.listen(PORT, HOST, () => {
  console.log(`🚀 API Gateway listening on ${HOST}:${PORT}`);
  console.log(`📊 Health:           http://localhost:${PORT}/health`);
  console.log(`🔄 Proxy /api/*  →   ${DATA_SERVICE_URL}`);
  console.log(`🤖 Proxy /api/ai/* → ${AI_SERVICE_URL}`);
});

function shutdown(signal: string) {
  console.log(`[gateway] ${signal} received → closing server...`);
  server.close((err?: Error) => {
    if (err) {
      console.error("[gateway] Error during close:", err);
      process.exit(1);
    }
    console.log("[gateway] Closed cleanly.");
    process.exit(0);
  });
  // Force-exit if something hangs (open sockets, etc.)
  setTimeout(() => {
    console.warn("[gateway] Forced shutdown after 10s");
    process.exit(1);
  }, 10_000).unref();
}

process.on("SIGINT", () => shutdown("SIGINT"));
process.on("SIGTERM", () => shutdown("SIGTERM"));
process.on("unhandledRejection", (reason) => {
  console.error("[gateway] Unhandled Rejection:", reason);
});
process.on("uncaughtException", (err) => {
  console.error("[gateway] Uncaught Exception:", err);
  // In prod you may decide to exit(1); keeping alive here for dev.
  // if (process.env.NODE_ENV === "production") process.exit(1);
});

export default server;



--------------------------------------------------------------------------------
FILE: backend\api-gateway\tsconfig.json
--------------------------------------------------------------------------------
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "outDir": "dist",
    "rootDir": "src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["src"]
}



========================================================================================================================
  BACKEND SERVICE: AUTH-SERVICE
========================================================================================================================


--------------------------------------------------------------------------------
FILE: backend\auth-service\.env
--------------------------------------------------------------------------------
SERVICE_NAME=auth-service
PORT=3001
CORS_ORIGIN=http://localhost:5173

DATABASE_URL=postgresql://cwic:cwic@localhost:5432/cwic   # or your compose conn string
REDIS_URL=redis://localhost:6379/0

JWT_ACCESS_SECRET=change_me_access_32chars_min
JWT_REFRESH_SECRET=change_me_refresh_32chars_min
ACCESS_TTL=15m
REFRESH_TTL=7d



--------------------------------------------------------------------------------
FILE: backend\auth-service\Dockerfile
--------------------------------------------------------------------------------
# ---------- deps ----------
FROM node:20-alpine AS deps
WORKDIR /app
# Copy ONLY manifests first for better caching
COPY package*.json ./
# Lockfile may not exist in dev â€” fall back to npm i
RUN if [ -f package-lock.json ]; then \
    npm ci --no-audit --no-fund; \
    else \
    npm i --no-audit --no-fund; \
    fi

# ---------- build ----------
FROM node:20-alpine AS build
WORKDIR /app
# Bring node_modules from deps layer (already installed)
COPY --from=deps /app/node_modules ./node_modules
# Copy manifests + tsconfig for build metadata
COPY package*.json tsconfig.json ./
# Copy source
COPY src ./src
# Build: use script if present, otherwise fallback to tsc
RUN npm run build || npx tsc -p tsconfig.json

# ---------- runtime ----------
FROM node:20-alpine AS runtime
WORKDIR /app
ENV NODE_ENV=production
# Minimal utilities (init + health probe)
RUN apk add --no-cache dumb-init curl
# Non-root user for safety
USER node
# Copy runtime bits
COPY --from=deps  /app/node_modules ./node_modules
COPY --from=build /app/dist        ./dist
EXPOSE 8001
HEALTHCHECK --interval=30s --timeout=5s --retries=3 \
    CMD curl -fsS http://localhost:8001/health || exit 1
ENTRYPOINT ["dumb-init","--"]
CMD ["node","--enable-source-maps","dist/server.js"]



--------------------------------------------------------------------------------
FILE: backend\auth-service\Dockerfile.dev
--------------------------------------------------------------------------------
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3000
CMD [ "npm", "run", "dev" ]



--------------------------------------------------------------------------------
FILE: backend\auth-service\package.json
--------------------------------------------------------------------------------
{
  "name": "cwic-auth-service",
  "version": "1.0.0",
  "type": "module",
  "main": "dist/server.js",
  "scripts": {
    "dev": "nodemon --watch src --ext ts --exec \"npx tsx src/server.ts\"",
    "build": "tsc",
    "start": "node dist/server.js",
    "typecheck": "tsc --noEmit"
  },
  "engines": {
    "node": ">=18"
  },
  "dependencies": {
    "bcryptjs": "^2.4.3",
    "compression": "^1.7.4",
    "cors": "^2.8.5",
    "dotenv": "^16.4.5",
    "express": "^4.19.2",
    "helmet": "^7.1.0",
    "jsonwebtoken": "^9.0.2",
    "pg": "^8.12.0",
    "redis": "^4.7.0",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@types/bcryptjs": "^2.4.6",
    "@types/express": "^4.17.21",
    "@types/jsonwebtoken": "^9.0.5",
    "@types/ms": "^2.1.0",
    "@types/node": "^20.19.10",
    "nodemon": "^3.1.0",
    "tsx": "^4.20.4",
    "typescript": "^5.4.5"
  }
}



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\app.ts
--------------------------------------------------------------------------------
import compression from 'compression';
import cors from 'cors';
import express from 'express';
import helmet from 'helmet';

export const app = express();

app.use(helmet());
app.use(cors());
app.use(compression());
app.use(express.json());

// Health endpoint for Docker HEALTHCHECK
app.get('/health', (_req, res) => {
  res.status(200).json({ status: 'ok', service: 'auth-service' });
});



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\config\env.ts
--------------------------------------------------------------------------------
import 'dotenv/config';
import { z } from 'zod';

const Env = z.object({
  NODE_ENV: z.enum(['development','test','production']).default('development'),
  PORT: z.coerce.number().default(3001),
  CORS_ORIGIN: z.string().default('http://localhost:5173'),

  DATABASE_URL: z.string(),
  REDIS_URL: z.string(),

  JWT_ACCESS_SECRET: z.string().min(16),
  JWT_REFRESH_SECRET: z.string().min(16),
  ACCESS_TTL: z.string().default('15m'),
  REFRESH_TTL: z.string().default('7d'),
});

export const env = Env.parse(process.env);
export const corsOrigins = env.CORS_ORIGIN.split(',').map(s => s.trim());



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\db.ts
--------------------------------------------------------------------------------
import { Pool } from 'pg';
import { env } from './config/env.js';

export const pool = new Pool({
  connectionString: env.DATABASE_URL,
  max: 15,
  idleTimeoutMillis: 30_000,
  statement_timeout: 60_000,
});



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\middleware\error.ts
--------------------------------------------------------------------------------
import type { NextFunction, Request, Response } from 'express';

export class HttpError extends Error {
  status: number; code?: string;
  constructor(status: number, message: string, code?: string) {
    super(message); this.status = status; this.code = code;
  }
}
export function errorHandler(err: any, _req: Request, res: Response, _next: NextFunction) {
  const status = err.status || 500;
  res.status(status).json({
    type: 'about:blank',
    title: err.code || 'ServerError',
    status,
    detail: err.message || 'Unexpected error'
  });
}



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\middleware\requireAuth.ts
--------------------------------------------------------------------------------
import type { NextFunction, Request, Response } from 'express';
import jwt from 'jsonwebtoken';
import { env } from '../config/env.js';

export function requireAuth(roles?: string[]) {
  return (req: Request, res: Response, next: NextFunction) => {
    const h = req.headers.authorization || '';
    const token = h.startsWith('Bearer ') ? h.slice(7) : null;
    if (!token) return res.status(401).json({ detail: 'Missing token' });
    try {
      const decoded: any = jwt.verify(token, env.JWT_ACCESS_SECRET);
      (req as any).user = decoded;
      if (roles && !roles.some(r => (decoded.roles || []).includes(r))) {
        return res.status(403).json({ detail: 'Forbidden' });
      }
      next();
    } catch { return res.status(401).json({ detail: 'Invalid token' }); }
  };
}



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\redis.ts
--------------------------------------------------------------------------------
import { createClient } from 'redis';
import { env } from './config/env.js';

export const redis = createClient({ url: env.REDIS_URL });

export async function initRedis() {
  redis.on('error', (e) => console.error('[redis] error', e));
  if (!redis.isOpen) {
    await redis.connect();
  }
}



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\routes\auth.ts
--------------------------------------------------------------------------------
import { randomUUID } from 'crypto';
import { Router } from 'express';
import { z } from 'zod';
import { pool } from '../db.js';
import { HttpError } from '../middleware/error.js';
import { redis } from '../redis.js';
import { signAccess, signRefresh, verifyRefresh } from '../utils/jwt.js';
import { hashPassword, verifyPassword } from '../utils/password.js';

export const authRouter = Router();

const RegisterSchema = z.object({
  email: z.string().email(),
  password: z.string().min(8),
  display_name: z.string().min(2).max(100)
});

const LoginSchema = z.object({
  email: z.string().email(),
  password: z.string().min(8)
});

/** POST /auth/register */
authRouter.post('/register', async (req, res, next) => {
  try {
    const { email, password, display_name } = RegisterSchema.parse(req.body);
    const pwHash = await hashPassword(password);

    const { rows } = await pool.query(
      `INSERT INTO users (email, password_hash, display_name, roles, is_verified)
       VALUES ($1, $2, $3, ARRAY['user'], false)
       ON CONFLICT (email) DO NOTHING
       RETURNING id, email, display_name, roles, is_verified, created_at`,
      [email, pwHash, display_name]
    );
    const user = rows[0];
    if (!user) throw new HttpError(409, 'Email already registered', 'EmailTaken');

    const sid = randomUUID();
    const access = signAccess({ sub: user.id, roles: user.roles, sid });
    const refresh = signRefresh({ sub: user.id, sid });
    res.status(201).json({ user, tokens: { access, refresh } });
  } catch (e) { next(e); }
});

/** POST /auth/login */
authRouter.post('/login', async (req, res, next) => {
  try {
    const { email, password } = LoginSchema.parse(req.body);
    const { rows } = await pool.query(
      'SELECT id, email, password_hash, display_name, roles, is_verified FROM users WHERE email=$1',
      [email]
    );
    const u = rows[0];
    if (!u || !(await verifyPassword(password, u.password_hash))) {
      throw new HttpError(401, 'Invalid credentials', 'AuthFailed');
    }
    const sid = randomUUID();
    const access = signAccess({ sub: u.id, roles: u.roles, sid });
    const refresh = signRefresh({ sub: u.id, sid });
    res.json({ user: { id: u.id, email: u.email, display_name: u.display_name, roles: u.roles }, tokens: { access, refresh } });
  } catch (e) { next(e); }
});

/** POST /auth/refresh */
authRouter.post('/refresh', async (req, res, next) => {
  try {
    const { refresh } = (req.body || {}) as { refresh?: string };
    if (!refresh) throw new HttpError(400, 'Missing refresh token', 'BadRequest');

    let decoded: any;
    try { decoded = verifyRefresh(refresh); }
    catch { throw new HttpError(401, 'Invalid refresh token', 'InvalidRefresh'); }

    // revoked?
    const revoked = await redis.get(`revoked:${decoded.sid}`);
    if (revoked) throw new HttpError(401, 'Refresh revoked', 'TokenRevoked');

    // rotate: revoke old session id
    await redis.setEx(`revoked:${decoded.sid}`, 60 * 60 * 24 * 8, '1');
    const newSid = randomUUID();

    // fetch roles to embed (optional)
    const { rows } = await pool.query('SELECT roles FROM users WHERE id=$1', [decoded.sub]);
    const roles = rows[0]?.roles || ['user'];

    const access = signAccess({ sub: decoded.sub, roles, sid: newSid });
    const newRefresh = signRefresh({ sub: decoded.sub, sid: newSid });
    res.json({ access, refresh: newRefresh });
  } catch (e) { next(e); }
});

/** POST /auth/logout */
authRouter.post('/logout', async (req, res, _next) => {
  try {
    const { refresh } = (req.body || {}) as { refresh?: string };
    if (refresh) {
      try {
        const decoded: any = verifyRefresh(refresh);
        await redis.setEx(`revoked:${decoded.sid}`, 60 * 60 * 24 * 8, '1');
      } catch { /* ignore */ }
    }
    res.status(204).send();
  } catch { res.status(204).send(); }
});

/** GET /auth/me (requires Authorization: Bearer <access>) */
authRouter.get('/me', async (req, res, next) => {
  try {
    const h = req.headers.authorization || '';
    const token = h.startsWith('Bearer ') ? h.slice(7) : '';
    if (!token) throw new HttpError(401, 'Missing token', 'AuthRequired');

    // Verify access; we donâ€™t need roles here
    const jwt = await import('jsonwebtoken');
    const { env } = await import('../config/env.js');
    const decoded: any = jwt.default.verify(token, env.JWT_ACCESS_SECRET);

    const { rows } = await pool.query(
      'SELECT id, email, display_name, roles, is_verified, created_at FROM users WHERE id=$1',
      [decoded.sub]
    );
    const u = rows[0];
    if (!u) throw new HttpError(404, 'User not found', 'NotFound');
    res.json(u);
  } catch (e) { next(e); }
});



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\server.ts
--------------------------------------------------------------------------------
import 'dotenv/config'; // load env early
import process from 'node:process';
import { app } from './app.js'; // NodeNext: use .js for relative import

const port = Number(process.env.PORT) || 8001;
app.listen(port, () => {
  console.log(`auth-service listening on :${port}`);
});



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\utils\jwt.ts
--------------------------------------------------------------------------------
import jwt, { type JwtPayload, type Secret, type SignOptions } from 'jsonwebtoken';
import type { StringValue } from 'ms';
import { env } from '../config/env.js';

const accessOpts: SignOptions = { expiresIn: env.ACCESS_TTL as StringValue };
const refreshOpts: SignOptions = { expiresIn: env.REFRESH_TTL as StringValue };

export function signAccess(payload: JwtPayload | string | object) {
  return jwt.sign(payload as any, env.JWT_ACCESS_SECRET as Secret, accessOpts);
}

export function signRefresh(payload: JwtPayload | string | object) {
  return jwt.sign(payload as any, env.JWT_REFRESH_SECRET as Secret, refreshOpts);
}

export function verifyAccess(token: string) {
  return jwt.verify(token, env.JWT_ACCESS_SECRET as Secret);
}

export function verifyRefresh(token: string) {
  return jwt.verify(token, env.JWT_REFRESH_SECRET as Secret);
}



--------------------------------------------------------------------------------
FILE: backend\auth-service\src\utils\password.ts
--------------------------------------------------------------------------------
import bcrypt from 'bcryptjs';
export async function hashPassword(pw: string) { return bcrypt.hash(pw, 12); }
export async function verifyPassword(pw: string, hash: string) { return bcrypt.compare(pw, hash); }



--------------------------------------------------------------------------------
FILE: backend\auth-service\tsconfig.json
--------------------------------------------------------------------------------
{
  "compilerOptions": {
    "target": "ES2021",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "outDir": "dist",
    "rootDir": "src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "resolveJsonModule": true,
    "sourceMap": true,
    "types": ["node"]
  },
  "include": ["src"]
}



========================================================================================================================
  BACKEND SERVICE: DATA-SERVICE
========================================================================================================================


--------------------------------------------------------------------------------
FILE: backend\data-service\.env
--------------------------------------------------------------------------------
SERVICE_NAME=data-service
PORT=3002
HOST=0.0.0.0
NODE_ENV=development

# Frontend + Gateway (for browser)
CORS_ORIGIN=http://localhost:5173,http://localhost:3000,http://localhost:8000

# Local DB/Redis on your machine
DATABASE_URL=postgresql://cwic_user:cwic_secure_pass@localhost:5432/cwic_platform
REDIS_URL=redis://localhost:6379/0

DB_POOL_MAX=20
DB_POOL_MIN=2
DB_IDLE_TIMEOUT=30000
DB_CONNECTION_TIMEOUT=10000
DB_SSL=false

JWT_SECRET=devsecret
JWT_EXPIRES_IN=7d

LOG_LEVEL=debug
ENABLE_FILE_LOGGING=false

ENABLE_METRICS=true
ENABLE_HEALTH_CHECK=true

RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=1000

ENABLE_ASSET_DISCOVERY=true
ENABLE_LINEAGE_TRACKING=true
ENABLE_QUALITY_SCORING=true
ENABLE_COMPLIANCE_CHECKS=true

DEBUG_SQL=false
ENABLE_QUERY_LOGGING=false
MOCK_EXTERNAL_SERVICES=false
SKIP_AUTH=true



--------------------------------------------------------------------------------
FILE: backend\data-service\.env.docker
--------------------------------------------------------------------------------
SERVICE_NAME=data-service
PORT=3002
HOST=0.0.0.0
NODE_ENV=development

CORS_ORIGIN=http://localhost:5173,http://localhost:3000,http://localhost:8000

# Docker DNS names
DATABASE_URL=postgresql://cwic_user:cwic_secure_pass@db:5432/cwic_platform
REDIS_URL=redis://redis:6379/0

DB_POOL_MAX=20
DB_POOL_MIN=2
DB_IDLE_TIMEOUT=30000
DB_CONNECTION_TIMEOUT=10000
DB_SSL=false

JWT_SECRET=devsecret
JWT_EXPIRES_IN=7d

LOG_LEVEL=debug
ENABLE_FILE_LOGGING=false

ENABLE_METRICS=true
ENABLE_HEALTH_CHECK=true



--------------------------------------------------------------------------------
FILE: backend\data-service\Dockerfile
--------------------------------------------------------------------------------
# ---------- deps ----------
FROM node:20-alpine AS deps
WORKDIR /app
COPY package*.json ./
RUN if [ -f package-lock.json ]; then \
    npm ci --no-audit --no-fund; \
    else \
    npm i --no-audit --no-fund; \
    fi

# ---------- build ----------
FROM node:20-alpine AS build
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY package*.json tsconfig.json ./
COPY src ./src
RUN npm run build || npx tsc -p tsconfig.json

# ---------- runtime ----------
FROM node:20-alpine AS runtime
WORKDIR /app
ENV NODE_ENV=production
USER node
COPY --from=deps  /app/node_modules ./node_modules
COPY --from=build /app/dist        ./dist
EXPOSE 3002
CMD ["node","dist/server.js"]



--------------------------------------------------------------------------------
FILE: backend\data-service\Dockerfile.dev
--------------------------------------------------------------------------------
FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
# optional, nodemon usable from local deps, but global is fine:
RUN npm i -g nodemon
COPY tsconfig.json ./
# You can skip copying src here because compose mounts the code.
EXPOSE 8000
CMD ["npm","run","dev"]



--------------------------------------------------------------------------------
FILE: backend\data-service\package.json
--------------------------------------------------------------------------------
{
  "name": "cwic-data-service",
  "version": "1.0.0",
  "description": "CWIC Data Governance Service - Data Source and Asset Management",
  "main": "dist/server.js",
  "scripts": {
    "build": "tsc",
    "start": "node dist/server.js",
    "dev": "ts-node-dev --respawn --transpile-only src/server.ts",
    "dev:watch": "nodemon --exec ts-node src/server.ts",
    "test": "jest",
    "test:watch": "jest --watch",
    "test:coverage": "jest --coverage",
    "lint": "eslint src/**/*.ts",
    "lint:fix": "eslint src/**/*.ts --fix",
    "format": "prettier --write src/**/*.ts",
    "migrate": "ts-node src/scripts/migrate.ts",
    "migrate:rollback": "ts-node src/scripts/migrate.ts --rollback",
    "seed": "ts-node src/scripts/seed.ts",
    "docker:build": "docker build -t cwic-data-service .",
    "docker:run": "docker run -p 8002:8002 --env-file .env cwic-data-service"
  },
  "keywords": [
    "data-governance",
    "data-management",
    "typescript",
    "nodejs",
    "postgresql",
    "rest-api"
  ],
  "author": "CWIC Team",
  "license": "MIT",
  "dependencies": {
    "@aws-sdk/client-s3": "^3.465.0",
    "axios": "^1.6.2",
    "bcryptjs": "^2.4.3",
    "compression": "^1.7.4",
    "cors": "^2.8.5",
    "csv-parser": "^3.0.0",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "express-rate-limit": "^7.1.5",
    "express-validator": "^7.0.1",
    "helmet": "^7.1.0",
    "joi": "^17.11.0",
    "jsonwebtoken": "^9.0.2",
    "mongodb": "^6.3.0",
    "mssql": "^10.0.4",
    "multer": "^1.4.5-lts.1",
    "mysql2": "^3.6.5",
    "nanoid": "^4.0.2",
    "pg": "^8.11.3",
    "redis": "^4.6.10",
    "uuid": "^9.0.1",
    "winston": "^3.11.0",
    "xlsx": "^0.18.5"
  },
  "devDependencies": {
    "@types/bcryptjs": "^2.4.6",
    "@types/compression": "^1.7.5",
    "@types/cors": "^2.8.17",
    "@types/express": "^4.17.21",
    "@types/jest": "^29.5.8",
    "@types/jsonwebtoken": "^9.0.5",
    "@types/mssql": "^9.1.7",
    "@types/multer": "^1.4.11",
    "@types/node": "^20.10.4",
    "@types/pg": "^8.10.9",
    "@types/supertest": "^2.0.16",
    "@types/uuid": "^9.0.7",
    "@typescript-eslint/eslint-plugin": "^6.13.1",
    "@typescript-eslint/parser": "^6.13.1",
    "eslint": "^8.55.0",
    "jest": "^29.7.0",
    "nodemon": "^3.0.2",
    "prettier": "^3.1.0",
    "supertest": "^6.3.3",
    "ts-jest": "^29.1.1",
    "ts-node": "^10.9.1",
    "ts-node-dev": "^2.0.0",
    "typescript": "^5.3.3"
  },
  "engines": {
    "node": ">=18.0.0",
    "npm": ">=8.0.0"
  }
}



--------------------------------------------------------------------------------
FILE: backend\data-service\src\app.ts
--------------------------------------------------------------------------------
// backend/data-service/src/app.ts
import compression from 'compression';
import cors from 'cors';
import express from 'express';
import helmet from 'helmet';

import { config, validateConfig } from './config/env';
import { logger } from './utils/logger';

// Mount AFTER probes:
import { errorHandler, notFoundHandler, requestIdMiddleware } from './middleware/error';
import { defaultRateLimit } from './middleware/rateLimit';

// Routes (mounted later) — NOTE: match the actual filenames on disk (Linux is case-sensitive)
import assetRoutes from './routes/assets';
import dataSourceRoutes from './routes/dataSources';
import sourceRoutes from './routes/sources';

// Services
import { DatabaseService } from './services/DatabaseService';

// Generic async wrapper so thrown/rejected handlers hit error middleware
const asyncHandler =
  (fn: express.RequestHandler): express.RequestHandler =>
    (req, res, next) => Promise.resolve(fn(req, res, next)).catch(next);

class App {
  public app: express.Application;
  private dbService: DatabaseService;
  public ready = false;

  constructor() {
    this.app = express();
    this.dbService = new DatabaseService();

    this.validateEnvironment();

    // 1) Probes FIRST (no external deps)
    this.registerProbes();

    // 2) Base middleware (safe)
    this.registerBaseMiddleware();

    // 3) Dependent middleware (may touch Redis/etc)
    this.registerDependentMiddleware();

    // 4) API routes (mounted WITHOUT /api — gateway adds /api and then strips it)
    this.registerApiRoutes();

    // 5) Error handlers LAST
    this.registerErrorHandlers();
  }

  private validateEnvironment(): void {
    try {
      validateConfig();
      logger.info('Environment validation passed');
    } catch (error) {
      logger.error('Environment validation failed:', error);
      process.exit(1);
    }
  }

  /** Liveness/readiness endpoints that must not drop sockets */
  private registerProbes(): void {
    // Liveness: never touches DB
    this.app.get('/health', (_req, res) => {
      res.status(200).json({
        status: 'ok',
        service: 'cwic-data-service',
        env: config.server.env,
        uptimeSec: Math.round(process.uptime()),
      });
    });
    this.app.head('/health', (_req, res) => res.sendStatus(200));

    // Readiness: checks DB; returns 503 if not ready
    this.app.get('/ready', asyncHandler(async (_req, res) => {
      const dbHealth = await this.dbService.healthCheck().catch((e: any) => ({
        status: 'unhealthy',
        error: e?.message || String(e),
      }));
      const good = dbHealth.status === 'healthy';
      this.ready = good;
      res.status(good ? 200 : 503).json({
        status: good ? 'ready' : 'not_ready',
        database: dbHealth,
      });
    }));
    this.app.head('/ready', asyncHandler(async (_req, res) => {
      const dbHealth = await this.dbService.healthCheck().catch(() => ({ status: 'unhealthy' }));
      res.sendStatus(dbHealth.status === 'healthy' ? 200 : 503);
    }));
  }

  private registerBaseMiddleware(): void {
    this.app.use(helmet(config.security.helmet));

    // CORS — data-service is usually behind the gateway, but allow local direct dev too.
    // Include custom headers you send from the frontend.
    this.app.use(cors({
      origin: config.server.corsOrigin,
      credentials: true,
      optionsSuccessStatus: 200,
      methods: ['GET','POST','PUT','DELETE','PATCH','OPTIONS'],
      allowedHeaders: [
        'Content-Type','Authorization','X-Request-ID','X-API-Key',
        'X-Client-Env','X-Client-Version','X-Client-Type','X-Platform','X-Health-Check'
      ],
      exposedHeaders: ['X-Request-Id'],
      maxAge: 86400,
    }));

    this.app.use(compression());
    this.app.use(express.json({ limit: '10mb' }));
    this.app.use(express.urlencoded({ extended: true, limit: '10mb' }));
  }

  private registerDependentMiddleware(): void {
    this.app.use(requestIdMiddleware);
    this.app.use(defaultRateLimit);

    // Lightweight request logging
    this.app.use((req, res, next) => {
      const t0 = Date.now();
      res.on('finish', () => {
        logger.info('HTTP Request', {
          method: req.method,
          url: req.originalUrl || req.url,
          statusCode: res.statusCode,
          durationMs: Date.now() - t0,
          requestId: (req as any).requestId,
          userAgent: req.get('User-Agent'),
          ip: req.ip,
        });
      });
      next();
    });
  }

  private registerApiRoutes(): void {
    // Metrics (optional & resilient)
    if (config.monitoring.enableMetrics) {
      this.app.get('/metrics', asyncHandler(async (_req, res) => {
        let pool = { total: 0, idle: 0, waiting: 0 };
        try { pool = this.dbService.getPoolStats(); } catch {}
        const mu = process.memoryUsage();
        const lines = [
          '# HELP cwic_data_service_uptime_seconds Total uptime of the service',
          '# TYPE cwic_data_service_uptime_seconds counter',
          `cwic_data_service_uptime_seconds ${process.uptime()}`,
          '# HELP cwic_data_service_memory_usage_bytes Memory usage in bytes',
          '# TYPE cwic_data_service_memory_usage_bytes gauge',
          `cwic_data_service_memory_usage_bytes{type="rss"} ${mu.rss}`,
          `cwic_data_service_memory_usage_bytes{type="heapTotal"} ${mu.heapTotal}`,
          `cwic_data_service_memory_usage_bytes{type="heapUsed"} ${mu.heapUsed}`,
          '# HELP cwic_data_service_db_connections Database connection pool stats',
          '# TYPE cwic_data_service_db_connections gauge',
          `cwic_data_service_db_connections{state="total"} ${pool.total}`,
          `cwic_data_service_db_connections{state="idle"} ${pool.idle}`,
          `cwic_data_service_db_connections{state="waiting"} ${pool.waiting}`,
        ];
        res.set('Content-Type','text/plain; version=0.0.4');
        res.send(lines.join('\n'));
      }));
    }

    /**
     * IMPORTANT:
     * The API gateway rewrites '^/api' → '' before forwarding here.
     * So we mount WITHOUT '/api' prefixes.
     *
     * e.g. Frontend calls: GET /api/data-sources
     *      Gateway forwards to this service as: GET /data-sources
     */
    this.app.use('/data-sources', dataSourceRoutes);
    this.app.use('/assets',       assetRoutes);
    this.app.use('/sources',      sourceRoutes);

    // Human-friendly index at root (also works when gateway forwards /api → '/')
    const indexHandler: express.RequestHandler = (_req, res) => {
      res.json({
        service: 'CWIC Data Service',
        version: process.env.npm_package_version || '1.0.0',
        docs: {
          '/data-sources': 'Data source management',
          '/assets':       'Data asset management',
          '/sources':      'Source configuration',
          '/health':       'Liveness (no deps)',
          '/ready':        'Readiness (checks DB)',
          '/metrics':      'Prometheus metrics',
        },
      });
    };
    this.app.get('/', indexHandler);

    // For convenience if someone hits /api directly against this service (not required through gateway)
    this.app.get('/api', indexHandler);
  }

  private registerErrorHandlers(): void {
    this.app.use(notFoundHandler);
    this.app.use(errorHandler); // catch-all → JSON (prevents empty-reply)
  }

  /**
   * Initialize dependencies (DB migrations + connectivity).
   * Do NOT throw; keep server alive so /health remains 200.
   */
  public async initialize(): Promise<void> {
    try {
      logger.info('Initializing database (migrations + connectivity)...');
      await this.dbService.runMigrations();
      const dbHealth = await this.dbService.healthCheck();
      if (dbHealth.status !== 'healthy') {
        throw new Error(`Database is not healthy: ${dbHealth.error}`);
      }
      this.ready = true;
      logger.info('Database initialized; service READY.');
    } catch (err) {
      this.ready = false;
      logger.error('App initialization failed (service NOT ready):', err);
      // swallow—/ready will report 503, /health stays 200
    }
  }

  public getExpressApp(): express.Application { return this.app; }
  public getDbService(): DatabaseService { return this.dbService; }
}

export default App;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\config\env.ts
--------------------------------------------------------------------------------
// backend/data-service/src/config/env.ts
import dotenv from 'dotenv';
import path from 'path';
import { URL } from 'url';

dotenv.config({ path: path.resolve(process.cwd(), '.env') });

export interface Config {
  server: {
    port: number;
    host: string;
    env: string;
    corsOrigin: string | string[];
    serviceName: string;
  };
  database: {
    url: string;
    host: string;
    port: number;
    name: string;
    user: string;
    password: string;
    ssl: boolean;
    poolMax: number;
    poolMin: number;
    idleTimeout: number;
    connectionTimeout: number;
  };
  security: {
    jwtSecret: string;
    jwtExpiresIn: string;
    helmet: {
      contentSecurityPolicy: boolean;
      crossOriginEmbedderPolicy: boolean;
    };
  };
  monitoring: {
    enableMetrics: boolean;
    enableHealthCheck: boolean;
  };
  logging: {
    level: string;
    enableFileLogging: boolean;
  };
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

function coerceBool(v: unknown, def = false): boolean {
  const s = String(v ?? '').trim().toLowerCase();
  if (!s) return def;
  return s === '1' || s === 'true' || s === 'yes' || s === 'on';
}

function coerceInt(v: unknown, def: number): number {
  const n = Number(v);
  return Number.isFinite(n) ? n : def;
}

/** Accept CSV or leave string; if empty, default to dev origins */
function resolveCorsOrigin(raw?: string): string | string[] {
  if (!raw) return ['http://localhost:5173', 'http://localhost:3000', 'http://localhost:8000'];
  const parts = raw.split(',').map((o) => o.trim()).filter(Boolean);
  return parts.length <= 1 ? (parts[0] ?? '') : parts;
}

/** Strip accidental "DATABASE_URL=" prefix and surrounding quotes */
function normalizeDatabaseUrl(input?: string): string {
  let v = (input ?? '').trim();
  if (!v) return '';
  if (/^DATABASE_URL=/i.test(v)) v = v.replace(/^DATABASE_URL=/i, '').trim();
  if ((v.startsWith('"') && v.endsWith('"')) || (v.startsWith("'") && v.endsWith("'"))) {
    v = v.slice(1, -1);
  }
  return v;
}

/** Parse PostgreSQL/MySQL/MSSQL-style URL */
function parseDatabaseUrl(url: string): {
  host: string;
  port: number;
  database: string;
  user: string;
  password: string;
} {
  const normalized = normalizeDatabaseUrl(url);
  try {
    const u = new URL(normalized);
    return {
      host: u.hostname,
      port: parseInt(u.port) || 5432,
      database: u.pathname.replace(/^\//, ''),
      user: decodeURIComponent(u.username),
      password: decodeURIComponent(u.password),
    };
  } catch {
    throw new Error(`Invalid DATABASE_URL format: ${normalized}`);
  }
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ load & build config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const rawDbUrl = process.env.DATABASE_URL || 'postgresql://cwic_user:cwic_secure_pass@localhost:5432/cwic_platform';
const databaseUrl = normalizeDatabaseUrl(rawDbUrl);
const parsedDb = parseDatabaseUrl(databaseUrl);

export const config: Config = {
  server: {
    port: coerceInt(process.env.PORT, 3002),
    host: process.env.HOST || '0.0.0.0',
    env: process.env.NODE_ENV || 'development',
    corsOrigin: resolveCorsOrigin(process.env.CORS_ORIGIN),
    serviceName: process.env.SERVICE_NAME || 'data-service',
  },
  database: {
    url: databaseUrl,
    host: parsedDb.host,
    port: parsedDb.port,
    name: parsedDb.database,
    user: parsedDb.user,
    password: parsedDb.password,
    ssl: coerceBool(process.env.DB_SSL, process.env.NODE_ENV === 'production'),
    poolMax: coerceInt(process.env.DB_POOL_MAX, 20),
    poolMin: coerceInt(process.env.DB_POOL_MIN, 2),
    idleTimeout: coerceInt(process.env.DB_IDLE_TIMEOUT, 30_000),
    connectionTimeout: coerceInt(process.env.DB_CONNECTION_TIMEOUT, 10_000),
  },
  security: {
    jwtSecret: process.env.JWT_SECRET || 'your-super-secret-jwt-key-change-this-in-production',
    jwtExpiresIn: process.env.JWT_EXPIRES_IN || '7d',
    helmet: {
      contentSecurityPolicy: process.env.NODE_ENV === 'production',
      crossOriginEmbedderPolicy: false,
    },
  },
  monitoring: {
    enableMetrics: coerceBool(process.env.ENABLE_METRICS, process.env.NODE_ENV === 'production'),
    enableHealthCheck: process.env.ENABLE_HEALTH_CHECK !== 'false',
  },
  logging: {
    level: process.env.LOG_LEVEL || (process.env.NODE_ENV === 'production' ? 'info' : 'debug'),
    enableFileLogging: coerceBool(process.env.ENABLE_FILE_LOGGING, process.env.NODE_ENV === 'production'),
  },
};

export const isProduction = config.server.env === 'production';

/** Small shim for modules that expect `env.JWT_SECRET` etc. */
export const env = {
  NODE_ENV: config.server.env,
  PORT: String(config.server.port),
  HOST: config.server.host,
  SERVICE_NAME: config.server.serviceName,
  JWT_SECRET: config.security.jwtSecret,
  JWT_EXPIRES_IN: config.security.jwtExpiresIn,
  DATABASE_URL: config.database.url,
  LOG_LEVEL: config.logging.level,
  CORS_ORIGIN: Array.isArray(config.server.corsOrigin)
    ? config.server.corsOrigin.join(',')
    : config.server.corsOrigin,
} as const;

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ validation & logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

export const validateConfig = (): void => {
  if (!config.database.url) throw new Error('DATABASE_URL environment variable is required');

  // Re-validate URL (throws with normalized string if bad)
  parseDatabaseUrl(config.database.url);

  // Ports
  if (config.server.port < 1 || config.server.port > 65535) {
    throw new Error('Server port must be between 1 and 65535');
  }
  if (config.database.port < 1 || config.database.port > 65535) {
    throw new Error('Database port must be between 1 and 65535');
  }

  // JWT secret length in prod
  if (isProduction && config.security.jwtSecret.length < 32) {
    throw new Error('JWT secret must be at least 32 characters long in production');
  }

  // Pool sizes
  if (config.database.poolMax < config.database.poolMin) {
    throw new Error('Database pool max must be greater than or equal to pool min');
  }

  // CORS origins are URLs if provided as strings; arrays can include plain origins
  const origins = Array.isArray(config.server.corsOrigin)
    ? config.server.corsOrigin
    : [config.server.corsOrigin];
  origins.forEach((origin) => {
    if (!origin) return;
    try {
      // Allow wildcard-like strings only if exactly "*"
      if (origin === '*') return;
      new URL(origin);
    } catch {
      throw new Error(`Invalid CORS_ORIGIN URL: ${origin}`);
    }
  });
};

export const logConfig = (): void => {
  console.log('ðŸ”§ Service Configuration:');
  console.log(`   Service: ${config.server.serviceName}`);
  console.log(`   Port: ${config.server.port}`);
  console.log(`   Host: ${config.server.host}`);
  console.log(`   Environment: ${config.server.env}`);
  console.log(
    `   CORS Origin: ${
      Array.isArray(config.server.corsOrigin) ? config.server.corsOrigin.join(', ') : config.server.corsOrigin
    }`
  );
  console.log(`   Database: ${config.database.host}:${config.database.port}/${config.database.name}`);
  console.log(`   Database User: ${config.database.user}`);
  console.log(`   SSL: ${config.database.ssl ? 'enabled' : 'disabled'}`);
  console.log(`   Pool Max: ${config.database.poolMax}`);
  console.log(`   Pool Min: ${config.database.poolMin}`);
  console.log(`   Metrics: ${config.monitoring.enableMetrics ? 'enabled' : 'disabled'}`);
  console.log(`   Log Level: ${config.logging.level}`);
  console.log('');
};



--------------------------------------------------------------------------------
FILE: backend\data-service\src\controllers\AssetController.ts
--------------------------------------------------------------------------------
// backend/data-service/src/controllers/AssetController.ts
import { Request, Response } from 'express';
import { AssetService } from '../services/AssetService';
import { logger, loggerUtils } from '../utils/logger';

export interface AssetRequest extends Request {
  user?: {
    id: string;
    email: string;
    role: string;
  };
}

export interface Column {
  name: string;
  type: string;
  nullable: boolean;
  primaryKey: boolean;
  foreignKey?: { table: string; column: string };
  description?: string;
  tags?: string[];
}

export interface Asset {
  id: string;
  name: string;
  type:
    | 'table'
    | 'view'
    | 'procedure'
    | 'function'
    | 'schema'
    | 'file'
    | 'api_endpoint'
    | 'stream'
    | 'model';
  dataSourceId: string;
  schemaName?: string;
  tableName?: string;
  description?: string;
  columns?: Column[];
  tags?: string[];
  status: 'active' | 'inactive' | 'deprecated';
  classification?: 'public' | 'internal' | 'confidential' | 'restricted';
  createdAt: Date;
  updatedAt: Date;
  metadata?: {
    rowCount?: number;
    size?: string;
    lastAccessed?: Date;
    sensitivity?: 'public' | 'internal' | 'confidential' | 'restricted';
  };
}

type LineageDirection = 'upstream' | 'downstream' | 'both';

type ListFilters = {
  search?: string;
  type?: string;
  dataSourceId?: string;
  status?: string;
  tags?: string[];
  sensitivity?: string;
};

type Pagination = { page: number; limit: number };

export class AssetController {
  private assetService: AssetService;

  constructor() {
    this.assetService = new AssetService();
  }

  /**
   * GET /api/assets
   */
  public getAllAssets = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const {
        page = '1',
        limit = '20',
        search,
        type,
        dataSourceId,
        status,
        tags,
        sensitivity,
      } = req.query;

      const filters: ListFilters = {
        search: typeof search === 'string' ? search : undefined,
        type: typeof type === 'string' ? type : undefined,
        dataSourceId: typeof dataSourceId === 'string' ? dataSourceId : undefined,
        status: typeof status === 'string' ? status : undefined,
        tags:
          typeof tags === 'string'
            ? tags
                .split(',')
                .map((s) => s.trim())
                .filter(Boolean)
            : undefined,
        sensitivity: typeof sensitivity === 'string' ? sensitivity : undefined,
      };

      const pagination: Pagination = {
        page: Number(page) || 1,
        limit: Number(limit) || 20,
      };

      const t0 = Date.now();
      const result = await this.assetService.getAssets(filters, pagination);
      const dt = Date.now() - t0;

      loggerUtils.logDbOperation('select', 'assets', dt, true);

      res.status(200).json({
        success: true,
        data: result.assets,
        pagination: {
          page: result.page,
          limit: result.limit,
          total: result.total,
          totalPages: result.totalPages,
        },
        meta: {
          processingTime: `${dt}ms`,
          filters,
        },
      });
    } catch (error) {
      logger.error('Error fetching assets:', error);
      res.status(500).json({
        success: false,
        error: 'FETCH_ASSETS_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * GET /api/assets/:id
   */
  public getAssetById = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;

      const t0 = Date.now();
      const asset = await this.assetService.getAssetById(id);
      const dt = Date.now() - t0;

      if (!asset) {
        res.status(404).json({ success: false, error: 'NOT_FOUND', message: 'Asset not found' });
        return;
      }

      loggerUtils.logDbOperation('select', 'assets', dt, true);

      res.status(200).json({
        success: true,
        data: asset,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error fetching asset:', error);
      res.status(500).json({
        success: false,
        error: 'FETCH_ASSET_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * GET /api/assets/:id/schema
   */
  public getAssetSchema = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;

      const t0 = Date.now();
      const schema = await this.assetService.getAssetSchema(id);
      const dt = Date.now() - t0;

      if (!schema) {
        res.status(404).json({ success: false, error: 'NOT_FOUND', message: 'Asset schema not found' });
        return;
      }

      res.status(200).json({
        success: true,
        data: schema,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error fetching asset schema:', error);
      res.status(500).json({
        success: false,
        error: 'FETCH_SCHEMA_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * GET /api/assets/:id/lineage
   */
  public getAssetLineage = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const direction = (req.query.direction as LineageDirection) || 'both';

      const t0 = Date.now();
      const lineage = await this.assetService.getAssetLineage(id, direction);
      const dt = Date.now() - t0;

      res.status(200).json({
        success: true,
        data: lineage,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error fetching asset lineage:', error);
      res.status(500).json({
        success: false,
        error: 'FETCH_LINEAGE_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * GET /api/assets/:id/profile
   */
  public getAssetProfile = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;

      const t0 = Date.now();
      const profile = await this.assetService.getAssetProfile(id);
      const dt = Date.now() - t0;

      if (!profile) {
        res.status(404).json({
          success: false,
          error: 'NOT_FOUND',
          message: 'Asset profile not found',
        });
        return;
      }

      res.status(200).json({
        success: true,
        data: profile,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error fetching asset profile:', error);
      res.status(500).json({
        success: false,
        error: 'FETCH_PROFILE_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * POST /api/assets
   */
  public createAsset = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const assetData = req.body;

      const t0 = Date.now();
      const newAsset = await this.assetService.createAsset(assetData);
      const dt = Date.now() - t0;

      loggerUtils.logDbOperation('insert', 'assets', dt, true);
      logger.info(`Asset created: ${newAsset?.id}`, { userId: req.user?.id });

      res.status(201).json({
        success: true,
        data: newAsset,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error creating asset:', error);
      res.status(500).json({
        success: false,
        error: 'CREATE_ASSET_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * PUT /api/assets/:id
   */
  public updateAsset = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const updateData = req.body;

      const t0 = Date.now();
      const updatedAsset = await this.assetService.updateAsset(id, updateData);
      const dt = Date.now() - t0;

      if (!updatedAsset) {
        res.status(404).json({ success: false, error: 'NOT_FOUND', message: 'Asset not found' });
        return;
      }

      loggerUtils.logDbOperation('update', 'assets', dt, true);
      logger.info(`Asset updated: ${id}`, { userId: req.user?.id });

      res.status(200).json({
        success: true,
        data: updatedAsset,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error updating asset:', error);
      res.status(500).json({
        success: false,
        error: 'UPDATE_ASSET_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * PUT /api/assets/:id/classification
   */
  public updateClassification = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const { classification } = req.body as { classification: Asset['classification'] };

      const t0 = Date.now();
      const updated =
        typeof (this.assetService as any).updateClassification === 'function'
          ? await (this.assetService as any).updateClassification(id, { classification })
          : await this.assetService.updateAsset(id, { classification });
      const dt = Date.now() - t0;

      if (!updated) {
        res.status(404).json({ success: false, error: 'NOT_FOUND', message: 'Asset not found' });
        return;
      }

      loggerUtils.logDbOperation('update', 'assets', dt, true);

      res.status(200).json({
        success: true,
        data: updated,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error updating asset classification:', error);
      res.status(500).json({
        success: false,
        error: 'UPDATE_CLASSIFICATION_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * POST /api/assets/:id/tag
   */
  public addTags = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const { tags } = req.body as { tags: string[] };

      if (!Array.isArray(tags)) {
        res
          .status(400)
          .json({ success: false, error: 'VALIDATION_ERROR', message: 'Tags must be an array' });
        return;
      }

      const t0 = Date.now();
      const result = await this.assetService.addTags(id, tags);
      const dt = Date.now() - t0;

      loggerUtils.logDbOperation('update', 'asset_tags', dt, true);

      res.status(200).json({
        success: true,
        data: result,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error adding tags:', error);
      res.status(500).json({
        success: false,
        error: 'ADD_TAGS_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * DELETE /api/assets/:id/tag
   */
  public removeTags = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const { tags } = req.body as { tags: string[] };

      if (!Array.isArray(tags)) {
        res
          .status(400)
          .json({ success: false, error: 'VALIDATION_ERROR', message: 'Tags must be an array' });
        return;
      }

      const t0 = Date.now();
      const result = await this.assetService.removeTags(id, tags);
      const dt = Date.now() - t0;

      loggerUtils.logDbOperation('update', 'asset_tags', dt, true);

      res.status(200).json({
        success: true,
        data: result,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error removing tags:', error);
      res.status(500).json({
        success: false,
        error: 'REMOVE_TAGS_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * GET /api/assets/stats  (global)
   */
  public getAssetStats = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { period = '30d' } = req.query as { period?: string };

      const t0 = Date.now();
      // Prefer a dedicated overview method if it exists
      const stats =
        typeof (this.assetService as any).getOverviewStats === 'function'
          ? await (this.assetService as any).getOverviewStats(period)
          : await this.assetService.getAssets({}, { page: 1, limit: 1 }).then((r) => ({
              totalAssets: r.total,
            }));

      const dt = Date.now() - t0;

      res.status(200).json({
        success: true,
        data: stats,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error fetching asset stats:', error);
      res.status(500).json({
        success: false,
        error: 'FETCH_STATS_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * POST /api/assets/:id/scan
   */
  public scanAsset = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const { type = 'full', force = false } = req.body as {
        type?: 'full' | 'incremental' | 'schema_only' | 'profile_only';
        force?: boolean;
      };

      const t0 = Date.now();
      const result =
        typeof (this.assetService as any).scanAsset === 'function'
          ? await (this.assetService as any).scanAsset(id, { type, force })
          : await (this.assetService as any).syncAsset(id, { type, force }); // cast to any to avoid arity/type mismatch
      const dt = Date.now() - t0;

      logger.info(`Asset scan triggered: ${id}`, { userId: req.user?.id, type, force, duration: `${dt}ms` });

      res.status(200).json({
        success: true,
        data: result,
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error scanning asset:', error);
      res.status(500).json({
        success: false,
        error: 'SCAN_ASSET_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * DELETE /api/assets/:id
   */
  public deleteAsset = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { id } = req.params;

      const t0 = Date.now();
      const deleted =
        typeof (this.assetService as any).deleteAsset === 'function'
          ? await (this.assetService as any).deleteAsset(id)
          : await this.assetService.updateAsset(id, { status: 'inactive' as Asset['status'] });
      const dt = Date.now() - t0;

      if (!deleted) {
        res.status(404).json({ success: false, error: 'NOT_FOUND', message: 'Asset not found' });
        return;
      }

      loggerUtils.logDbOperation('delete', 'assets', dt, true);

      res.status(200).json({
        success: true,
        data: { id, deleted: true },
        meta: { processingTime: `${dt}ms` },
      });
    } catch (error) {
      logger.error('Error deleting asset:', error);
      res.status(500).json({
        success: false,
        error: 'DELETE_ASSET_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };

  /**
   * GET /api/assets/search
   */
  public searchAssets = async (req: AssetRequest, res: Response): Promise<void> => {
    try {
      const { q, type, limit = '20', page = '1' } = req.query;

      // Ensure `search` is a string (service expects a required string)
      const searchTerm: string = typeof q === 'string' ? q : '';

      const filters: { search: string; type?: string } = {
        search: searchTerm,
        type: typeof type === 'string' ? type : undefined,
      };

      const pagination: Pagination = {
        page: Number(page) || 1,
        limit: Number(limit) || 20,
      };

      const t0 = Date.now();
      const result = await this.assetService.searchAssets(filters, pagination);
      const dt = Date.now() - t0;

      loggerUtils.logDbOperation('select', 'assets', dt, true);

      res.status(200).json({
        success: true,
        data: result.assets,
        pagination: {
          page: result.page,
          limit: result.limit,
          total: result.total,
          totalPages: result.totalPages,
        },
        meta: {
          processingTime: `${dt}ms`,
          query: filters.search,
        },
      });
    } catch (error) {
      logger.error('Error searching assets:', error);
      res.status(500).json({
        success: false,
        error: 'SEARCH_ASSETS_FAILED',
        message: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  };
}

export default AssetController;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\controllers\DataSourceController.ts
--------------------------------------------------------------------------------
// backend/data-service/src/controllers/DataSourceController.ts
import { Request, Response } from 'express';
import { ConnectionTestService } from '../services/ConnectionTestService';
import { DataSourceService } from '../services/DataSourceService';
import { logger } from '../utils/logger';

// Pull the domain types your service expects
import type {
    DataSource,
    DataSourceFilters,
    DataSourceStatus,
    DataSourceType,
} from '../models/DataSource';

// Extend Request locally so we can read req.user without changing global types
type ReqWithUser = Request & { user?: { id?: string; email?: string; role?: string } };

// Normalize various aliases users may send from the UI into your canonical DataSourceType
const normalizeType = (t?: string): DataSourceType | undefined => {
  if (!t) return undefined;
  const x = String(t).toLowerCase();
  if (x === 'azure_sql' || x === 'azure-sql' || x === 'sqlserver' || x === 'sql-server') return 'mssql' as DataSourceType;
  return x as DataSourceType;
};

// Safely build filter object with correct types
const buildFilters = (q: Request['query']): DataSourceFilters => {
  const filters: DataSourceFilters = {};
  if (typeof q.status === 'string' && q.status.trim()) {
    filters.status = q.status.toLowerCase() as DataSourceStatus;
  }
  if (typeof q.type === 'string' && q.type.trim()) {
    filters.type = normalizeType(q.type) as DataSourceType;
  }
  return filters;
};

// Parse ints with sane defaults
const toInt = (v: unknown, def: number): number => {
  const n = Number(v);
  return Number.isFinite(n) && n > 0 ? n : def;
};

export class DataSourceController {
  private dataSourceService: DataSourceService;
  private connectionTestService: ConnectionTestService;

  constructor() {
    this.dataSourceService = new DataSourceService();
    this.connectionTestService = new ConnectionTestService();
  }

  // GET /api/data-sources
  getAllDataSources = async (req: Request, res: Response): Promise<void> => {
    try {
      const page = toInt(req.query.page, 1);
      const limit = toInt(req.query.limit, 10);
      const filters = buildFilters(req.query);

      const result = await this.dataSourceService.getAllDataSources({
        page,
        limit,
        filters,
      });

      res.json({
        success: true,
        data: result.dataSources,
        pagination: {
          page: result.page,
          limit: result.limit,
          total: result.total,
          totalPages: result.totalPages,
        },
      });
    } catch (error) {
      logger.error('Error fetching data sources:', error);
      res.status(500).json({
        success: false,
        error: { code: 'FETCH_ERROR', message: 'Failed to fetch data sources' },
      });
    }
  };

  // GET /api/data-sources/:id
  getDataSourceById = async (req: Request, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const dataSource = await this.dataSourceService.getDataSourceById(id);

      if (!dataSource) {
        res.status(404).json({
          success: false,
          error: { code: 'NOT_FOUND', message: 'Data source not found' },
        });
        return;
      }

      res.json({ success: true, data: dataSource });
    } catch (error) {
      logger.error('Error fetching data source:', error);
      res.status(500).json({
        success: false,
        error: { code: 'FETCH_ERROR', message: 'Failed to fetch data source' },
      });
    }
  };

  // POST /api/data-sources
  createDataSource = async (req: Request, res: Response): Promise<void> => {
    try {
      const dataSourceData = req.body;

      // Validate required fields
      const validationError = this.validateDataSourceData(dataSourceData);
      if (validationError) {
        res.status(400).json({
          success: false,
          error: { code: 'VALIDATION_ERROR', message: validationError },
        });
        return;
      }

      // Test connection before creating
      const connectionTest = await this.connectionTestService.testConnection(dataSourceData);
      if (!connectionTest.success) {
        res.status(400).json({
          success: false,
          error: {
            code: 'CONNECTION_FAILED',
            message: 'Failed to connect to data source',
            details: connectionTest.error,
          },
        });
        return;
      }

      const userId = (req as ReqWithUser).user?.id ?? 'system';

      const newDataSource = await this.dataSourceService.createDataSource({
        ...dataSourceData,
        type: normalizeType(dataSourceData.type) ?? dataSourceData.type, // normalize UI aliases
        status: 'connected',
        lastTestAt: new Date(),
        createdBy: userId,
      });

      res.status(201).json({ success: true, data: newDataSource });
    } catch (error) {
      logger.error('Error creating data source:', error);
      res.status(500).json({
        success: false,
        error: { code: 'CREATE_ERROR', message: 'Failed to create data source' },
      });
    }
  };

  // PUT /api/data-sources/:id
  updateDataSource = async (req: Request, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const updateData = req.body;
      const userId = (req as ReqWithUser).user?.id ?? 'system';

      const updatedDataSource = await this.dataSourceService.updateDataSource(id, {
        ...updateData,
        // normalize type if the UI sends azure-sql/sqlserver
        ...(updateData.type ? { type: normalizeType(updateData.type) } : {}),
        updatedBy: userId,
        updatedAt: new Date(),
      });

      if (!updatedDataSource) {
        res.status(404).json({
          success: false,
          error: { code: 'NOT_FOUND', message: 'Data source not found' },
        });
        return;
      }

      res.json({ success: true, data: updatedDataSource });
    } catch (error) {
      logger.error('Error updating data source:', error);
      res.status(500).json({
        success: false,
        error: { code: 'UPDATE_ERROR', message: 'Failed to update data source' },
      });
    }
  };

  // DELETE /api/data-sources/:id
  deleteDataSource = async (req: Request, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const deleted = await this.dataSourceService.deleteDataSource(id);

      if (!deleted) {
        res.status(404).json({
          success: false,
          error: { code: 'NOT_FOUND', message: 'Data source not found' },
        });
        return;
      }

      res.json({ success: true, message: 'Data source deleted successfully' });
    } catch (error) {
      logger.error('Error deleting data source:', error);
      res.status(500).json({
        success: false,
        error: { code: 'DELETE_ERROR', message: 'Failed to delete data source' },
      });
    }
  };

  // POST /api/data-sources/:id/test
  testConnection = async (req: Request, res: Response): Promise<void> => {
    try {
      const { id } = req.params;

      const dataSource = await this.dataSourceService.getDataSourceById(id);
      if (!dataSource) {
        res.status(404).json({
          success: false,
          error: { code: 'NOT_FOUND', message: 'Data source not found' },
        });
        return;
      }

      const testResult = await this.connectionTestService.testConnection(dataSource);

      // Build patch WITHOUT assigning null to lastError (omit on success)
      const patch: Partial<DataSource> = {
        lastTestAt: new Date(),
        status: (testResult.success ? 'connected' : 'error') as DataSourceStatus,
      };
      if (!testResult.success) {
        patch.lastError = testResult.error || 'Connection test failed';
      }

      await this.dataSourceService.updateDataSource(id, patch);

      res.json({
        success: true,
        data: {
          connectionStatus: testResult.success ? 'connected' : 'failed',
          responseTime: testResult.responseTime,
          details: testResult.details,
          error: testResult.error,
          testedAt: new Date(),
        },
      });
    } catch (error) {
      logger.error('Error testing connection:', error);
      res.status(500).json({
        success: false,
        error: { code: 'TEST_ERROR', message: 'Failed to test connection' },
      });
    }
  };

  // GET /api/data-sources/health
  getHealthSummary = async (_req: Request, res: Response): Promise<void> => {
    try {
      const summary = await this.dataSourceService.getHealthSummary();
      res.json({ success: true, data: summary });
    } catch (error) {
      logger.error('Error fetching health summary:', error);
      res.status(500).json({
        success: false,
        error: { code: 'HEALTH_ERROR', message: 'Failed to fetch health summary' },
      });
    }
  };

  // GET /api/data-sources/:id/schema
  getDataSourceSchema = async (req: Request, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const schema = await this.dataSourceService.getDataSourceSchema(id);
      res.json({ success: true, data: schema });
    } catch (error) {
      logger.error('Error fetching schema:', error);
      res.status(500).json({
        success: false,
        error: { code: 'SCHEMA_ERROR', message: 'Failed to fetch data source schema' },
      });
    }
  };

  // POST /api/data-sources/:id/sync
  syncDataSource = async (req: Request, res: Response): Promise<void> => {
    try {
      const { id } = req.params;
      const { force = false } = req.body ?? {};
      const syncResult = await this.dataSourceService.syncDataSource(id, { force: !!force });
      res.json({ success: true, data: syncResult });
    } catch (error) {
      logger.error('Error syncing data source:', error);
      res.status(500).json({
        success: false,
        error: { code: 'SYNC_ERROR', message: 'Failed to sync data source' },
      });
    }
  };

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ validation helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private validateDataSourceData(data: any): string | null {
    if (!data?.name) return 'Name is required';
    if (!data?.type) return 'Type is required';
    if (!data?.connectionConfig) return 'Connection configuration is required';

    const t = normalizeType(data.type);

    switch (t) {
      case 'postgresql':
      case 'mysql':
      case 'mssql':
        if (!data.connectionConfig.host) return 'Host is required for database connections';
        if (!data.connectionConfig.database) return 'Database name is required';
        break;

      case 'mongodb':
        if (!data.connectionConfig.connectionString && !data.connectionConfig.host) {
          return 'Connection string or host is required for MongoDB';
        }
        break;

      case 's3':
        if (!data.connectionConfig.bucket) return 'Bucket name is required for S3';
        break;

      case 'api':
        if (!data.connectionConfig.baseUrl) return 'Base URL is required for API connections';
        break;

      default:
        // Unknown types are validated in the service/factory path
        break;
    }

    return null;
  }
}



--------------------------------------------------------------------------------
FILE: backend\data-service\src\db.ts
--------------------------------------------------------------------------------
// backend/data-service/src/db.ts - Updated with correct imports
import { Pool, PoolConfig } from 'pg';
import { config } from './config/env';
import { logger } from './utils/logger';

export interface DatabaseConfig extends PoolConfig {
  host: string;
  port: number;
  database: string;
  user: string;
  password: string;
  ssl?: boolean | object;
  max?: number;
  min?: number;
  idleTimeoutMillis?: number;
  connectionTimeoutMillis?: number;
}

class Database {
  private pool: Pool;
  private static instance: Database;

  constructor() {
    const dbConfig: DatabaseConfig = {
      host: config.database.host,
      port: config.database.port,
      database: config.database.name,
      user: config.database.user,
      password: config.database.password,
      ssl: config.database.ssl ? { rejectUnauthorized: false } : false,
      max: config.database.poolMax,
      min: config.database.poolMin,
      idleTimeoutMillis: config.database.idleTimeout,
      connectionTimeoutMillis: config.database.connectionTimeout,
    };

    this.pool = new Pool(dbConfig);

    // Handle pool events
    this.pool.on('connect', (client) => {
      logger.debug('New database client connected', {
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
      });
    });

    this.pool.on('acquire', (client) => {
      logger.debug('Database client acquired from pool', {
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
        waitingClients: this.pool.waitingCount,
      });
    });

    this.pool.on('remove', (client) => {
      logger.debug('Database client removed from pool', {
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
      });
    });

    this.pool.on('error', (err, client) => {
      logger.error('Database pool error:', {
        error: err.message,
        stack: err.stack,
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
      });
    });

    logger.info('Database pool initialized', {
      host: dbConfig.host,
      port: dbConfig.port,
      database: dbConfig.database,
      maxConnections: dbConfig.max,
      minConnections: dbConfig.min,
      ssl: !!dbConfig.ssl,
    });
  }

  public static getInstance(): Database {
    if (!Database.instance) {
      Database.instance = new Database();
    }
    return Database.instance;
  }

  public getPool(): Pool {
    return this.pool;
  }

  public async query(text: string, params?: any[]): Promise<any> {
    const start = Date.now();
    const queryId = Math.random().toString(36).substring(7);
    
    try {
      logger.debug('Database query started', {
        queryId,
        query: text.substring(0, 100) + (text.length > 100 ? '...' : ''),
        paramCount: params?.length || 0,
      });

      const result = await this.pool.query(text, params);
      const duration = Date.now() - start;
      
      logger.debug('Database query completed', {
        queryId,
        duration: `${duration}ms`,
        rowCount: result.rowCount,
        command: result.command,
      });
      
      return result;
    } catch (error) {
      const duration = Date.now() - start;
      logger.error('Database query failed', {
        queryId,
        query: text.substring(0, 100) + (text.length > 100 ? '...' : ''),
        duration: `${duration}ms`,
        error: error instanceof Error ? error.message : 'Unknown error',
        paramCount: params?.length || 0,
      });
      throw error;
    }
  }

  public async transaction<T>(callback: (client: any) => Promise<T>): Promise<T> {
    const client = await this.pool.connect();
    const transactionId = Math.random().toString(36).substring(7);
    
    try {
      await client.query('BEGIN');
      logger.debug('Database transaction started', { transactionId });
      
      const result = await callback(client);
      
      await client.query('COMMIT');
      logger.debug('Database transaction committed', { transactionId });
      
      return result;
    } catch (error) {
      await client.query('ROLLBACK');
      logger.error('Database transaction rolled back', {
        transactionId,
        error: error instanceof Error ? error.message : 'Unknown error'
      });
      throw error;
    } finally {
      client.release();
      logger.debug('Database transaction client released', { transactionId });
    }
  }

  public async healthCheck(): Promise<{
    status: 'healthy' | 'unhealthy';
    latency?: number;
    connections?: {
      total: number;
      idle: number;
      waiting: number;
    };
    error?: string;
    timestamp: string;
  }> {
    const start = Date.now();
    
    try {
      // Simple connectivity test
      await this.pool.query('SELECT 1 as health_check');
      const latency = Date.now() - start;
      
      return {
        status: 'healthy',
        latency,
        connections: {
          total: this.pool.totalCount,
          idle: this.pool.idleCount,
          waiting: this.pool.waitingCount
        },
        timestamp: new Date().toISOString(),
      };
    } catch (error) {
      const latency = Date.now() - start;
      return {
        status: 'unhealthy',
        latency,
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString(),
      };
    }
  }

  public async deepHealthCheck(): Promise<{
    status: 'healthy' | 'unhealthy';
    checks: {
      connectivity: boolean;
      writeable: boolean;
      performant: boolean;
    };
    latency: number;
    error?: string;
  }> {
    const start = Date.now();
    const checks = {
      connectivity: false,
      writeable: false,
      performant: false,
    };

    try {
      // Test connectivity
      await this.pool.query('SELECT 1');
      checks.connectivity = true;

      // Test write capability (create temp table and drop it)
      const tempTableName = `health_check_${Date.now()}`;
      await this.pool.query(`CREATE TEMP TABLE ${tempTableName} (id INTEGER)`);
      await this.pool.query(`INSERT INTO ${tempTableName} VALUES (1)`);
      await this.pool.query(`DROP TABLE ${tempTableName}`);
      checks.writeable = true;

      const latency = Date.now() - start;
      
      // Check if performance is acceptable (under 1 second)
      checks.performant = latency < 1000;

      return {
        status: Object.values(checks).every(Boolean) ? 'healthy' : 'unhealthy',
        checks,
        latency,
      };
    } catch (error) {
      const latency = Date.now() - start;
      return {
        status: 'unhealthy',
        checks,
        latency,
        error: error instanceof Error ? error.message : 'Unknown error',
      };
    }
  }

  public getStats(): {
    totalCount: number;
    idleCount: number;
    waitingCount: number;
  } {
    return {
      totalCount: this.pool.totalCount,
      idleCount: this.pool.idleCount,
      waitingCount: this.pool.waitingCount,
    };
  }

  public async getDetailedStats(): Promise<{
    pool: {
      totalCount: number;
      idleCount: number;
      waitingCount: number;
      maxConnections: number;
      minConnections: number;
    };
    database: {
      version: string;
      size: string;
      activeConnections: number;
    };
  }> {
    try {
      // Get database version
      const versionResult = await this.pool.query('SELECT version()');
      const version = versionResult.rows[0].version;

      // Get database size
      const sizeResult = await this.pool.query(`
        SELECT pg_size_pretty(pg_database_size(current_database())) as size
      `);
      const size = sizeResult.rows[0].size;

      // Get active connections count
      const connectionsResult = await this.pool.query(`
        SELECT count(*) as active_connections 
        FROM pg_stat_activity 
        WHERE state = 'active'
      `);
      const activeConnections = parseInt(connectionsResult.rows[0].active_connections);

      return {
        pool: {
          totalCount: this.pool.totalCount,
          idleCount: this.pool.idleCount,
          waitingCount: this.pool.waitingCount,
          maxConnections: config.database.poolMax,
          minConnections: config.database.poolMin,
        },
        database: {
          version: version.split(' ')[1], // Extract version number
          size,
          activeConnections,
        },
      };
    } catch (error) {
      logger.error('Error getting detailed database stats:', error);
      throw error;
    }
  }

  public async close(): Promise<void> {
    try {
      logger.info('Closing database pool...');
      await this.pool.end();
      logger.info('Database pool closed successfully');
    } catch (error) {
      logger.error('Error closing database pool:', error);
      throw error;
    }
  }

  // Migration and schema management
  public async runMigrations(): Promise<void> {
    try {
      logger.info('Running database migrations...');

      // Create migrations table if it doesn't exist
      await this.pool.query(`
        CREATE TABLE IF NOT EXISTS migrations (
          id SERIAL PRIMARY KEY,
          name VARCHAR(255) NOT NULL UNIQUE,
          executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        )
      `);

      // List of migrations to run
      const migrations = [
        {
          name: '001_create_data_sources_table',
          sql: `
            CREATE TABLE IF NOT EXISTS data_sources (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              name VARCHAR(100) NOT NULL,
              type VARCHAR(50) NOT NULL,
              host VARCHAR(255),
              port INTEGER,
              database_name VARCHAR(100),
              username VARCHAR(100),
              password_encrypted TEXT,
              ssl BOOLEAN DEFAULT FALSE,
              connection_string TEXT,
              description TEXT,
              tags TEXT[] DEFAULT '{}',
              status VARCHAR(20) DEFAULT 'active',
              metadata JSONB DEFAULT '{}',
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
              updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            );
            CREATE INDEX IF NOT EXISTS idx_data_sources_type ON data_sources(type);
            CREATE INDEX IF NOT EXISTS idx_data_sources_status ON data_sources(status);
          `
        },
        {
          name: '002_create_assets_table',
          sql: `
            CREATE TABLE IF NOT EXISTS assets (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              name VARCHAR(100) NOT NULL,
              type VARCHAR(50) NOT NULL,
              data_source_id UUID NOT NULL REFERENCES data_sources(id) ON DELETE CASCADE,
              schema_name VARCHAR(100),
              table_name VARCHAR(100),
              description TEXT,
              columns JSONB DEFAULT '[]',
              tags TEXT[] DEFAULT '{}',
              status VARCHAR(20) DEFAULT 'active',
              metadata JSONB DEFAULT '{}',
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
              updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            );
            CREATE INDEX IF NOT EXISTS idx_assets_data_source_id ON assets(data_source_id);
            CREATE INDEX IF NOT EXISTS idx_assets_type ON assets(type);
            CREATE INDEX IF NOT EXISTS idx_assets_status ON assets(status);
          `
        },
        {
          name: '003_create_asset_lineage_table',
          sql: `
            CREATE TABLE IF NOT EXISTS asset_lineage (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              upstream_asset_id UUID NOT NULL REFERENCES assets(id) ON DELETE CASCADE,
              downstream_asset_id UUID NOT NULL REFERENCES assets(id) ON DELETE CASCADE,
              relationship_type VARCHAR(50) NOT NULL,
              description TEXT,
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            );
            CREATE INDEX IF NOT EXISTS idx_asset_lineage_upstream ON asset_lineage(upstream_asset_id);
            CREATE INDEX IF NOT EXISTS idx_asset_lineage_downstream ON asset_lineage(downstream_asset_id);
          `
        }
      ];

      for (const migration of migrations) {
        // Check if migration has already been run
        const result = await this.pool.query(
          'SELECT id FROM migrations WHERE name = $1',
          [migration.name]
        );

        if (result.rows.length === 0) {
          logger.info(`Running migration: ${migration.name}`);
          
          // Run migration in a transaction
          await this.transaction(async (client) => {
            await client.query(migration.sql);
            await client.query(
              'INSERT INTO migrations (name) VALUES ($1)',
              [migration.name]
            );
          });

          logger.info(`Migration completed: ${migration.name}`);
        } else {
          logger.debug(`Migration already applied: ${migration.name}`);
        }
      }

      logger.info('All migrations completed successfully');
    } catch (error) {
      logger.error('Migration failed:', error);
      throw error;
    }
  }
}

// Export singleton instance
export const db = Database.getInstance();
export default db;


--------------------------------------------------------------------------------
FILE: backend\data-service\src\db\pool.ts
--------------------------------------------------------------------------------
// backend/data-service/src/db/pool.ts
import { Pool } from "pg";

const {
  DATABASE_URL,
  DB_POOL_MAX = "20",
  DB_IDLE_TIMEOUT = "30000",
  DB_CONNECTION_TIMEOUT = "10000",
  DB_SSL = "false",
} = process.env;

if (!DATABASE_URL) {
  throw new Error("DATABASE_URL is not set for data-service");
}

export const pool = new Pool({
  connectionString: DATABASE_URL,
  max: Number(DB_POOL_MAX),
  idleTimeoutMillis: Number(DB_IDLE_TIMEOUT),
  connectionTimeoutMillis: Number(DB_CONNECTION_TIMEOUT),
  ssl: String(DB_SSL).toLowerCase() === "true" ? { rejectUnauthorized: false } : undefined,
});

pool.on("error", (err) => {
  // Do not crash the process on idle client error; log and continue.
  console.error("[pg] Pool error:", err);
});



--------------------------------------------------------------------------------
FILE: backend\data-service\src\middleware\auth.ts
--------------------------------------------------------------------------------
import type { NextFunction, Request, Response } from 'express';
import jwt from 'jsonwebtoken';
import { env } from '../config/env';
import { logger } from '../utils/logger';

/** Augment Express typings for req.user */
declare global {
  namespace Express {
    interface UserClaims {
      id?: string;
      sub?: string;
      email?: string;
      role?: string;
      permissions?: string[];
      [k: string]: any;
    }
    interface Request {
      user?: UserClaims;
    }
  }
}

const IS_PROD = process.env.NODE_ENV === 'production';
const JWT_SECRET = (env?.JWT_SECRET || process.env.JWT_SECRET || '').trim();
const DEV_BYPASS = !IS_PROD && (
  process.env.SKIP_AUTH === 'true' ||
  process.env.MOCK_AUTH === 'true'
);

/** Resolve secret or fail (prod). In dev we allow a safe default. */
function getJwtSecret(): string {
  if (JWT_SECRET) return JWT_SECRET;
  if (!IS_PROD) {
    logger.warn('[auth] JWT_SECRET not set; using dev fallback "devsecret".');
    return 'devsecret';
  }
  throw new Error('JWT is not configured (missing JWT_SECRET).');
}

function extractToken(req: Request): string | undefined {
  // Authorization: Bearer <token>
  const h = (req.headers.authorization || req.headers.Authorization) as string | undefined;
  if (typeof h === 'string' && h.startsWith('Bearer ')) return h.slice(7).trim();

  // Optional cookie fallback (useful for local dev)
  const cookie = req.headers.cookie;
  if (cookie) {
    const m = cookie.match(/(?:^|;\s*)access_token=([^;]+)/i);
    if (m) return decodeURIComponent(m[1]);
  }

  return undefined;
}

/** Build a normalized user object */
function normalizeUser(decoded: any): Express.UserClaims {
  const id = decoded.id || decoded.sub;
  return {
    id,
    sub: decoded.sub,
    email: decoded.email,
    role: decoded.role,
    permissions: Array.isArray(decoded.permissions) ? decoded.permissions : [],
    ...decoded,
  };
}

/** Hard auth: rejects on missing/invalid token (unless dev bypass enabled) */
export function authMiddleware(req: Request, res: Response, next: NextFunction) {
  if (DEV_BYPASS) return next(); // dev skip

  const token = extractToken(req);
  if (!token) {
    return res.status(401).json({ success: false, error: 'Access denied. No token provided.' });
  }

  try {
    const decoded = jwt.verify(token, getJwtSecret(), { algorithms: ['HS256'] });
    req.user = normalizeUser(decoded);
    return next();
  } catch (err: any) {
    if (err instanceof jwt.TokenExpiredError) {
      return res.status(401).json({ success: false, error: 'Access denied. Token expired.' });
    }
    if (err instanceof jwt.JsonWebTokenError) {
      return res.status(401).json({ success: false, error: 'Access denied. Invalid token.' });
    }
    return res.status(401).json({ success: false, error: 'Access denied. Token verification failed.' });
  }
}

/** Soft auth: attaches req.user if valid; otherwise continues unauthenticated */
export function optionalAuthMiddleware(req: Request, _res: Response, next: NextFunction) {
  if (DEV_BYPASS) return next(); // dev skip

  const token = extractToken(req);
  if (!token) return next();

  try {
    const decoded = jwt.verify(token, getJwtSecret(), { algorithms: ['HS256'] });
    req.user = normalizeUser(decoded);
  } catch {
    // ignore; proceed unauthenticated
  }
  return next();
}

/** Authorization helpers */
export function requireRole(roles: string | string[]) {
  const allowed = Array.isArray(roles) ? roles : [roles];
  return (req: Request, res: Response, next: NextFunction) => {
    if (!req.user) return res.status(401).json({ success: false, error: 'Authentication required.' });
    if (!req.user.role || !allowed.includes(req.user.role)) {
      return res.status(403).json({ success: false, error: 'Insufficient role.' });
    }
    next();
  };
}

export function requirePermission(perms: string | string[]) {
  const needed = Array.isArray(perms) ? perms : [perms];
  return (req: Request, res: Response, next: NextFunction) => {
    if (!req.user) return res.status(401).json({ success: false, error: 'Authentication required.' });
    const have = new Set(req.user.permissions || []);
    const ok = needed.some((p) => have.has(p));
    if (!ok) return res.status(403).json({ success: false, error: 'Insufficient permissions.' });
    next();
  };
}

/** Common permission constants (keep if you use them elsewhere) */
export const DATA_PERMISSIONS = {
  READ_DATA_SOURCES: 'read:data_sources',
  WRITE_DATA_SOURCES: 'write:data_sources',
  DELETE_DATA_SOURCES: 'delete:data_sources',
  READ_ASSETS: 'read:assets',
  WRITE_ASSETS: 'write:assets',
  DELETE_ASSETS: 'delete:assets',
  MANAGE_LINEAGE: 'manage:lineage',
  MANAGE_QUALITY: 'manage:quality',
  MANAGE_COMPLIANCE: 'manage:compliance',
  MANAGE_USERS: 'manage:users',
  MANAGE_ROLES: 'manage:roles',
  VIEW_ANALYTICS: 'view:analytics',
  EXPORT_DATA: 'export:data',
} as const;

export const requireAdmin = requireRole(['admin', 'super_admin']);
export const requireDataSourceRead   = requirePermission(DATA_PERMISSIONS.READ_DATA_SOURCES);
export const requireDataSourceWrite  = requirePermission(DATA_PERMISSIONS.WRITE_DATA_SOURCES);
export const requireDataSourceDelete = requirePermission(DATA_PERMISSIONS.DELETE_DATA_SOURCES);



--------------------------------------------------------------------------------
FILE: backend\data-service\src\middleware\error.ts
--------------------------------------------------------------------------------
// backend/data-service/src/middleware/error.ts
import { NextFunction, Request, Response } from 'express';
import { isProduction } from '../config/env';
import { logger } from '../utils/logger';

export interface ErrorResponse {
  success: false;
  error: {
    code: string;
    message: string;
    details?: any;
    timestamp: string;
    requestId?: string;
  };
}

export class AppError extends Error {
  public statusCode: number;
  public code: string;
  public isOperational: boolean;
  public details?: any;

  constructor(
    message: string,
    statusCode: number = 500,
    code: string = 'INTERNAL_ERROR',
    isOperational: boolean = true,
    details?: any
  ) {
    super(message);
    this.statusCode = statusCode;
    this.code = code;
    this.isOperational = isOperational;
    this.details = details;
    Error.captureStackTrace?.(this, this.constructor);
  }
}

/** Factory used by other middleware/controllers */
export const createError = (
  message: string,
  statusCode: number = 500,
  code: string = 'INTERNAL_ERROR',
  details?: any
): AppError => new AppError(message, statusCode, code, true, details);

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Common error types
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export class ValidationError extends AppError {
  constructor(message: string, details?: any) {
    super(message, 400, 'VALIDATION_ERROR', true, details);
  }
}
export class NotFoundError extends AppError {
  constructor(resource: string = 'Resource') {
    super(`${resource} not found`, 404, 'NOT_FOUND', true);
  }
}
export class UnauthorizedError extends AppError {
  constructor(message: string = 'Unauthorized') {
    super(message, 401, 'UNAUTHORIZED', true);
  }
}
export class ForbiddenError extends AppError {
  constructor(message: string = 'Forbidden') {
    super(message, 403, 'FORBIDDEN', true);
  }
}
export class ConflictError extends AppError {
  constructor(message: string = 'Resource already exists') {
    super(message, 409, 'CONFLICT', true);
  }
}
export class DatabaseError extends AppError {
  constructor(message: string, details?: any) {
    super(message, 500, 'DATABASE_ERROR', true, details);
  }
}
export class ExternalServiceError extends AppError {
  constructor(service: string, message: string, details?: any) {
    super(`External service error (${service}): ${message}`, 502, 'EXTERNAL_SERVICE_ERROR', true, details);
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Error handler
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const errorHandler = (
  error: Error | AppError,
  req: Request,
  res: Response,
  next: NextFunction
): void => {
  if (res.headersSent) return next(error);

  // Defaults
  let statusCode = 500;
  let code = 'INTERNAL_ERROR';
  let message = 'Internal server error';
  let details: any;

  if (error instanceof AppError) {
    statusCode = error.statusCode;
    code = error.code;
    message = error.message;
    details = error.details;
  } else if (error.name === 'ValidationError') {
    statusCode = 400;
    code = 'VALIDATION_ERROR';
    message = error.message;
  } else if (error.name === 'JsonWebTokenError') {
    statusCode = 401;
    code = 'INVALID_TOKEN';
    message = 'Invalid token';
  } else if (error.name === 'TokenExpiredError') {
    statusCode = 401;
    code = 'TOKEN_EXPIRED';
    message = 'Token expired';
  } else if ((error as any).code === '23505') {
    statusCode = 409;
    code = 'DUPLICATE_ENTRY';
    message = 'Resource already exists';
  } else if ((error as any).code === '23503') {
    statusCode = 400;
    code = 'FOREIGN_KEY_VIOLATION';
    message = 'Referenced resource does not exist';
  } else if ((error as any).code === 'ECONNREFUSED') {
    statusCode = 503;
    code = 'SERVICE_UNAVAILABLE';
    message = 'External service unavailable';
  } else if ((error as any).code === 'ENOTFOUND') {
    statusCode = 503;
    code = 'SERVICE_UNREACHABLE';
    message = 'External service unreachable';
  } else if ((error as any).code === 'ETIMEDOUT') {
    statusCode = 504;
    code = 'SERVICE_TIMEOUT';
    message = 'External service timeout';
  }

  const requestId =
    (req as any).requestId ||
    req.get('X-Request-ID') ||
    `req_${Date.now()}_${Math.random().toString(36).slice(2, 9)}`;

  const logPayload = {
    requestId,
    method: req.method,
    url: req.originalUrl || req.url,
    statusCode,
    code,
    message: (error as any).message,
    stack: (error as any).stack,
    body: req.body,
    params: req.params,
    query: req.query,
    userAgent: req.get('User-Agent'),
    ip: req.ip,
  };

  if (statusCode >= 500) {
    logger.error('Server error:', logPayload);
  } else {
    logger.warn('Client error:', logPayload);
  }

  const payload: ErrorResponse = {
    success: false,
    error: {
      code,
      message,
      timestamp: new Date().toISOString(),
      requestId,
    },
  };

  // Only include details/stack outside production
  if (!isProduction) {
    if (details) payload.error.details = details;
    payload.error.details = {
      ...(payload.error.details || {}),
      stack: (error as any).stack,
    };
  }

  res.status(statusCode).json(payload);
};

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const notFoundHandler = (req: Request, res: Response): void => {
  const requestId =
    (req as any).requestId ||
    req.get('X-Request-ID') ||
    `req_${Date.now()}_${Math.random().toString(36).slice(2, 9)}`;

  const payload: ErrorResponse = {
    success: false,
    error: {
      code: 'NOT_FOUND',
      message: `Route ${req.method} ${req.originalUrl} not found`,
      timestamp: new Date().toISOString(),
      requestId,
    },
  };

  logger.warn('Route not found:', {
    method: req.method,
    url: req.originalUrl,
    ip: req.ip,
    userAgent: req.get('User-Agent'),
    requestId,
  });

  res.status(404).json(payload);
};

// Wrap async route handlers
export const asyncHandler = (fn: (...args: any[]) => Promise<any>) => {
  return (req: Request, res: Response, next: NextFunction) => {
    Promise.resolve(fn(req, res, next)).catch(next);
  };
};

// Attach/propagate a request ID for tracing
export const requestIdMiddleware = (req: Request, res: Response, next: NextFunction): void => {
  const requestId = req.get('X-Request-ID') || `req_${Date.now()}_${Math.random().toString(36).slice(2, 9)}`;
  (req as any).requestId = requestId;
  res.set('X-Request-ID', requestId);
  next();
};

export default errorHandler;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\middleware\rateLimit.ts
--------------------------------------------------------------------------------
import type { RequestHandler } from 'express';
import rateLimit from 'express-rate-limit';
import { logger, loggerUtils } from '../utils/logger';

// helper to coerce the rate-limit handler to Express's RequestHandler
const asExpressHandler = (h: unknown) => h as unknown as RequestHandler;

/** Default rate limiting middleware for general use */
const _defaultRateLimit = rateLimit({
  windowMs: 15 * 60 * 1000,
  max: 1000,
  message: {
    success: false,
    error: 'RATE_LIMIT_EXCEEDED',
    message: 'Too many requests from this IP, please try again later.',
    retryAfter: '15 minutes',
  },
  standardHeaders: true,
  legacyHeaders: false,
  handler: (req: any, res: any) => {
    logger.warn('Default rate limit exceeded', {
      ip: req.ip,
      userAgent: req.get('User-Agent'),
      path: req.path,
    });
    res.status(429).json({
      success: false,
      error: 'RATE_LIMIT_EXCEEDED',
      message: 'Too many requests from this IP, please try again later.',
      retryAfter: res.get('Retry-After'),
      timestamp: new Date().toISOString(),
    });
  },
});
export const defaultRateLimit: RequestHandler = asExpressHandler(_defaultRateLimit);

/** Custom key generator */
const keyGenerator = (req: any): string => {
  const user = req.user;
  return user ? `user:${user.id}` : `ip:${req.ip}`;
};

/** Common response body */
const rateLimitMessage = (req: any, res: any) => {
  const user = req.user;
  loggerUtils.logAuth('rate_limit_exceeded', user?.id, req.ip, req.get('User-Agent'));
  return {
    success: false,
    error: 'RATE_LIMIT_EXCEEDED',
    message: 'Too many requests. Please try again later.',
    retryAfter: res.get('Retry-After'),
    timestamp: new Date().toISOString(),
  };
};

// Skip function (keep any to avoid cross-tree type binding)
const skipSuccessfulRequests = (req: any, res: any): boolean =>
  res.statusCode < 400 && req.path === '/health';

/** General rate limiting middleware */
export const rateLimitMiddleware: RequestHandler = asExpressHandler(
  rateLimit({
    windowMs: 15 * 60 * 1000,
    max: 100,
    standardHeaders: true,
    legacyHeaders: false,
    keyGenerator,
    skip: skipSuccessfulRequests,
    handler: (req: any, res: any) => {
      res.status(429).json(rateLimitMessage(req, res));
    },
  })
);

/** Strict auth limiter */
export const strictRateLimitMiddleware: RequestHandler = asExpressHandler(
  rateLimit({
    windowMs: 15 * 60 * 1000,
    max: 5,
    standardHeaders: true,
    legacyHeaders: false,
    keyGenerator: (req: any) => `auth:${req.ip}`,
    handler: (req: any, res: any) => {
      res.status(429).json(rateLimitMessage(req, res));
    },
  })
);

/** API limiter */
export const apiRateLimitMiddleware: RequestHandler = asExpressHandler(
  rateLimit({
    windowMs: 60 * 1000,
    max: 20,
    standardHeaders: true,
    legacyHeaders: false,
    keyGenerator,
    skip: skipSuccessfulRequests,
    handler: (req: any, res: any) => {
      res.status(429).json(rateLimitMessage(req, res));
    },
  })
);

/** Connection test limiter */
export const connectionTestRateLimitMiddleware: RequestHandler = asExpressHandler(
  rateLimit({
    windowMs: 5 * 60 * 1000,
    max: 10,
    standardHeaders: true,
    legacyHeaders: false,
    keyGenerator,
    handler: (req: any, res: any) => {
      res.status(429).json(rateLimitMessage(req, res));
    },
  })
);

/** Search limiter */
export const searchRateLimitMiddleware: RequestHandler = asExpressHandler(
  rateLimit({
    windowMs: 60 * 1000,
    max: 30,
    standardHeaders: true,
    legacyHeaders: false,
    keyGenerator,
    skip: skipSuccessfulRequests,
    handler: (req: any, res: any) => {
      res.status(429).json(rateLimitMessage(req, res));
    },
  })
);

/** Export limiter */
export const exportRateLimitMiddleware: RequestHandler = asExpressHandler(
  rateLimit({
    windowMs: 60 * 60 * 1000,
    max: 5,
    standardHeaders: true,
    legacyHeaders: false,
    keyGenerator,
    handler: (req: any, res: any) => {
      res.status(429).json(rateLimitMessage(req, res));
    },
  })
);

/** Upload limiter */
export const uploadRateLimitMiddleware: RequestHandler = asExpressHandler(
  rateLimit({
    windowMs: 15 * 60 * 1000,
    max: 10,
    standardHeaders: true,
    legacyHeaders: false,
    keyGenerator,
    handler: (req: any, res: any) => {
      res.status(429).json(rateLimitMessage(req, res));
    },
  })
);

/** Factory */
export const createRateLimit = (options: {
  windowMs: number;
  max: number;
  message?: string;
  skipSuccessfulRequests?: boolean;
}): RequestHandler =>
  asExpressHandler(
    rateLimit({
      windowMs: options.windowMs,
      max: options.max,
      message: options.message
        ? {
            success: false,
            error: 'RATE_LIMIT_EXCEEDED',
            message: options.message,
            timestamp: new Date().toISOString(),
          }
        : undefined,
      standardHeaders: true,
      legacyHeaders: false,
      keyGenerator,
      skip: options.skipSuccessfulRequests ? skipSuccessfulRequests : undefined,
      handler: (req: any, res: any) => {
        res.status(429).json(rateLimitMessage(req, res));
      },
    })
  );

export const RATE_LIMITS = {
  GENERAL: { windowMs: 15 * 60 * 1000, max: 100 },
  AUTH: { windowMs: 15 * 60 * 1000, max: 5 },
  API: { windowMs: 1 * 60 * 1000, max: 20 },
  CONNECTION_TEST: { windowMs: 5 * 60 * 1000, max: 10 },
  SEARCH: { windowMs: 1 * 60 * 1000, max: 30 },
  EXPORT: { windowMs: 60 * 60 * 1000, max: 5 },
  UPLOAD: { windowMs: 15 * 60 * 1000, max: 10 },
} as const;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\middleware\validation.ts
--------------------------------------------------------------------------------
// src/middleware/validation.ts - Data source validation rules
import { NextFunction, Request, Response } from 'express';
import { body, param, query, validationResult } from 'express-validator';

/**
 * Validate request and return errors if any
 */
export const validateRequest = (req: Request, res: Response, next: NextFunction) => {
  const errors = validationResult(req);
  
  if (!errors.isEmpty()) {
    const errorMessage = formatValidationError(errors.array());
    return res.status(400).json({
      success: false,
      error: 'VALIDATION_ERROR',
      message: errorMessage,
      details: errors.array(),
      timestamp: new Date().toISOString(),
    });
  }
  
  next();
};

/**
 * Validation error formatter
 */
export const formatValidationError = (errors: any[]): string => {
  return errors.map(error => {
    if (error.type === 'field') {
      return `${error.path}: ${error.msg}`;
    }
    return error.msg;
  }).join(', ');
};

/**
 * Data source validation rules
 */
export const validateDataSource = [
  body('name')
    .trim()
    .isLength({ min: 1, max: 100 })
    .withMessage('Name must be between 1 and 100 characters'),
  
  body('type')
    .isIn(['postgres', 'mysql', 'sqlserver', 'mongodb', 's3', 'api', 'file'])
    .withMessage('Invalid data source type'),
  
  body('host')
    .optional()
    .isLength({ min: 1, max: 255 })
    .withMessage('Host must be between 1 and 255 characters'),
  
  body('port')
    .optional()
    .isInt({ min: 1, max: 65535 })
    .withMessage('Port must be between 1 and 65535'),
  
  body('database')
    .optional()
    .isLength({ min: 1, max: 100 })
    .withMessage('Database name must be between 1 and 100 characters'),
  
  body('username')
    .optional()
    .isLength({ min: 1, max: 100 })
    .withMessage('Username must be between 1 and 100 characters'),
  
  body('password')
    .optional()
    .isLength({ min: 1, max: 255 })
    .withMessage('Password must be between 1 and 255 characters'),
  
  body('ssl')
    .optional()
    .isBoolean()
    .withMessage('SSL must be a boolean value'),
  
  body('description')
    .optional()
    .isLength({ max: 500 })
    .withMessage('Description must be less than 500 characters'),
  
  body('tags')
    .optional()
    .isArray()
    .withMessage('Tags must be an array'),
  
  body('tags.*')
    .optional()
    .isString()
    .trim()
    .isLength({ min: 1, max: 50 })
    .withMessage('Each tag must be between 1 and 50 characters'),

  validateRequest
];

/**
 * Data source update validation rules
 */
export const validateDataSourceUpdate = [
  param('id')
    .isUUID()
    .withMessage('Invalid data source ID'),
  
  body('name')
    .optional()
    .trim()
    .isLength({ min: 1, max: 100 })
    .withMessage('Name must be between 1 and 100 characters'),
  
  body('host')
    .optional()
    .isLength({ min: 1, max: 255 })
    .withMessage('Host must be between 1 and 255 characters'),
  
  body('port')
    .optional()
    .isInt({ min: 1, max: 65535 })
    .withMessage('Port must be between 1 and 65535'),
  
  body('database')
    .optional()
    .isLength({ min: 1, max: 100 })
    .withMessage('Database name must be between 1 and 100 characters'),
  
  body('username')
    .optional()
    .isLength({ min: 1, max: 100 })
    .withMessage('Username must be between 1 and 100 characters'),
  
  body('password')
    .optional()
    .isLength({ min: 1, max: 255 })
    .withMessage('Password must be between 1 and 255 characters'),
  
  body('ssl')
    .optional()
    .isBoolean()
    .withMessage('SSL must be a boolean value'),
  
  body('description')
    .optional()
    .isLength({ max: 500 })
    .withMessage('Description must be less than 500 characters'),
  
  body('tags')
    .optional()
    .isArray()
    .withMessage('Tags must be an array'),

  validateRequest
];

/**
 * Asset validation rules
 */
export const validateAsset = [
  body('name')
    .trim()
    .isLength({ min: 1, max: 100 })
    .withMessage('Name must be between 1 and 100 characters'),
  
  body('type')
    .isIn(['table', 'view', 'procedure', 'function', 'schema'])
    .withMessage('Invalid asset type'),
  
  body('dataSourceId')
    .isUUID()
    .withMessage('Invalid data source ID'),
  
  body('schemaName')
    .optional()
    .isLength({ min: 1, max: 100 })
    .withMessage('Schema name must be between 1 and 100 characters'),
  
  body('tableName')
    .optional()
    .isLength({ min: 1, max: 100 })
    .withMessage('Table name must be between 1 and 100 characters'),
  
  body('description')
    .optional()
    .isLength({ max: 500 })
    .withMessage('Description must be less than 500 characters'),
  
  body('tags')
    .optional()
    .isArray()
    .withMessage('Tags must be an array'),
  
  body('status')
    .optional()
    .isIn(['active', 'inactive', 'deprecated'])
    .withMessage('Invalid status'),

  validateRequest
];

/**
 * ID parameter validation
 */
export const validateId = [
  param('id')
    .isUUID()
    .withMessage('Invalid ID format'),
  
  validateRequest
];

/**
 * Pagination validation
 */
export const validatePagination = [
  query('page')
    .optional()
    .isInt({ min: 1 })
    .withMessage('Page must be a positive integer'),
  
  query('limit')
    .optional()
    .isInt({ min: 1, max: 100 })
    .withMessage('Limit must be between 1 and 100'),
  
  query('search')
    .optional()
    .isString()
    .trim()
    .isLength({ max: 100 })
    .withMessage('Search term must be less than 100 characters'),

  validateRequest
];

/**
 * Connection test validation
 */
export const validateConnectionTest = [
  body('type')
    .isIn(['postgres', 'mysql', 'sqlserver', 'mongodb', 's3', 'api'])
    .withMessage('Invalid connection type'),
  
  body('host')
    .isLength({ min: 1, max: 255 })
    .withMessage('Host is required and must be less than 255 characters'),
  
  body('port')
    .isInt({ min: 1, max: 65535 })
    .withMessage('Port must be between 1 and 65535'),
  
  body('database')
    .optional()
    .isLength({ min: 1, max: 100 })
    .withMessage('Database name must be between 1 and 100 characters'),
  
  body('username')
    .isLength({ min: 1, max: 100 })
    .withMessage('Username is required and must be less than 100 characters'),
  
  body('password')
    .isLength({ min: 1, max: 255 })
    .withMessage('Password is required and must be less than 255 characters'),

  validateRequest
];

/**
 * Tags validation
 */
export const validateTags = [
  body('tags')
    .isArray({ min: 1 })
    .withMessage('Tags must be a non-empty array'),
  
  body('tags.*')
    .isString()
    .trim()
    .isLength({ min: 1, max: 50 })
    .withMessage('Each tag must be between 1 and 50 characters'),

  validateRequest
];


--------------------------------------------------------------------------------
FILE: backend\data-service\src\models\Connection.ts
--------------------------------------------------------------------------------
// backend/data-service/src/models/Connection.ts
// Back-compat shim: re-export the canonical connection types from DataSource.
// This prevents drift between modules and ensures 'mssql' etc. exist here too.

export type {
  APIConnection, AzureBlobConnection, BigQueryConnection, ConnectionConfig, ConnectionTestResult, DatabricksConnection, ElasticsearchConnection, FileConnection, GCSConnection, KafkaConnection, MongoDBConnection, MSSQLConnection, MySQLConnection, OracleConnection, PostgreSQLConnection, RedisConnection, RedshiftConnection, S3Connection, SnowflakeConnection
} from './DataSource';




--------------------------------------------------------------------------------
FILE: backend\data-service\src\models\ConnectionRuntime.ts
--------------------------------------------------------------------------------
// backend/data-service/src/models/ConnectionRuntime.ts

export interface ConnectionPool {
  id: string;
  dataSourceId: string;
  poolSize: number;
  activeConnections: number;
  idleConnections: number;
  waitingConnections: number;
  createdAt: Date;
  lastUsedAt: Date;
}

export interface ConnectionMetrics {
  dataSourceId: string;
  totalConnections: number;
  successfulConnections: number;
  failedConnections: number;
  averageResponseTime: number;
  lastConnectionAt?: Date;
  lastFailureAt?: Date;
  lastErrorMessage?: string;
  uptime: number;       // percentage
  availability: number; // percentage
}

export interface ConnectionHistory {
  id: string;
  dataSourceId: string;
  connectionStatus: 'success' | 'failed' | 'timeout' | 'refused';
  responseTime?: number; // ms
  errorCode?: string;
  errorMessage?: string;
  connectionDetails?: {
    host?: string;
    port?: number;
    database?: string;
    serverVersion?: string;
    serverInfo?: Record<string, any>;
  };
  testedAt: Date;
  testedBy?: string;
  testType: 'manual' | 'scheduled' | 'health_check' | 'startup';
}

export interface ConnectionCredentials {
  username?: string;
  password?: string;
  apiKey?: string;
  secretKey?: string;
  accessToken?: string;
  refreshToken?: string;
  tokenType?: string;
  certificate?: string;
  privateKey?: string;
  passphrase?: string;
  accessKeyId?: string;
  secretAccessKey?: string;
  serviceAccountKey?: string;
  authMethod?: 'basic' | 'bearer' | 'oauth2' | 'certificate' | 'api-key' | 'iam';
  authUrl?: string;
  tokenUrl?: string;
  scope?: string[];
}

export interface RuntimeConnectionSecurity {
  ssl: boolean;
  sslMode?: 'disable' | 'allow' | 'prefer' | 'require' | 'verify-ca' | 'verify-full';
  sslCert?: string;
  sslKey?: string;
  sslRootCert?: string;
  verifyCertificate: boolean;
  allowSelfSigned: boolean;
  encryption?: 'none' | 'tls' | 'ssl';
  tlsVersion?: string;
}

export interface RuntimeConnectionOptions {
  connectionTimeout: number; // ms
  queryTimeout: number;      // ms
  idleTimeout: number;       // ms
  minConnections: number;
  maxConnections: number;
  acquireTimeout: number;    // ms
  retryAttempts: number;
  retryDelay: number;        // ms
  exponentialBackoff: boolean;
  keepAlive: boolean;
  keepAliveInterval?: number; // ms
  charset?: string;
  timezone?: string;
  applicationName?: string;
  customOptions: Record<string, any>;
}

export interface ConnectionTest {
  id: string;
  dataSourceId: string;
  testType: 'basic' | 'advanced' | 'schema' | 'performance';
  status: 'pending' | 'running' | 'completed' | 'failed';
  testQueries?: string[];
  expectedResults?: any[];
  performanceThresholds?: {
    maxResponseTime: number;
    minThroughput: number;
  };
  results?: {
    connectionSuccessful: boolean;
    responseTime: number;
    queryResults?: any[];
    performanceMetrics?: {
      throughput: number;
      latency: number;
      errorRate: number;
    };
    schemaInfo?: {
      databases?: string[];
      tables?: string[];
      columns?: string[];
    };
  };
  startedAt: Date;
  completedAt?: Date;
  duration?: number; // ms
  errorMessage?: string;
  testedBy: string;
}

export interface ConnectionFactory {
  createConnection(dataSourceId: string): Promise<any>;
  testConnection(dataSourceId: string): Promise<ConnectionTest>;
  closeConnection(dataSourceId: string): Promise<void>;
  getConnectionMetrics(dataSourceId: string): Promise<ConnectionMetrics>;
}

export const defaultConnectionOptions: RuntimeConnectionOptions = {
  connectionTimeout: 30000,
  queryTimeout: 60000,
  idleTimeout: 300000,
  minConnections: 1,
  maxConnections: 10,
  acquireTimeout: 30000,
  retryAttempts: 3,
  retryDelay: 1000,
  exponentialBackoff: true,
  keepAlive: true,
  keepAliveInterval: 30000,
  customOptions: {},
};

// helper (only for telemetry layer â€” separate from canonical config)
export function getDefaultPort(type: string): number {
  const ports: Record<string, number> = {
    postgresql: 5432,
    mysql: 3306,
    mssql: 1433,
    oracle: 1521,
    mongodb: 27017,
    redis: 6379,
    elasticsearch: 9200,
    cassandra: 9042,
    clickhouse: 8123,
  };
  return ports[type] || 0;
}



--------------------------------------------------------------------------------
FILE: backend\data-service\src\models\DataSource.ts
--------------------------------------------------------------------------------
/* ===========================================================================
 * DataSource models & helpers (canonical, production-ready)
 * =========================================================================== */

export type DataSourceType =
  | 'postgresql'
  | 'mysql'
  | 'mssql'
  | 'oracle'
  | 'mongodb'
  | 'redis'
  | 's3'
  | 'azure-blob'
  | 'gcs'
  | 'snowflake'
  | 'bigquery'
  | 'redshift'
  | 'databricks'
  | 'api'
  | 'file'
  | 'kafka'
  | 'elasticsearch';

export type DataSourceStatus =
  | 'pending'
  | 'connected'
  | 'disconnected'
  | 'error'
  | 'warning'
  | 'syncing'
  | 'testing';

/* ---------------------------------------------------------------------------
 * TLS / SSL
 * --------------------------------------------------------------------------- */
export interface ConnectionSecurity {
  enabled?: boolean;
  rejectUnauthorized?: boolean;
  ca?: string;
  cert?: string;
  key?: string;
  mode?: 'disable' | 'allow' | 'prefer' | 'require' | 'verify-ca' | 'verify-full';
}

/* ---------------------------------------------------------------------------
 * Shared base for all connectors
 * --------------------------------------------------------------------------- */
export interface CommonConnectionFields {
  host?: string;
  port?: number;
  database?: string | number;
  username?: string;
  password?: string;
  schema?: string;
  connectionString?: string;
  ssl?: boolean | ConnectionSecurity;

  timeout?: number;
  maxConnections?: number;
  retryAttempts?: number;
}

/* ---------------------------------------------------------------------------
 * Per-connector configs (discriminated by `type`)
 * --------------------------------------------------------------------------- */
// Relational
export interface PostgreSQLConnection extends CommonConnectionFields { type: 'postgresql' }
export interface MySQLConnection     extends CommonConnectionFields { type: 'mysql' }
export interface MSSQLConnection     extends CommonConnectionFields { type: 'mssql' }
export interface OracleConnection    extends CommonConnectionFields { type: 'oracle' }
export interface RedshiftConnection  extends CommonConnectionFields { type: 'redshift' }
export interface DatabricksConnection extends CommonConnectionFields {
  type: 'databricks';
  httpPath?: string;
  token?: string;
}

// Doc/KV
export interface MongoDBConnection extends CommonConnectionFields { type: 'mongodb' }
export interface RedisConnection extends CommonConnectionFields {
  type: 'redis';
  cluster?: boolean;
  nodes?: Array<{ host: string; port: number }>;
}

// Object storage
export interface S3Connection extends CommonConnectionFields {
  type: 's3';
  bucket?: string;
  region?: string;
  accessKeyId?: string;
  secretAccessKey?: string;
  prefix?: string;
}
export interface AzureBlobConnection extends CommonConnectionFields {
  type: 'azure-blob';
  container?: string;
  accountName?: string;
  accountKey?: string;
  connectionString?: string; // Azure style
  prefix?: string;
}
export interface GCSConnection extends CommonConnectionFields {
  type: 'gcs';
  bucket?: string;
  serviceAccountKey?: string; // JSON
  prefix?: string;
}

// SaaS DW / Analytics
export interface SnowflakeConnection extends CommonConnectionFields {
  type: 'snowflake';
  warehouse?: string;
  role?: string;
}
export interface BigQueryConnection extends CommonConnectionFields {
  type: 'bigquery';
  projectId?: string;
  dataset?: string;
  location?: string;
  serviceAccountKey?: string; // JSON
}

// REST / File / Streaming / Search
export interface APIConnection extends CommonConnectionFields {
  type: 'api';
  baseUrl?: string;
  apiKey?: string;
  headers?: Record<string, string>;
  authentication?: {
    type: 'basic' | 'bearer' | 'oauth2' | 'api-key';
    credentials: Record<string, string>;
  };
}
export interface FileConnection extends CommonConnectionFields {
  type: 'file';
  path?: string;
  format?: 'csv' | 'json' | 'parquet' | 'avro' | 'xml';
}
export interface KafkaConnection extends CommonConnectionFields {
  type: 'kafka';
  brokers?: string[];  // host:port
  topics?: string[];
  consumerGroup?: string;
  sasl?: {
    mechanism: 'plain' | 'scram-sha-256' | 'scram-sha-512';
    username: string;
    password: string;
  };
  ssl?: boolean | ConnectionSecurity;
}
export interface ElasticsearchConnection extends CommonConnectionFields { type: 'elasticsearch' }

/* ---------------------------------------------------------------------------
 * Canonical union
 * --------------------------------------------------------------------------- */
export type ConnectionConfig =
  | PostgreSQLConnection
  | MySQLConnection
  | MSSQLConnection
  | OracleConnection
  | MongoDBConnection
  | RedisConnection
  | S3Connection
  | AzureBlobConnection
  | GCSConnection
  | SnowflakeConnection
  | BigQueryConnection
  | RedshiftConnection
  | DatabricksConnection
  | APIConnection
  | FileConnection
  | KafkaConnection
  | ElasticsearchConnection;

/* ---------------------------------------------------------------------------
 * Filters & pagination
 * --------------------------------------------------------------------------- */
export interface DataSourceFilters {
  status?: DataSourceStatus;
  type?: DataSourceType;
  search?: string;
  tags?: string[];
}
export interface ListOptions {
  page?: number;
  limit?: number;
  filters?: DataSourceFilters;
}

/* ---------------------------------------------------------------------------
 * DataSource record
 * --------------------------------------------------------------------------- */
export interface DataSourceMetadata {
  version?: string;
  driver?: string;
  encoding?: string;
  timezone?: string;
  tableCount?: number;
  estimatedSize?: string;
  lastSchemaUpdate?: Date;
  customFields?: Record<string, any>;
}

export interface DataSource {
  id: string;
  name: string;
  description?: string;
  type: DataSourceType;
  status: DataSourceStatus;
  connectionConfig: ConnectionConfig;
  tags: string[];
  metadata: DataSourceMetadata;

  createdAt: Date;
  updatedAt: Date;
  createdBy: string;
  updatedBy?: string;
  deletedAt?: Date;

  lastTestAt?: Date;
  lastSyncAt?: Date;
  lastError?: string;

  responseTime?: number;
  availability?: number;

  syncEnabled?: boolean;
  syncSchedule?: string; // cron
  syncOptions?: {
    fullSync?: boolean;
    incrementalField?: string;
    batchSize?: number;
  };
}

/* ---------------------------------------------------------------------------
 * PostgreSQL DDL
 * --------------------------------------------------------------------------- */
export const createDataSourcesTable = `
  CREATE TABLE IF NOT EXISTS data_sources (
    id VARCHAR(255) PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    type VARCHAR(50) NOT NULL,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    connection_config JSONB NOT NULL,
    tags JSONB DEFAULT '[]',
    metadata JSONB DEFAULT '{}',

    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
    created_by VARCHAR(255) NOT NULL,
    updated_by VARCHAR(255),
    deleted_at TIMESTAMP,

    last_test_at TIMESTAMP,
    last_sync_at TIMESTAMP,
    last_error TEXT,

    response_time INTEGER,
    availability DECIMAL(5,2),

    sync_enabled BOOLEAN DEFAULT false,
    sync_schedule VARCHAR(255),
    sync_options JSONB DEFAULT '{}'
  );

  CREATE INDEX IF NOT EXISTS idx_data_sources_type ON data_sources(type);
  CREATE INDEX IF NOT EXISTS idx_data_sources_status ON data_sources(status);
  CREATE INDEX IF NOT EXISTS idx_data_sources_created_by ON data_sources(created_by);
  CREATE INDEX IF NOT EXISTS idx_data_sources_deleted_at ON data_sources(deleted_at);
  CREATE INDEX IF NOT EXISTS idx_data_sources_last_sync ON data_sources(last_sync_at);

  CREATE INDEX IF NOT EXISTS idx_data_sources_active
    ON data_sources(status, type)
    WHERE deleted_at IS NULL;
`;

/* ---------------------------------------------------------------------------
 * Result & stats
 * --------------------------------------------------------------------------- */
export interface ConnectionTestResult {
  success: boolean;
  responseTime?: number;
  error?: string;
  details?: {
    version?: string;
    serverInfo?: Record<string, any>;
    capabilities?: string[];
  };
  testedAt: Date;
}

export interface DataSourceStats {
  dataSourceId: string;
  totalTables: number;
  totalColumns: number;
  totalRows: number;
  estimatedSize: string;
  lastUpdated: Date;
  tableStats: TableStats[];
}
export interface TableStats {
  name: string;
  schema?: string;
  rowCount: number;
  columnCount: number;
  sizeBytes: number;
  lastModified?: Date;
}

/* ---------------------------------------------------------------------------
 * Helpers
 * --------------------------------------------------------------------------- */
export function validateDataSourceType(s: string): s is DataSourceType {
  const all: readonly DataSourceType[] = [
    'postgresql','mysql','mssql','oracle','mongodb','redis',
    's3','azure-blob','gcs','snowflake','bigquery','redshift',
    'databricks','api','file','kafka','elasticsearch',
  ];
  return (all as readonly string[]).includes(s);
}

export function validateDataSourceStatus(s: string): s is DataSourceStatus {
  const all: readonly DataSourceStatus[] = [
    'pending','connected','disconnected','error','warning','syncing','testing',
  ];
  return (all as readonly string[]).includes(s);
}

/** Map common aliases to canonical types */
export function normalizeDataSourceType(t: string): DataSourceType {
  const x = (t || '').toLowerCase();
  if (x === 'azure_sql' || x === 'azure-sql' || x === 'sqlserver' || x === 'sql-server') return 'mssql';
  return validateDataSourceType(x) ? (x as DataSourceType) : (x as DataSourceType);
}

export function getDefaultPort(type: DataSourceType): number | undefined {
  switch (type) {
    case 'postgresql': return 5432;
    case 'mysql': return 3306;
    case 'mssql': return 1433;
    case 'oracle': return 1521;
    case 'mongodb': return 27017;
    case 'redis': return 6379;
    case 'elasticsearch': return 9200;
    default: return undefined;
  }
}

/** Strong validation per connector (narrow, then read special props). */
export function validateConnectionConfig(type: DataSourceType, cfg: Partial<ConnectionConfig>): string[] {
  const errors: string[] = [];
  const t = normalizeDataSourceType(type);
  const need = (ok: boolean, msg: string) => { if (!ok) errors.push(msg); };

  switch (t) {
    case 'postgresql':
    case 'mysql':
    case 'mssql':
    case 'oracle':
    case 'redshift':
    case 'databricks': {
      need(!!(cfg.connectionString || cfg.host), 'Host or connection string is required');
      need(!!(cfg.connectionString || cfg.database), 'Database is required');
      break;
    }

    case 'mongodb': {
      need(!!(cfg.connectionString || cfg.host), 'Host or connection string is required');
      break;
    }

    case 'redis': {
      const rc = cfg as any;
      if (!rc.cluster) {
        need(!!(rc.host || rc.connectionString), 'Host or connection string is required');
      } else {
        need(Array.isArray(rc.nodes) && rc.nodes.length > 0, 'At least one cluster node is required');
      }
      break;
    }

    case 's3': {
      const sc = cfg as any;
      need(!!sc.bucket, 'Bucket is required');
      need(!!sc.region, 'Region is required');
      break;
    }

    case 'azure-blob': {
      const ac = cfg as any;
      need(!!ac.container, 'Container is required');
      break;
    }

    case 'gcs': {
      const gc = cfg as any;
      need(!!gc.bucket, 'Bucket is required');
      break;
    }

    case 'snowflake': {
      const sn = cfg as any;
      need(!!sn.host, 'Account URL/host is required');
      need(!!sn.username, 'Username is required');
      need(!!sn.database, 'Database is required');
      break;
    }

    case 'bigquery': {
      const bq = cfg as any;
      need(!!bq.serviceAccountKey, 'Service account key is required');
      break;
    }

    case 'api': {
      const api = cfg as any;
      need(!!api.baseUrl, 'Base URL is required');
      if (api.baseUrl) {
        try { new URL(api.baseUrl); } catch { errors.push('Base URL must be a valid URL'); }
      }
      break;
    }

    case 'file': {
      const fc = cfg as any;
      need(!!fc.path, 'File path is required');
      break;
    }

    case 'kafka': {
      const kc = cfg as any;
      need(Array.isArray(kc.brokers) && kc.brokers.length > 0, 'At least one broker is required');
      break;
    }

    case 'elasticsearch': {
      need(!!(cfg.host || cfg.connectionString), 'Host or connection string is required');
      break;
    }
  }

  if (cfg.port !== undefined && (cfg.port < 1 || cfg.port > 65535)) {
    errors.push('Port must be between 1 and 65535');
  }
  return errors;
}

/* ---------------------------------------------------------------------------
 * UI helpers
 * --------------------------------------------------------------------------- */
export function getDataSourceDisplayName(type: DataSourceType): string {
  const t = normalizeDataSourceType(type);
  const display: Record<DataSourceType, string> = {
    postgresql: 'PostgreSQL',
    mysql: 'MySQL',
    mssql: 'SQL Server',
    oracle: 'Oracle',
    mongodb: 'MongoDB',
    redis: 'Redis',
    s3: 'Amazon S3',
    'azure-blob': 'Azure Blob Storage',
    gcs: 'Google Cloud Storage',
    snowflake: 'Snowflake',
    bigquery: 'Google BigQuery',
    redshift: 'Amazon Redshift',
    databricks: 'Databricks',
    api: 'REST API',
    file: 'File System',
    kafka: 'Apache Kafka',
    elasticsearch: 'Elasticsearch',
  };
  return display[t] || t;
}

export function getDataSourceIcon(type: DataSourceType): string {
  const t = normalizeDataSourceType(type);
  const icons: Record<DataSourceType, string> = {
    postgresql: 'ðŸ˜',
    mysql: 'ðŸ¬',
    mssql: 'ðŸ¢',
    oracle: 'ðŸ›ï¸',
    mongodb: 'ðŸƒ',
    redis: 'ðŸ“¦',
    s3: 'â˜ï¸',
    'azure-blob': 'â˜ï¸',
    gcs: 'â˜ï¸',
    snowflake: 'â„ï¸',
    bigquery: 'ðŸ“Š',
    redshift: 'ðŸ“Š',
    databricks: 'ðŸ§±',
    api: 'ðŸ”Œ',
    file: 'ðŸ“',
    kafka: 'ðŸš€',
    elasticsearch: 'ðŸ”',
  };
  return icons[t] || 'ðŸ’¾';
}

export function getStatusColor(status: DataSourceStatus): string {
  const colors: Record<DataSourceStatus, string> = {
    pending: '#fbbf24',
    connected: '#10b981',
    disconnected: '#6b7280',
    error: '#ef4444',
    warning: '#f59e0b',
    syncing: '#3b82f6',
    testing: '#8b5cf6',
  };
  return colors[status];
}

/* ---------------------------------------------------------------------------
 * Safe creation template for UI flows
 * --------------------------------------------------------------------------- */
export type NewDataSourceTemplate =
  Partial<Omit<DataSource, 'connectionConfig'>> & {
    connectionConfig: Partial<ConnectionConfig>;
  };

export function createDataSourceTemplate(kind: DataSourceType): NewDataSourceTemplate {
  const t = normalizeDataSourceType(kind);
  return {
    type: t,
    status: 'pending',
    tags: [],
    metadata: {},
    connectionConfig: {
      port: getDefaultPort(t),
      timeout: 30_000,
      maxConnections: 10,
      retryAttempts: 3,
    },
    syncEnabled: false,
    syncOptions: {
      fullSync: true,
      batchSize: 1000,
    },
  };
}

/* ---------------------------------------------------------------------------
 * Type guards
 * --------------------------------------------------------------------------- */
export const isAPIConfig = (c: Partial<ConnectionConfig>): c is APIConnection =>
  (c as APIConnection).type === 'api' || typeof (c as APIConnection).baseUrl === 'string';

export const isS3Config = (c: Partial<ConnectionConfig>): c is S3Connection =>
  (c as S3Connection).type === 's3' || !!(c as S3Connection).accessKeyId || !!(c as S3Connection).secretAccessKey;

export const isSnowflakeConfig = (c: Partial<ConnectionConfig>): c is SnowflakeConnection =>
  (c as SnowflakeConnection).type === 'snowflake';

export const isBigQueryConfig = (c: Partial<ConnectionConfig>): c is BigQueryConnection =>
  (c as BigQueryConnection).type === 'bigquery' || typeof (c as BigQueryConnection).serviceAccountKey === 'string';

/* ---------------------------------------------------------------------------
 * Back-compat type aliases (old names used elsewhere)
 * --------------------------------------------------------------------------- */
export type APIConfig        = APIConnection;
export type BigQueryConfig   = BigQueryConnection;
export type S3Config         = S3Connection;
export type SnowflakeConfig  = SnowflakeConnection;
// For places that refer to "AzureSQLConnection", use MSSQL config
export type AzureSQLConnection = MSSQLConnection;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\repositories\dataSourcesRepo.ts
--------------------------------------------------------------------------------
// backend/data-service/src/repositories/dataSourcesRepo.ts
import { pool } from "../db/pool";

export type SortBy =
  | "name"
  | "type"
  | "status"
  | "createdAt"
  | "updatedAt"
  | "lastSyncAt"
  | "lastTestAt";

const SORT_MAP: Record<SortBy, string> = {
  name: "name",
  type: "type",
  status: "status",
  createdAt: "created_at",
  updatedAt: "updated_at",
  lastSyncAt: "last_sync_at",
  lastTestAt: "last_test_at",
};

export async function listDataSources(opts: {
  page?: number;
  limit?: number;
  sortBy?: SortBy;
  sortOrder?: "asc" | "desc";
}) {
  const page = Math.max(1, Number(opts.page ?? 1));
  const limit = Math.min(100, Math.max(1, Number(opts.limit ?? 20)));
  const offset = (page - 1) * limit;

  const sortBy = (opts.sortBy ?? "updatedAt") as SortBy;
  const sortCol = SORT_MAP[sortBy] ?? "updated_at";
  const sortOrder = (opts.sortOrder ?? "desc").toLowerCase() === "asc" ? "asc" : "desc";

  const client = await pool.connect();
  try {
    const countSql = `SELECT COUNT(*)::int AS total FROM data_sources WHERE deleted_at IS NULL`;
    const countRes = await client.query<{ total: number }>(countSql);
    const total = countRes.rows[0]?.total ?? 0;

    const sql = `
      SELECT
        id,
        name,
        description,
        type,
        status,
        connection_config           AS "connectionConfig",
        tags,
        metadata,
        created_at                  AS "createdAt",
        updated_at                  AS "updatedAt",
        created_by                  AS "createdBy",
        updated_by                  AS "updatedBy",
        deleted_at                  AS "deletedAt",
        last_test_at                AS "lastTestAt",
        last_sync_at                AS "lastSyncAt",
        last_error                  AS "lastError",
        response_time               AS "responseTime",
        availability,
        sync_enabled                AS "syncEnabled",
        sync_schedule               AS "syncSchedule",
        sync_options                AS "syncOptions"
      FROM data_sources
      WHERE deleted_at IS NULL
      ORDER BY ${sortCol} ${sortOrder}
      LIMIT $1 OFFSET $2
    `;
    const res = await client.query(sql, [limit, offset]);

    return {
      success: true as const,
      data: res.rows,
      page,
      limit,
      total,
      totalPages: Math.ceil(total / limit),
      sortBy,
      sortOrder,
    };
  } finally {
    client.release();
  }
}



--------------------------------------------------------------------------------
FILE: backend\data-service\src\routes\assets.ts
--------------------------------------------------------------------------------
// backend/data-service/src/routes/assets.ts
import { Router, type RequestHandler } from 'express';
import { body, param, query } from 'express-validator';
import { AssetController } from '../controllers/AssetController';
import { authMiddleware } from '../middleware/auth';
import { asyncHandler } from '../middleware/error';
import { createRateLimit } from '../middleware/rateLimit';
import { validateRequest } from '../middleware/validation';

const router = Router();
const assetController = new AssetController();

// Apply authentication to all routes
router.use(authMiddleware as unknown as RequestHandler);

// Validation schemas
const createAssetValidation = [
  body('name').isString().isLength({ min: 1, max: 255 }).withMessage('Name must be between 1 and 255 characters'),
  body('type')
    .isIn(['table', 'view', 'file', 'api_endpoint', 'stream', 'model'])
    .withMessage('Invalid asset type'),
  body('dataSourceId').isString().withMessage('Data source ID is required'),
  body('path').isString().withMessage('Asset path is required'),
  body('description').optional().isString().isLength({ max: 1000 }).withMessage('Description must be less than 1000 characters'),
  body('metadata').optional().isObject().withMessage('Metadata must be an object'),
  body('tags').optional().isArray().withMessage('Tags must be an array'),
  body('classification')
    .optional()
    .isIn(['public', 'internal', 'confidential', 'restricted'])
    .withMessage('Invalid classification level'),
];

const updateAssetValidation = [
  param('id').isString().withMessage('Asset ID must be a string'),
  body('name').optional().isString().isLength({ min: 1, max: 255 }).withMessage('Name must be between 1 and 255 characters'),
  body('description').optional().isString().isLength({ max: 1000 }).withMessage('Description must be less than 1000 characters'),
  body('metadata').optional().isObject().withMessage('Metadata must be an object'),
  body('tags').optional().isArray().withMessage('Tags must be an array'),
  body('classification')
    .optional()
    .isIn(['public', 'internal', 'confidential', 'restricted'])
    .withMessage('Invalid classification level'),
];

const paginationValidation = [
  query('page').optional().isInt({ min: 1 }).withMessage('Page must be a positive integer'),
  query('limit').optional().isInt({ min: 1, max: 100 }).withMessage('Limit must be between 1 and 100'),
  query('type')
    .optional()
    .isIn(['table', 'view', 'file', 'api_endpoint', 'stream', 'model'])
    .withMessage('Invalid asset type filter'),
  query('dataSourceId').optional().isString().withMessage('Data source ID must be a string'),
  query('classification')
    .optional()
    .isIn(['public', 'internal', 'confidential', 'restricted'])
    .withMessage('Invalid classification filter'),
  query('search').optional().isString().isLength({ min: 1, max: 100 }).withMessage('Search term must be between 1 and 100 characters'),
];

const idValidation = [param('id').isString().withMessage('Asset ID must be a string')];

// --- Rate limiters (cast to RequestHandler to avoid type incompatibilities) ---
const asHandler = (h: any) => h as unknown as RequestHandler;

const listAssetsLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 100 }));
const searchAssetsLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 50 }));
const getAssetStatsLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 30 }));
const getAssetLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 200 }));
const getSchemaLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 30 }));
const getLineageLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 20 }));
const getProfileLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 10 }));
const createAssetLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 20 }));
const scanAssetLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 5 }));
const tagOperationsLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 30 }));
const updateAssetLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 30 }));
const updateClassificationLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 20 }));
const deleteAssetLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 10 }));

/**
 * @route GET /api/assets
 * @desc Get all assets with pagination, filtering, and search
 * @access Private
 */
router.get('/', paginationValidation, validateRequest, listAssetsLimit, asyncHandler(assetController.getAllAssets));

/**
 * @route GET /api/assets/search
 * @desc Search assets by name, description, or metadata
 * @access Private
 */
router.get(
  '/search',
  [
    query('q').isString().isLength({ min: 1, max: 100 }).withMessage('Search query must be between 1 and 100 characters'),
    query('type')
      .optional()
      .isIn(['table', 'view', 'file', 'api_endpoint', 'stream', 'model'])
      .withMessage('Invalid asset type filter'),
    query('limit').optional().isInt({ min: 1, max: 50 }).withMessage('Limit must be between 1 and 50'),
  ],
  validateRequest,
  searchAssetsLimit,
  asyncHandler(assetController.searchAssets),
);

/**
 * @route GET /api/assets/stats
 * @desc Get asset statistics and overview (global)
 * @access Private
 */
router.get('/stats', getAssetStatsLimit, asyncHandler(assetController.getAssetStats));

/**
 * @route GET /api/assets/:id
 * @desc Get a specific asset by ID
 * @access Private
 */
router.get('/:id', idValidation, validateRequest, getAssetLimit, asyncHandler(assetController.getAssetById));

/**
 * @route GET /api/assets/:id/schema
 * @desc Get schema information for an asset
 * @access Private
 */
router.get('/:id/schema', idValidation, validateRequest, getSchemaLimit, asyncHandler(assetController.getAssetSchema));

/**
 * @route GET /api/assets/:id/lineage
 * @desc Get data lineage for an asset
 * @access Private
 */
router.get('/:id/lineage', idValidation, validateRequest, getLineageLimit, asyncHandler(assetController.getAssetLineage));

/**
 * @route GET /api/assets/:id/profile
 * @desc Get data profile for an asset
 * @access Private
 */
router.get('/:id/profile', idValidation, validateRequest, getProfileLimit, asyncHandler(assetController.getAssetProfile));

/**
 * @route GET /api/assets/:id/stats
 * @desc Get usage statistics for a specific asset
 * @access Private
 */
router.get('/:id/stats', idValidation, validateRequest, getAssetStatsLimit, asyncHandler(assetController.getAssetStats));

/**
 * @route POST /api/assets
 * @desc Create a new asset
 * @access Private
 */
router.post('/', createAssetValidation, validateRequest, createAssetLimit, asyncHandler(assetController.createAsset));

/**
 * @route POST /api/assets/:id/scan
 * @desc Trigger a scan/discovery for an asset
 * @access Private
 */
router.post(
  '/:id/scan',
  idValidation,
  [
    body('type').optional().isIn(['full', 'incremental', 'schema_only', 'profile_only']).withMessage('Invalid scan type'),
    body('force').optional().isBoolean().withMessage('Force must be a boolean'),
  ],
  validateRequest,
  scanAssetLimit,
  asyncHandler(assetController.scanAsset),
);

/**
 * @route POST /api/assets/:id/tags
 * @desc Add tags to an asset
 * @access Private
 */
router.post(
  '/:id/tags',
  idValidation,
  [
    body('tags').isArray({ min: 1 }).withMessage('Tags must be a non-empty array'),
    body('tags.*').isString().isLength({ min: 1, max: 50 }).withMessage('Each tag must be between 1 and 50 characters'),
  ],
  validateRequest,
  tagOperationsLimit,
  asyncHandler(assetController.addTags),
);

/**
 * @route PUT /api/assets/:id
 * @desc Update an asset
 * @access Private
 */
router.put('/:id', updateAssetValidation, validateRequest, updateAssetLimit, asyncHandler(assetController.updateAsset));

/**
 * @route PUT /api/assets/:id/classification
 * @desc Update asset classification
 * @access Private
 */
router.put(
  '/:id/classification',
  idValidation,
  [
    body('classification')
      .isIn(['public', 'internal', 'confidential', 'restricted'])
      .withMessage('Invalid classification level'),
    body('reason').optional().isString().isLength({ max: 500 }).withMessage('Reason must be less than 500 characters'),
  ],
  validateRequest,
  updateClassificationLimit,
  asyncHandler(assetController.updateClassification),
);

/**
 * @route DELETE /api/assets/:id
 * @desc Delete an asset (soft delete)
 * @access Private
 */
router.delete('/:id', idValidation, validateRequest, deleteAssetLimit, asyncHandler(assetController.deleteAsset));

/**
 * @route DELETE /api/assets/:id/tags
 * @desc Remove tags from an asset
 * @access Private
 */
router.delete(
  '/:id/tags',
  idValidation,
  [body('tags').isArray({ min: 1 }).withMessage('Tags must be a non-empty array'), body('tags.*').isString().withMessage('Each tag must be a string')],
  validateRequest,
  tagOperationsLimit,
  asyncHandler(assetController.removeTags),
);

export default router;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\routes\dataSources.ts
--------------------------------------------------------------------------------
// backend/data-service/src/routes/datasources.ts
import { Router, type RequestHandler } from 'express';
import { body, param, query } from 'express-validator';
import { DataSourceController } from '../controllers/DataSourceController';
import { authMiddleware } from '../middleware/auth';
import { asyncHandler } from '../middleware/error';
import { createRateLimit } from '../middleware/rateLimit';
import { validateRequest } from '../middleware/validation';

const router = Router();
const dataSourceController = new DataSourceController();

// Helper to coerce middlewares from different type trees to Express' RequestHandler
const asHandler = (h: any) => h as unknown as RequestHandler;

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Canonical enums & normalizers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Canonical types stored/used internally
const CANONICAL_TYPES = [
  'postgresql', 'mysql', 'mssql', 'oracle', 'mongodb', 'redis',
  's3', 'azure-blob', 'gcs', 'snowflake', 'bigquery', 'redshift',
  'databricks', 'api', 'file', 'kafka', 'elasticsearch',
] as const;
type CanonicalType = (typeof CANONICAL_TYPES)[number];

// Allow these on input (includes aliases)
const INPUT_TYPES: ReadonlySet<string> = new Set<string>([
  ...CANONICAL_TYPES,
  'postgres', // alias we want to accept and normalize
]);

function normalizeType(t?: string): CanonicalType | undefined {
  if (!t) return undefined;
  const lower = t.toLowerCase();
  const mapped = lower === 'postgres' ? 'postgresql' : lower;
  return (CANONICAL_TYPES as readonly string[]).includes(mapped)
    ? (mapped as CanonicalType)
    : undefined;
}

// Canonical statuses used by your FE/DataSource model
const CANONICAL_STATUSES = [
  'active', 'inactive', 'pending', 'error', 'testing',
] as const;
type CanonicalStatus = (typeof CANONICAL_STATUSES)[number];

// Accept legacy names on input
const INPUT_STATUSES: ReadonlySet<string> = new Set<string>([
  ...CANONICAL_STATUSES,
  'connected',       // â†’ active
  'disconnected',    // â†’ inactive
  'warning',         // â†’ pending (closest FE bucket)
  'syncing',         // you track syncing separately in UI; map to 'pending' for filters
]);

function normalizeStatus(s?: string): CanonicalStatus | undefined {
  if (!s) return undefined;
  const lower = s.toLowerCase();
  switch (lower) {
    case 'connected': return 'active';
    case 'disconnected': return 'inactive';
    case 'warning': return 'pending';
    case 'syncing': return 'pending'; // treat as pending for list filters
    case 'active':
    case 'inactive':
    case 'pending':
    case 'error':
    case 'testing':
      return lower;
    default:
      return undefined;
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Security â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
router.use(asHandler(authMiddleware));

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const createDataSourceValidation = [
  body('name')
    .isString().isLength({ min: 1, max: 255 })
    .withMessage('Name must be between 1 and 255 characters'),
  body('description')
    .optional().isString().isLength({ max: 1000 })
    .withMessage('Description must be less than 1000 characters'),

  // Accept alias on input, then normalize to canonical
  body('type')
    .isString()
    .custom((val) => INPUT_TYPES.has(String(val).toLowerCase()))
    .withMessage('Invalid data source type')
    .customSanitizer((val) => normalizeType(String(val))),

  body('connectionConfig')
    .isObject()
    .withMessage('Connection configuration must be an object'),
  body('tags')
    .optional().isArray()
    .withMessage('Tags must be an array'),
  body('metadata')
    .optional().isObject()
    .withMessage('Metadata must be an object'),
];

const updateDataSourceValidation = [
  param('id')
    .isString()
    .withMessage('ID must be a string'),

  body('name')
    .optional().isString().isLength({ min: 1, max: 255 })
    .withMessage('Name must be between 1 and 255 characters'),
  body('description')
    .optional().isString().isLength({ max: 1000 })
    .withMessage('Description must be less than 1000 characters'),

  // If provided, validate & normalize type
  body('type')
    .optional()
    .isString()
    .custom((val) => INPUT_TYPES.has(String(val).toLowerCase()))
    .withMessage('Invalid data source type')
    .customSanitizer((val) => normalizeType(String(val))),

  body('connectionConfig')
    .optional().isObject()
    .withMessage('Connection configuration must be an object'),
  body('tags')
    .optional().isArray()
    .withMessage('Tags must be an array'),
  body('metadata')
    .optional().isObject()
    .withMessage('Metadata must be an object'),
];

// List/search pagination & filters
const paginationValidation = [
  query('page')
    .optional().isInt({ min: 1 })
    .withMessage('Page must be a positive integer'),
  query('limit')
    .optional().isInt({ min: 1, max: 100 })
    .withMessage('Limit must be between 1 and 100'),

  // Accept legacy names and normalize
  query('status')
    .optional()
    .isString()
    .custom((val) => INPUT_STATUSES.has(String(val).toLowerCase()))
    .withMessage('Invalid status filter')
    .customSanitizer((val) => normalizeStatus(String(val))),

  // Accept alias type and normalize
  query('type')
    .optional()
    .isString()
    .custom((val) => INPUT_TYPES.has(String(val).toLowerCase()))
    .withMessage('Invalid type filter')
    .customSanitizer((val) => normalizeType(String(val))),
];

const idValidation = [
  param('id').isString().withMessage('ID must be a string'),
];

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Rate limits per route â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const listLimit    = asHandler(createRateLimit({ windowMs: 60_000, max: 100 }));
const healthLimit  = asHandler(createRateLimit({ windowMs: 60_000, max: 60  }));
const getByIdLimit = asHandler(createRateLimit({ windowMs: 60_000, max: 200 }));
const schemaLimit  = asHandler(createRateLimit({ windowMs: 60_000, max: 30  }));
const createLimit  = asHandler(createRateLimit({ windowMs: 60_000, max: 20  }));
const testLimit    = asHandler(createRateLimit({ windowMs: 60_000, max: 10  }));
const syncLimit    = asHandler(createRateLimit({ windowMs: 60_000, max: 5   }));
const updateLimit  = asHandler(createRateLimit({ windowMs: 60_000, max: 30  }));
const deleteLimit  = asHandler(createRateLimit({ windowMs: 60_000, max: 10  }));

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Routes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * @route GET /api/data-sources
 * @desc  Get all data sources with pagination and filtering
 * @access Private
 */
router.get(
  '/',
  paginationValidation,
  validateRequest,
  listLimit,
  asyncHandler(dataSourceController.getAllDataSources),
);

/**
 * @route GET /api/data-sources/health
 * @desc  Get health summary of all data sources
 * @access Private
 */
router.get(
  '/health',
  healthLimit,
  asyncHandler(dataSourceController.getHealthSummary),
);

/**
 * @route GET /api/data-sources/:id
 * @desc  Get a specific data source by ID
 * @access Private
 */
router.get(
  '/:id',
  idValidation,
  validateRequest,
  getByIdLimit,
  asyncHandler(dataSourceController.getDataSourceById),
);

/**
 * @route GET /api/data-sources/:id/schema
 * @desc  Get schema information for a data source
 * @access Private
 */
router.get(
  '/:id/schema',
  idValidation,
  validateRequest,
  schemaLimit,
  asyncHandler(dataSourceController.getDataSourceSchema),
);

/**
 * @route POST /api/data-sources
 * @desc  Create a new data source (type is normalized here)
 * @access Private
 */
router.post(
  '/',
  createDataSourceValidation,
  validateRequest,
  createLimit,
  asyncHandler(dataSourceController.createDataSource),
);

/**
 * @route POST /api/data-sources/:id/test
 * @desc  Test connection to a data source
 * @access Private
 */
router.post(
  '/:id/test',
  idValidation,
  validateRequest,
  testLimit,
  asyncHandler(dataSourceController.testConnection),
);

/**
 * @route POST /api/data-sources/:id/sync
 * @desc  Sync a data source (discover schema, update metadata)
 * @access Private
 */
router.post(
  '/:id/sync',
  idValidation,
  body('force').optional().isBoolean().withMessage('Force must be a boolean'),
  validateRequest,
  syncLimit,
  asyncHandler(dataSourceController.syncDataSource),
);

/**
 * @route PUT /api/data-sources/:id
 * @desc  Update a data source (type is normalized here if provided)
 * @access Private
 */
router.put(
  '/:id',
  updateDataSourceValidation,
  validateRequest,
  updateLimit,
  asyncHandler(dataSourceController.updateDataSource),
);

/**
 * @route DELETE /api/data-sources/:id
 * @desc  Delete a data source (soft delete)
 * @access Private
 */
router.delete(
  '/:id',
  idValidation,
  validateRequest,
  deleteLimit,
  asyncHandler(dataSourceController.deleteDataSource),
);

export default router;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\routes\sources.ts
--------------------------------------------------------------------------------
// backend/data-service/src/routes/sources.ts
import { Router, type RequestHandler } from 'express';
import { body, param, query } from 'express-validator';
import { DataSourceController } from '../controllers/DataSourceController';
import { authMiddleware } from '../middleware/auth';
import { asyncHandler } from '../middleware/error';
import { createRateLimit } from '../middleware/rateLimit';
import { validateDataSource, validateDataSourceUpdate, validateRequest } from '../middleware/validation';

const router = Router();
const dataSourceController = new DataSourceController();

// Helper to coerce mixed-type middlewares into Express' RequestHandler
const asHandler = (h: any) => h as unknown as RequestHandler;

// ðŸ” Auth on every route
router.use(asHandler(authMiddleware));

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Validation
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const paginationValidation = [
  query('page').optional().isInt({ min: 1 }).withMessage('Page must be a positive integer'),
  query('limit').optional().isInt({ min: 1, max: 100 }).withMessage('Limit must be between 1 and 100'),
  query('type')
    .optional()
    .isIn([
      'postgresql', 'mysql', 'mssql', 'oracle', 'mongodb', 'redis',
      's3', 'azure-blob', 'gcs', 'snowflake', 'bigquery', 'redshift',
      'databricks', 'api', 'file', 'kafka', 'elasticsearch',
    ])
    .withMessage('Invalid source type filter'),
  query('status')
    .optional()
    .isIn(['pending', 'connected', 'disconnected', 'error', 'warning', 'syncing', 'testing'])
    .withMessage('Invalid status filter'),
];

const idValidation = [param('id').isUUID().withMessage('Source ID must be a valid UUID')];

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Per-route rate limiters (use the factory; donâ€™t â€œcallâ€ a handler as a fn)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const listSourcesLimit          = asHandler(createRateLimit({ windowMs: 60_000, max: 100 }));
const healthLimit               = asHandler(createRateLimit({ windowMs: 60_000, max: 60 }));
const getSourceLimit            = asHandler(createRateLimit({ windowMs: 60_000, max: 200 }));
const schemaLimit               = asHandler(createRateLimit({ windowMs: 60_000, max: 30 }));
const createSourceLimit         = asHandler(createRateLimit({ windowMs: 60_000, max: 20 }));
const testLimit                 = asHandler(createRateLimit({ windowMs: 60_000, max: 10 }));
const discoverSourceLimit       = asHandler(createRateLimit({ windowMs: 60_000, max: 5 }));
const updateSourceLimit         = asHandler(createRateLimit({ windowMs: 60_000, max: 30 }));
const updateStatusLimit         = asHandler(createRateLimit({ windowMs: 60_000, max: 20 }));
const deleteSourceLimit         = asHandler(createRateLimit({ windowMs: 60_000, max: 10 }));

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Routes â€“ only methods that exist on DataSourceController
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * @route GET /api/sources
 * @desc Get all data sources with pagination & filters
 */
router.get(
  '/',
  paginationValidation,
  validateRequest,
  listSourcesLimit,
  asyncHandler(dataSourceController.getAllDataSources),
);

/**
 * @route GET /api/sources/health
 * @desc Health summary of all data sources
 */
router.get(
  '/health',
  healthLimit,
  asyncHandler(dataSourceController.getHealthSummary),
);

/**
 * @route GET /api/sources/:id
 * @desc Get a data source by ID
 */
router.get(
  '/:id',
  idValidation,
  validateRequest,
  getSourceLimit,
  asyncHandler(dataSourceController.getDataSourceById),
);

/**
 * @route GET /api/sources/:id/schema
 * @desc Get schema for a data source
 */
router.get(
  '/:id/schema',
  idValidation,
  validateRequest,
  schemaLimit,
  asyncHandler(dataSourceController.getDataSourceSchema),
);

/**
 * @route POST /api/sources
 * @desc Create a new data source
 */
router.post(
  '/',
  validateDataSource,
  validateRequest,
  createSourceLimit,
  asyncHandler(dataSourceController.createDataSource),
);

/**
 * @route POST /api/sources/:id/test
 * @desc Test connection for an existing source
 * (if you want a â€œtest without creatingâ€ endpoint, add a controller method first)
 */
router.post(
  '/:id/test',
  idValidation,
  validateRequest,
  testLimit,
  asyncHandler(dataSourceController.testConnection),
);

/**
 * @route POST /api/sources/:id/discover
 * @desc Trigger discovery/sync on a source
 */
router.post(
  '/:id/discover',
  idValidation,
  [
    body('force').optional().isBoolean().withMessage('Force must be a boolean'),
  ],
  validateRequest,
  discoverSourceLimit,
  asyncHandler(dataSourceController.syncDataSource),
);

/**
 * @route PUT /api/sources/:id
 * @desc Update a data source
 */
router.put(
  '/:id',
  validateDataSourceUpdate,
  validateRequest,
  updateSourceLimit,
  asyncHandler(dataSourceController.updateDataSource),
);

/**
 * @route PUT /api/sources/:id/status
 * @desc Update status for a data source (reuses updateDataSource)
 */
router.put(
  '/:id/status',
  idValidation,
  [
    body('status')
      .isIn(['pending', 'connected', 'disconnected', 'error', 'warning', 'syncing', 'testing'])
      .withMessage('Invalid status'),
    body('reason').optional().isString().isLength({ max: 500 }).withMessage('Reason must be less than 500 characters'),
  ],
  validateRequest,
  updateStatusLimit,
  asyncHandler(dataSourceController.updateDataSource),
);

/**
 * @route DELETE /api/sources/:id
 * @desc Delete a data source
 */
router.delete(
  '/:id',
  idValidation,
  validateRequest,
  deleteSourceLimit,
  asyncHandler(dataSourceController.deleteDataSource),
);

export default router;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\server.ts
--------------------------------------------------------------------------------
// src/server.ts
import 'dotenv/config';
import App from './app';
import { config, logConfig } from './config/env';
import { logger } from './utils/logger';

type AppLifecycle = {
  initialize?: () => Promise<void> | void;
  cleanup?: () => Promise<void> | void;
  getExpressApp: () => import('express').Express;
};

class Server {
  private app: App;
  private server: any;

  constructor() {
    this.app = new (App as any)();
  }

  public async start(): Promise<void> {
    try {
      // Log env at boot (safe if logConfig is a no-op)
      try { logConfig?.(); } catch (e) { logger.warn('[data-service] logConfig failed', e as any); }

      // Start listening first so /health is available ASAP
      const host = config?.server?.host || process.env.HOST || '0.0.0.0';
      const port = Number(config?.server?.port || process.env.PORT || 3002);

      const expressApp = (this.app as unknown as AppLifecycle).getExpressApp();
      this.server = expressApp.listen(port, host, () => {
        logger.info(`🚀 data-service listening on http://${host}:${port}`);
        logger.info(`📍 liveness:  GET /health`);
        logger.info(`📍 readiness: GET /ready`);
        logger.info(`🌱 NODE_ENV=${process.env.NODE_ENV || 'development'}`);
      });

      // Visibility if bind fails
      this.server.on('error', (err: any) => {
        logger.error('[data-service] HTTP server error', {
          message: err?.message,
          code: err?.code,
          stack: err?.stack,
        });
      });

      // Initialize dependencies (DB/redis/etc.) without blocking health
      const lifecycle = this.app as unknown as AppLifecycle;
      Promise.resolve(lifecycle.initialize?.()).catch((err: any) => {
        logger.error('[data-service] App init failed', { message: err?.message, stack: err?.stack });
      });

      // Graceful shutdown
      const shutdown = (signal: string) => {
        logger.warn(`[data-service] ${signal} received. Shutting down...`);
        this.server?.close(async (err?: Error) => {
          if (err) {
            logger.error('[data-service] Error during close', { message: err.message, stack: err.stack });
            process.exit(1);
            return;
          }
          logger.info('[data-service] HTTP server closed');

          try {
            if (typeof lifecycle.cleanup === 'function') {
              await Promise.resolve(lifecycle.cleanup());
            }
          } catch (e: any) {
            logger.error('[data-service] cleanup failed', { message: e?.message, stack: e?.stack });
          } finally {
            process.exit(0);
          }
        });

        // Failsafe timer
        setTimeout(() => {
          logger.warn('[data-service] Forced shutdown after 10s');
          process.exit(1);
        }, 10_000).unref();
      };

      process.on('SIGINT', () => shutdown('SIGINT'));
      process.on('SIGTERM', () => shutdown('SIGTERM'));
      process.on('uncaughtException', (err: any) => {
        logger.error('[data-service] Uncaught exception', { message: err?.message, stack: err?.stack });
      });
      process.on('unhandledRejection', (reason: any) => {
        logger.error('[data-service] Unhandled rejection', {
          reason: reason?.message || String(reason),
          stack: reason?.stack,
        });
      });
    } catch (e: any) {
      logger.error('[data-service] Startup failed', { message: e?.message, stack: e?.stack });
      process.exit(1);
    }
  }
}

if (require.main === module) {
  new Server()
    .start()
    .catch((err) => {
      logger.error('[data-service] Failed to start server', { message: err?.message, stack: err?.stack });
      process.exit(1);
    });
}

export default Server;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\services\AssetService.ts
--------------------------------------------------------------------------------
// src/services/AssetService.ts
import { Asset, Column } from '../controllers/AssetController';
import { logger } from '../utils/logger';
import { DatabaseService } from './DatabaseService';

export interface AssetFilters {
  search?: string;
  type?: string;
  dataSourceId?: string;
  status?: string;
  tags?: string[];
  sensitivity?: string;
}

export interface AssetPagination {
  page: number;
  limit: number;
}

export interface AssetResult {
  assets: Asset[];
  total: number;
  page: number;
  limit: number;
  totalPages: number;
}

export interface AssetLineage {
  upstream: Asset[];
  downstream: Asset[];
  relationships: LineageRelationship[];
}

export interface LineageRelationship {
  fromAsset: string;
  toAsset: string;
  type: 'table_to_view' | 'procedure_call' | 'data_flow' | 'dependency';
  description?: string;
}

export interface AssetStats {
  accessCount: number;
  lastAccessed: Date | null;
  avgQueryTime: number;
  dataVolume: {
    current: number;
    trend: 'increasing' | 'decreasing' | 'stable';
    changePercent: number;
  };
  qualityScore: number;
  usageMetrics: {
    date: string;
    count: number;
  }[];
}

export class AssetService {
  private db: DatabaseService;

  constructor() {
    this.db = new DatabaseService();
  }

  /**
   * Get assets with filtering and pagination
   */
  public async getAssets(filters: AssetFilters, pagination: AssetPagination): Promise<AssetResult> {
    try {
      const { page, limit } = pagination;
      const offset = (page - 1) * limit;

      // Build dynamic WHERE clause
      const conditions: string[] = [];
      const params: any[] = [];
      let paramIndex = 1;

      if (filters.search) {
        conditions.push(`(a.name ILIKE $${paramIndex} OR a.description ILIKE $${paramIndex})`);
        params.push(`%${filters.search}%`);
        paramIndex++;
      }

      if (filters.type) {
        conditions.push(`a.type = $${paramIndex}`);
        params.push(filters.type);
        paramIndex++;
      }

      if (filters.dataSourceId) {
        conditions.push(`a.data_source_id = $${paramIndex}`);
        params.push(filters.dataSourceId);
        paramIndex++;
      }

      if (filters.status) {
        conditions.push(`a.status = $${paramIndex}`);
        params.push(filters.status);
        paramIndex++;
      }

      if (filters.sensitivity) {
        conditions.push(`a.metadata->>'sensitivity' = $${paramIndex}`);
        params.push(filters.sensitivity);
        paramIndex++;
      }

      if (filters.tags && filters.tags.length > 0) {
        conditions.push(`a.tags && $${paramIndex}`);
        params.push(filters.tags);
        paramIndex++;
      }

      const whereClause = conditions.length > 0 ? `WHERE ${conditions.join(' AND ')}` : '';

      // Count query
      const countQuery = `
        SELECT COUNT(*) as total
        FROM assets a
        ${whereClause}
      `;

      // Data query
      const dataQuery = `
        SELECT 
          a.*,
          ds.name as data_source_name,
          ds.type as data_source_type
        FROM assets a
        LEFT JOIN data_sources ds ON a.data_source_id = ds.id
        ${whereClause}
        ORDER BY a.updated_at DESC
        LIMIT $${paramIndex} OFFSET $${paramIndex + 1}
      `;

      params.push(limit, offset);

      const [countResult, dataResult] = await Promise.all([
        this.db.query(countQuery, params.slice(0, -2)),
        this.db.query(dataQuery, params)
      ]);

      const total = parseInt(countResult.rows[0].total);
      const totalPages = Math.ceil(total / limit);

      const assets: Asset[] = dataResult.rows.map(row => ({
        id: row.id,
        name: row.name,
        type: row.type,
        dataSourceId: row.data_source_id,
        schemaName: row.schema_name,
        tableName: row.table_name,
        description: row.description,
        columns: row.columns || [],
        tags: row.tags || [],
        status: row.status,
        createdAt: new Date(row.created_at),
        updatedAt: new Date(row.updated_at),
        metadata: row.metadata || {},
      }));

      return {
        assets,
        total,
        page,
        limit,
        totalPages,
      };
    } catch (error) {
      logger.error('Error in getAssets:', error);
      throw new Error('Failed to fetch assets');
    }
  }

  /**
   * Get asset by ID
   */
  public async getAssetById(id: string): Promise<Asset | null> {
    try {
      const query = `
        SELECT 
          a.*,
          ds.name as data_source_name,
          ds.type as data_source_type
        FROM assets a
        LEFT JOIN data_sources ds ON a.data_source_id = ds.id
        WHERE a.id = $1
      `;

      const result = await this.db.query(query, [id]);

      if (result.rows.length === 0) {
        return null;
      }

      const row = result.rows[0];
      return {
        id: row.id,
        name: row.name,
        type: row.type,
        dataSourceId: row.data_source_id,
        schemaName: row.schema_name,
        tableName: row.table_name,
        description: row.description,
        columns: row.columns || [],
        tags: row.tags || [],
        status: row.status,
        createdAt: new Date(row.created_at),
        updatedAt: new Date(row.updated_at),
        metadata: row.metadata || {},
      };
    } catch (error) {
      logger.error('Error in getAssetById:', error);
      throw new Error('Failed to fetch asset');
    }
  }

  /**
   * Create a new asset
   */
  public async createAsset(assetData: Partial<Asset>): Promise<Asset> {
    try {
      const {
        name,
        type,
        dataSourceId,
        schemaName,
        tableName,
        description,
        columns,
        tags,
        status = 'active',
        metadata = {}
      } = assetData;

      const query = `
        INSERT INTO assets (
          name, type, data_source_id, schema_name, table_name,
          description, columns, tags, status, metadata
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        RETURNING *
      `;

      const values = [
        name,
        type,
        dataSourceId,
        schemaName,
        tableName,
        description,
        JSON.stringify(columns || []),
        tags || [],
        status,
        JSON.stringify(metadata)
      ];

      const result = await this.db.query(query, values);
      return this.mapRowToAsset(result.rows[0]);
    } catch (error) {
      logger.error('Error in createAsset:', error);
      throw new Error('Failed to create asset');
    }
  }

  /**
   * Search assets
   */
  public async searchAssets(
    filters: { search: string; type?: string },
    pagination: AssetPagination
  ): Promise<AssetResult> {
    try {
      const { page, limit } = pagination;
      const offset = (page - 1) * limit;

      // Build search query with full-text search
      const conditions: string[] = [];
      const params: any[] = [];
      let paramIndex = 1;

      if (filters.search) {
        conditions.push(`(
          to_tsvector('english', coalesce(a.name, '') || ' ' || coalesce(a.description, '')) 
          @@ plainto_tsquery('english', $${paramIndex})
          OR a.name ILIKE $${paramIndex + 1}
          OR a.description ILIKE $${paramIndex + 1}
        )`);
        params.push(filters.search, `%${filters.search}%`);
        paramIndex += 2;
      }

      if (filters.type) {
        conditions.push(`a.type = $${paramIndex}`);
        params.push(filters.type);
        paramIndex++;
      }

      const whereClause = conditions.length > 0 ? `WHERE ${conditions.join(' AND ')}` : '';

      // Count query
      const countQuery = `
        SELECT COUNT(*) as total
        FROM assets a
        ${whereClause}
      `;

      // Data query
      const dataQuery = `
        SELECT 
          a.*,
          ds.name as data_source_name,
          ds.type as data_source_type,
          ts_rank(to_tsvector('english', coalesce(a.name, '') || ' ' || coalesce(a.description, '')), 
                   plainto_tsquery('english', $1)) as search_rank
        FROM assets a
        LEFT JOIN data_sources ds ON a.data_source_id = ds.id
        ${whereClause}
        ORDER BY ${filters.search ? 'search_rank DESC,' : ''} a.updated_at DESC
        LIMIT $${paramIndex} OFFSET $${paramIndex + 1}
      `;

      params.push(limit, offset);

      const [countResult, dataResult] = await Promise.all([
        this.db.query(countQuery, params.slice(0, -2)),
        this.db.query(dataQuery, params)
      ]);

      const total = parseInt(countResult.rows[0].total);
      const totalPages = Math.ceil(total / limit);

      const assets: Asset[] = dataResult.rows.map(row => this.mapRowToAsset(row));

      return {
        assets,
        total,
        page,
        limit,
        totalPages,
      };
    } catch (error) {
      logger.error('Error in searchAssets:', error);
      throw new Error('Failed to search assets');
    }
  }

  /**
   * Get asset schema details
   */
  public async getAssetSchema(id: string): Promise<Column[] | null> {
    try {
      const query = `
        SELECT columns
        FROM assets
        WHERE id = $1
      `;

      const result = await this.db.query(query, [id]);

      if (result.rows.length === 0) {
        return null;
      }

      return result.rows[0].columns || [];
    } catch (error) {
      logger.error('Error in getAssetSchema:', error);
      throw new Error('Failed to fetch asset schema');
    }
  }

  /**
   * Get asset lineage
   */
  public async getAssetLineage(id: string, direction: 'upstream' | 'downstream' | 'both'): Promise<AssetLineage> {
    try {
      // Mock implementation - in real scenario, you'd analyze SQL queries, ETL processes, etc.
      const upstreamQuery = `
        SELECT DISTINCT a.*
        FROM assets a
        JOIN asset_lineage al ON a.id = al.upstream_asset_id
        WHERE al.downstream_asset_id = $1
      `;

      const downstreamQuery = `
        SELECT DISTINCT a.*
        FROM assets a
        JOIN asset_lineage al ON a.id = al.downstream_asset_id
        WHERE al.upstream_asset_id = $1
      `;

      const relationshipsQuery = `
        SELECT *
        FROM asset_lineage
        WHERE upstream_asset_id = $1 OR downstream_asset_id = $1
      `;

      let upstream: Asset[] = [];
      let downstream: Asset[] = [];

      if (direction === 'upstream' || direction === 'both') {
        const upstreamResult = await this.db.query(upstreamQuery, [id]);
        upstream = upstreamResult.rows.map(this.mapRowToAsset);
      }

      if (direction === 'downstream' || direction === 'both') {
        const downstreamResult = await this.db.query(downstreamQuery, [id]);
        downstream = downstreamResult.rows.map(this.mapRowToAsset);
      }

      const relationshipsResult = await this.db.query(relationshipsQuery, [id]);
      const relationships: LineageRelationship[] = relationshipsResult.rows.map(row => ({
        fromAsset: row.upstream_asset_id,
        toAsset: row.downstream_asset_id,
        type: row.relationship_type,
        description: row.description,
      }));

      return {
        upstream,
        downstream,
        relationships,
      };
    } catch (error) {
      logger.error('Error in getAssetLineage:', error);
      throw new Error('Failed to fetch asset lineage');
    }
  }

  /**
   * Update asset
   */
  public async updateAsset(id: string, updateData: Partial<Asset>): Promise<Asset | null> {
    try {
      const setClause: string[] = [];
      const params: any[] = [];
      let paramIndex = 1;

      if (updateData.name) {
        setClause.push(`name = $${paramIndex++}`);
        params.push(updateData.name);
      }

      if (updateData.description) {
        setClause.push(`description = $${paramIndex++}`);
        params.push(updateData.description);
      }

      if (updateData.status) {
        setClause.push(`status = $${paramIndex++}`);
        params.push(updateData.status);
      }

      if (updateData.tags) {
        setClause.push(`tags = $${paramIndex++}`);
        params.push(updateData.tags);
      }

      if (updateData.metadata) {
        setClause.push(`metadata = $${paramIndex++}`);
        params.push(JSON.stringify(updateData.metadata));
      }

      setClause.push(`updated_at = NOW()`);
      params.push(id);

      const query = `
        UPDATE assets
        SET ${setClause.join(', ')}
        WHERE id = $${paramIndex}
        RETURNING *
      `;

      const result = await this.db.query(query, params);

      if (result.rows.length === 0) {
        return null;
      }

      return this.mapRowToAsset(result.rows[0]);
    } catch (error) {
      logger.error('Error in updateAsset:', error);
      throw new Error('Failed to update asset');
    }
  }

  /**
   * Add tags to asset
   */
  public async addTags(id: string, tags: string[]): Promise<Asset | null> {
    try {
      const query = `
        UPDATE assets
        SET tags = COALESCE(tags, '{}') || $1::text[],
            updated_at = NOW()
        WHERE id = $2
        RETURNING *
      `;

      const result = await this.db.query(query, [tags, id]);

      if (result.rows.length === 0) {
        return null;
      }

      return this.mapRowToAsset(result.rows[0]);
    } catch (error) {
      logger.error('Error in addTags:', error);
      throw new Error('Failed to add tags');
    }
  }

  /**
   * Remove tags from asset
   */
  public async removeTags(id: string, tags: string[]): Promise<Asset | null> {
    try {
      const query = `
        UPDATE assets
        SET tags = ARRAY(SELECT unnest(tags) EXCEPT SELECT unnest($1::text[])),
            updated_at = NOW()
        WHERE id = $2
        RETURNING *
      `;

      const result = await this.db.query(query, [tags, id]);

      if (result.rows.length === 0) {
        return null;
      }

      return this.mapRowToAsset(result.rows[0]);
    } catch (error) {
      logger.error('Error in removeTags:', error);
      throw new Error('Failed to remove tags');
    }
  }

  /**
   * Get asset usage statistics
   */
  public async getAssetStats(id: string, period: string): Promise<AssetStats> {
    try {
      // Mock implementation - in real scenario, you'd aggregate from usage logs
      const statsQuery = `
        SELECT 
          COALESCE(usage.access_count, 0) as access_count,
          usage.last_accessed,
          COALESCE(usage.avg_query_time, 0) as avg_query_time,
          COALESCE(metadata->>'rowCount', '0')::bigint as current_volume,
          COALESCE((metadata->>'qualityScore')::numeric, 85) as quality_score
        FROM assets a
        LEFT JOIN asset_usage_stats usage ON a.id = usage.asset_id
        WHERE a.id = $1
      `;

      const result = await this.db.query(statsQuery, [id]);
      
      if (result.rows.length === 0) {
        throw new Error('Asset not found');
      }

      const row = result.rows[0];

      // Mock usage metrics for the period
      const usageMetrics = this.generateMockUsageMetrics(period);

      return {
        accessCount: row.access_count,
        lastAccessed: row.last_accessed ? new Date(row.last_accessed) : null,
        avgQueryTime: row.avg_query_time,
        dataVolume: {
          current: row.current_volume,
          trend: 'increasing',
          changePercent: 12.5,
        },
        qualityScore: row.quality_score,
        usageMetrics,
      };
    } catch (error) {
      logger.error('Error in getAssetStats:', error);
      throw new Error('Failed to fetch asset stats');
    }
  }

  /**
   * Get asset profile/data preview
   */
  public async getAssetProfile(id: string): Promise<{
    asset: Asset;
    profile: {
      rowCount: number;
      columnCount: number;
      dataTypes: Record<string, number>;
      nullCounts: Record<string, number>;
      sampleData: any[];
      lastProfiledAt: Date;
    };
  } | null> {
    try {
      const asset = await this.getAssetById(id);
      if (!asset) {
        return null;
      }

      // Mock profile data - in real implementation, this would connect to the data source
      const profile = {
        rowCount: Math.floor(Math.random() * 1000000) + 1000,
        columnCount: asset.columns?.length || 0,
        dataTypes: {
          'string': Math.floor(Math.random() * 10) + 1,
          'integer': Math.floor(Math.random() * 5) + 1,
          'decimal': Math.floor(Math.random() * 3) + 1,
          'date': Math.floor(Math.random() * 2) + 1,
          'boolean': Math.floor(Math.random() * 2),
        },
        nullCounts: asset.columns?.reduce((acc, col) => {
          acc[col.name] = Math.floor(Math.random() * 100);
          return acc;
        }, {} as Record<string, number>) || {},
        sampleData: [
          // Mock sample data
          { id: 1, name: 'Sample Row 1', value: 100 },
          { id: 2, name: 'Sample Row 2', value: 200 },
          { id: 3, name: 'Sample Row 3', value: 300 },
        ],
        lastProfiledAt: new Date(),
      };

      return {
        asset,
        profile,
      };
    } catch (error) {
      logger.error('Error in getAssetProfile:', error);
      throw new Error('Failed to get asset profile');
    }
  }

  /**
   * Sync asset metadata from data source
   */
  public async syncAsset(id: string): Promise<{ success: boolean; updatedFields: string[] }> {
    try {
      // Mock implementation - in real scenario, you'd connect to the data source
      // and refresh metadata (row counts, schema changes, etc.)
      
      const updateQuery = `
        UPDATE assets
        SET 
          metadata = metadata || '{"lastSyncAt": "' || NOW() || '", "syncStatus": "completed"}',
          updated_at = NOW()
        WHERE id = $1
        RETURNING *
      `;

      const result = await this.db.query(updateQuery, [id]);

      if (result.rows.length === 0) {
        throw new Error('Asset not found');
      }

      return {
        success: true,
        updatedFields: ['metadata', 'rowCount', 'lastSyncAt'],
      };
    } catch (error) {
      logger.error('Error in syncAsset:', error);
      throw new Error('Failed to sync asset');
    }
  }

  /**
   * Delete asset
   */
  public async deleteAsset(id: string): Promise<boolean> {
    try {
      const query = `
        DELETE FROM assets
        WHERE id = $1
        RETURNING id
      `;

      const result = await this.db.query(query, [id]);
      return result.rows.length > 0;
    } catch (error) {
      logger.error('Error in deleteAsset:', error);
      throw new Error('Failed to delete asset');
    }
  }

  /**
   * Helper method to map database row to Asset object
   */
  private mapRowToAsset(row: any): Asset {
    return {
      id: row.id,
      name: row.name,
      type: row.type,
      dataSourceId: row.data_source_id,
      schemaName: row.schema_name,
      tableName: row.table_name,
      description: row.description,
      columns: row.columns || [],
      tags: row.tags || [],
      status: row.status,
      createdAt: new Date(row.created_at),
      updatedAt: new Date(row.updated_at),
      metadata: row.metadata || {},
    };
  }

  /**
   * Generate mock usage metrics for testing
   */
  private generateMockUsageMetrics(period: string): { date: string; count: number }[] {
    const days = period === '7d' ? 7 : period === '30d' ? 30 : 90;
    const metrics: { date: string; count: number }[] = [];

    for (let i = days - 1; i >= 0; i--) {
      const date = new Date();
      date.setDate(date.getDate() - i);
      
      metrics.push({
        date: date.toISOString().split('T')[0],
        count: Math.floor(Math.random() * 50) + 10,
      });
    }

    return metrics;
  }
}


--------------------------------------------------------------------------------
FILE: backend\data-service\src\services\ConnectionTestService.ts
--------------------------------------------------------------------------------
import { ListBucketsCommand, S3Client } from '@aws-sdk/client-s3';
import axios, { AxiosError } from 'axios';
import { MongoClient } from 'mongodb';
import { createConnection as createMySQLConnection } from 'mysql2/promise';
import { Client as PgClient } from 'pg';
import { createClient as createRedisClient } from 'redis';

import {
  ConnectionTestResult,
  DataSource,
  isAPIConfig,
  isBigQueryConfig,
  isS3Config,
  isSnowflakeConfig,
  type APIConfig,
  type BigQueryConfig,
  type S3Config,
  type SnowflakeConfig,
} from '../models/DataSource';
import { logger } from '../utils/logger';

const toStr = (v: string | number | undefined): string | undefined =>
  v == null ? undefined : String(v);

export class ConnectionTestService {
  async testConnection(dataSource: DataSource): Promise<ConnectionTestResult> {
    const startTime = Date.now();
    try {
      logger.info(`Testing connection for data source: ${dataSource.name} (${dataSource.type})`);
      let result: ConnectionTestResult;

      switch (dataSource.type) {
        case 'postgresql':
          result = await this.testPostgreSQL(dataSource); break;
        case 'mysql':
          result = await this.testMySQL(dataSource); break;
        case 'mssql':
          result = await this.testSQLServer(dataSource); break;
        case 'mongodb':
          result = await this.testMongoDB(dataSource); break;
        case 'redis':
          result = await this.testRedis(dataSource); break;
        case 's3':
          result = await this.testS3(dataSource); break;
        case 'api':
          result = await this.testAPI(dataSource); break;
        case 'snowflake':
          result = await this.testSnowflake(dataSource); break;
        case 'bigquery':
          result = await this.testBigQuery(dataSource); break;
        case 'elasticsearch':
          result = await this.testElasticsearch(dataSource); break;
        default:
          result = await this.testGeneric(dataSource);
      }

      result.responseTime = Date.now() - startTime;
      result.testedAt = new Date();
      logger.info(
        `Connection test completed for ${dataSource.name}: ${result.success ? 'SUCCESS' : 'FAILED'} (${result.responseTime}ms)`
      );
      return result;
    } catch (error: unknown) {
      const responseTime = Date.now() - startTime;
      logger.error(`Connection test failed for ${dataSource.name}:`, error);
      return {
        success: false,
        responseTime,
        error: error instanceof Error ? error.message : 'Unknown error occurred',
        testedAt: new Date(),
      };
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ PostgreSQL â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testPostgreSQL(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    let client: PgClient;

    if (c.connectionString) {
      client = new PgClient({ connectionString: c.connectionString });
    } else {
      const ssl =
        c.ssl === true ? { rejectUnauthorized: false }
        : typeof c.ssl === 'object' ? (c.ssl as object)
        : false;

      client = new PgClient({
        host: c.host,
        port: c.port || 5432,
        database: toStr(c.database),
        user: c.username,
        password: c.password,
        ssl,
        connectionTimeoutMillis: c.timeout || 30_000,
      });
    }

    try {
      await client.connect();
      const result = await client.query('SELECT version()');
      const version = result.rows?.[0]?.version as string | undefined;
      return {
        success: true,
        details: {
          version,
          serverInfo: { type: 'PostgreSQL' },
          capabilities: ['SQL', 'ACID', 'Indexing'],
        },
        testedAt: new Date(),
      };
    } finally {
      await client.end().catch(() => {});
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ MySQL â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testMySQL(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    const cfg: any = {
      host: c.host,
      port: c.port || 3306,
      database: toStr(c.database),
      user: c.username,
      password: c.password,
      connectTimeout: c.timeout || 30_000,
      acquireTimeout: c.timeout || 30_000,
    };

    if (c.ssl === true) cfg.ssl = { rejectUnauthorized: false };
    else if (typeof c.ssl === 'object') cfg.ssl = c.ssl;

    const conn = await createMySQLConnection(cfg);
    try {
      const [rows] = await conn.execute('SELECT VERSION() as version');
      const version = (rows as any)?.[0]?.version as string | undefined;
      return {
        success: true,
        details: {
          version,
          serverInfo: { type: 'MySQL' },
          capabilities: ['SQL', 'ACID', 'Replication'],
        },
        testedAt: new Date(),
      };
    } finally {
      await conn.end().catch(() => {});
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ SQL Server (mock) â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testSQLServer(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    return new Promise((resolve, reject) => {
      setTimeout(() => {
        if (!c.host || !c.username || !c.password) {
          reject(new Error('Missing required connection parameters'));
          return;
        }
        resolve({
          success: true,
          details: {
            version: 'SQL Server 2019',
            serverInfo: { type: 'SQL Server', host: c.host, port: c.port || 1433 },
            capabilities: ['SQL', 'ACID', 'Clustering'],
          },
          testedAt: new Date(),
        });
      }, 100);
    });
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ MongoDB â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testMongoDB(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    const auth = c.username ? `${c.username}:${c.password}@` : '';
    const port = c.port || 27017;
    const database = c.database ? `/${toStr(c.database)}` : '';
    const uri = c.connectionString || `mongodb://${auth}${c.host}:${port}${database}`;

    const client = new MongoClient(uri, { serverSelectionTimeoutMS: c.timeout || 30_000 });
    try {
      await client.connect();
      const serverStatus = await client.db().admin().serverStatus();
      return {
        success: true,
        details: {
          version: (serverStatus as any).version,
          serverInfo: { type: 'MongoDB', host: (serverStatus as any).host, uptime: (serverStatus as any).uptime },
          capabilities: ['Document Store', 'Indexing', 'Aggregation'],
        },
        testedAt: new Date(),
      };
    } finally {
      await client.close().catch(() => {});
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Redis â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testRedis(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    const client = createRedisClient({
      socket: { host: c.host, port: c.port || 6379, connectTimeout: c.timeout || 30_000 },
      password: c.password,
    });

    try {
      await client.connect();
      const info = await client.info();
      const version = info.match(/redis_version:([^\r\n]+)/)?.[1];
      return {
        success: true,
        details: {
          version,
          serverInfo: { type: 'Redis' },
          capabilities: ['Key-Value Store', 'Pub/Sub', 'Streams'],
        },
        testedAt: new Date(),
      };
    } finally {
      await client.disconnect().catch(() => {});
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ S3 â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testS3(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    if (!isS3Config(c)) throw new Error('Invalid S3 configuration');

    const s3 = c as S3Config;
    if (!s3.accessKeyId || !s3.secretAccessKey) {
      throw new Error('S3 credentials (accessKeyId and secretAccessKey) are required');
    }

    const s3Client = new S3Client({
      region: s3.region || 'us-east-1',
      credentials: { accessKeyId: s3.accessKeyId, secretAccessKey: s3.secretAccessKey },
    });

    const response = await s3Client.send(new ListBucketsCommand({}));
    const bucketExists = s3.bucket ? !!response.Buckets?.some(b => b.Name === s3.bucket) : true;

    return {
      success: true,
      details: {
        serverInfo: { type: 'Amazon S3', region: s3.region, bucketCount: response.Buckets?.length || 0, bucketExists },
        capabilities: ['Object Storage', 'Versioning', 'Lifecycle Management'],
      },
      testedAt: new Date(),
    };
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTTP API â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testAPI(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    if (!isAPIConfig(c)) throw new Error('Invalid API configuration');

    const api = c as APIConfig;
    if (!api.baseUrl) throw new Error('Base URL is required for API connections');

    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      'User-Agent': 'CWIC-Data-Service/1.0',
      ...(api.headers || {}),
    };

    if (api.apiKey) headers['Authorization'] = `Bearer ${api.apiKey}`;

    if (api.authentication) {
      const creds = api.authentication.credentials || {};
      switch (api.authentication.type) {
        case 'basic':
          if (creds.username && creds.password) {
            headers['Authorization'] = `Basic ${Buffer.from(`${creds.username}:${creds.password}`).toString('base64')}`;
          }
          break;
        case 'bearer':
          if (creds.token) headers['Authorization'] = `Bearer ${creds.token}`;
          break;
        case 'api-key':
          if (creds.apiKey) headers[creds.headerName || 'X-API-Key'] = creds.apiKey;
          break;
      }
    }

    try {
      const response = await axios.get(api.baseUrl, {
        headers,
        timeout: api.timeout || 30_000,
        validateStatus: s => s < 500,
      });

      return {
        success: true,
        details: {
          serverInfo: {
            type: 'REST API',
            statusCode: response.status,
            statusText: response.statusText,
            contentType: response.headers['content-type'],
          },
          capabilities: ['HTTP REST', 'JSON', 'Authentication'],
        },
        testedAt: new Date(),
      };
    } catch (err) {
      const error = err as AxiosError;
      if (error.response) {
        const ok = error.response.status < 500;
        return {
          success: ok,
          details: {
            serverInfo: {
              type: 'REST API',
              statusCode: error.response.status,
              statusText: error.response.statusText,
              errorMessage: error.message,
            },
            capabilities: ['HTTP REST'],
          },
          error: !ok ? `Server error: ${error.response.status} ${error.response.statusText}` : undefined,
          testedAt: new Date(),
        };
      }
      if (error.request) throw new Error(`Network error: ${error.message}`);
      throw error;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Snowflake (mock) â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testSnowflake(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    if (!isSnowflakeConfig(c)) throw new Error('Invalid Snowflake configuration');
    const sf = c as SnowflakeConfig;

    return new Promise((resolve, reject) => {
      setTimeout(() => {
        if (!sf.host || !sf.username || !sf.password) {
          reject(new Error('Missing required Snowflake connection parameters (host, username, password)'));
          return;
        }
        resolve({
          success: true,
          details: {
            version: 'Snowflake 6.0',
            serverInfo: { type: 'Snowflake', account: sf.host, warehouse: sf.warehouse },
            capabilities: ['SQL', 'Data Warehouse', 'Auto-scaling'],
          },
          testedAt: new Date(),
        });
      }, 200);
    });
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ BigQuery (mock) â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testBigQuery(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    if (!isBigQueryConfig(c)) throw new Error('Invalid BigQuery configuration');
    const bq = c as BigQueryConfig;

    return new Promise((resolve, reject) => {
      setTimeout(() => {
        if (!bq.projectId || !bq.serviceAccountKey) {
          reject(new Error('Missing required BigQuery connection parameters (projectId, serviceAccountKey)'));
          return;
        }
        resolve({
          success: true,
          details: {
            version: 'BigQuery API v2',
            serverInfo: { type: 'Google BigQuery', projectId: bq.projectId, location: bq.location || 'US' },
            capabilities: ['SQL', 'Analytics', 'Machine Learning'],
          },
          testedAt: new Date(),
        });
      }, 150);
    });
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Elasticsearch â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testElasticsearch(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig;
    if (!c.host) throw new Error('Host is required for Elasticsearch connections');

    const protocol = c.ssl ? 'https' : 'http';
    const port = c.port || 9200;
    const baseUrl = `${protocol}://${c.host}:${port}`;

    const headers: Record<string, string> = { 'Content-Type': 'application/json' };
    if (c.username && c.password) {
      headers['Authorization'] = `Basic ${Buffer.from(`${c.username}:${c.password}`).toString('base64')}`;
    }

    const health = await axios.get(`${baseUrl}/_cluster/health`, { headers, timeout: c.timeout || 30_000 });
    const root = await axios.get(`${baseUrl}/`, { headers, timeout: c.timeout || 30_000 });

    return {
      success: true,
      details: {
        version: (root.data as any)?.version?.number,
        serverInfo: {
          type: 'Elasticsearch',
          clusterName: health.data.cluster_name,
          status: health.data.status,
          numberOfNodes: health.data.number_of_nodes,
        },
        capabilities: ['Full-text Search', 'Analytics', 'Aggregations'],
      },
      testedAt: new Date(),
    };
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generic fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async testGeneric(ds: DataSource): Promise<ConnectionTestResult> {
    const c = ds.connectionConfig as any;
    return new Promise((resolve, reject) => {
      setTimeout(() => {
        if (!c.host && !c.baseUrl && !c.connectionString) {
          reject(new Error('At least one of host, baseUrl, or connectionString is required'));
          return;
        }
        resolve({
          success: true,
          details: {
            serverInfo: { type: ds.type, host: c.host, port: c.port },
            capabilities: ['Generic Connection'],
          },
          testedAt: new Date(),
        });
      }, 100);
    });
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  public async testMultipleConnections(dataSources: DataSource[]): Promise<ConnectionTestResult[]> {
    const promises = dataSources.map(ds =>
      this.testConnection(ds).catch(err => {
        const msg = err instanceof Error ? err.message : 'Unknown error';
        return { success: false, error: msg, responseTime: 0, testedAt: new Date() } as ConnectionTestResult;
      })
    );
    return Promise.all(promises);
  }

  public getSupportedTypes(): string[] {
    return [
      'postgresql','mysql','mssql','mongodb','redis','s3','api','file',
      'snowflake','bigquery','elasticsearch','oracle','azure-blob','gcs',
      'redshift','databricks','kafka',
    ];
  }
}



--------------------------------------------------------------------------------
FILE: backend\data-service\src\services\connectors\azureSql.ts
--------------------------------------------------------------------------------
import { ConnectionPool, config as MSSQLConfig, Request } from 'mssql';
import type { ConnectionConfig, ConnectionTestResult } from '../../models/Connection';
import { logger } from '../../utils/logger';
import {
  BaseConnector,
  type Column,
  type QueryResult,
  type Schema,
  type SchemaInfo,
  type Table,
  type View,
} from './base';

export interface AzureSqlConfig {
  server: string;
  database: string;
  user: string;
  password: string;
  port?: number;
  options?: {
    encrypt?: boolean;
    trustServerCertificate?: boolean;
    enableArithAbort?: boolean;
    connectionTimeout?: number;
    requestTimeout?: number;
  };
  pool?: {
    max?: number;
    min?: number;
    idleTimeoutMillis?: number;
  };
}

export class AzureSqlConnector extends BaseConnector {
  private pool: ConnectionPool | null = null;
  private config: AzureSqlConfig;

  constructor(conn: ConnectionConfig) {
    super('azure-sql', conn);
    this.config = this.parseConfig(conn);
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ config parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private parseConfig(c: ConnectionConfig): AzureSqlConfig {
    const anyC = c as any;
    const trustServerCertificate =
      typeof anyC.trustServerCertificate === 'boolean'
        ? anyC.trustServerCertificate
        : typeof c.ssl === 'object'
        ? (c.ssl as any)?.rejectUnauthorized === false
        : false;

    return {
      server: c.host ?? anyC.server ?? '',
      database: (c.database as string) ?? anyC.db ?? '',
      user: (c.username as string) ?? anyC.user ?? '',
      password: (c.password as string) ?? '',
      port: c.port ?? 1433,
      options: {
        encrypt: true, // Azure SQL requires encrypt
        trustServerCertificate,
        enableArithAbort: true,
        connectionTimeout: anyC.connectionTimeout ?? c.timeout ?? 30_000,
        requestTimeout: anyC.requestTimeout ?? c.timeout ?? 30_000,
      },
      pool: {
        max: anyC.maxConnections ?? 10,
        min: anyC.minConnections ?? 1,
        idleTimeoutMillis: anyC.idleTimeout ?? 30_000,
        ...(anyC.poolOptions ?? {}),
      },
    };
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ connect / disconnect â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  override async connect(): Promise<void> {
    try {
      if (this.pool?.connected) return;

      const poolConfig: MSSQLConfig = {
        server: this.config.server,
        database: this.config.database,
        user: this.config.user,
        password: this.config.password,
        port: this.config.port,
        options: this.config.options,
        pool: this.config.pool,
      };

      this.pool = new ConnectionPool(poolConfig);

      this.pool.on('error', (err: Error) => {
        logger.error('Azure SQL pool error:', err);
        this.emitConnectionEvent('error', err);
      });

      await this.pool.connect();

      (this as any).metrics.connected = true;
      this.emitConnectionEvent('connected', {
        server: this.config.server,
        database: this.config.database,
      });

      logger.info(`Connected to Azure SQL: ${this.config.server}/${this.config.database}`);
    } catch (error) {
      (this as any).metrics.connected = false;
      logger.error('Failed to connect to Azure SQL:', error);
      throw this.createConnectionError(error);
    }
  }

  override async disconnect(): Promise<void> {
    try {
      if (this.pool) {
        this.pool.removeAllListeners?.('error');
        await this.pool.close();
        this.pool = null;
        (this as any).metrics.connected = false;
        this.emitConnectionEvent('disconnected');
        logger.info('Disconnected from Azure SQL');
      }
    } catch (error) {
      logger.error('Error disconnecting from Azure SQL:', error);
      throw error;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ basic ops â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  override async testConnection(): Promise<ConnectionTestResult> {
    const t0 = Date.now();
    try {
      await this.connect();
      const req = new Request(this.pool!);
      const result = await req.query('SELECT @@VERSION AS version, DB_NAME() AS database_name');
      return {
        success: true,
        responseTime: Date.now() - t0,
        details: {
          version: (result as any)?.recordset?.[0]?.version,
          serverInfo: {
            type: 'Azure SQL Database',
            database: (result as any)?.recordset?.[0]?.database_name,
            server: this.config.server,
          },
          capabilities: ['SQL', 'ACID', 'Transactions', 'Stored Procedures', 'Functions'],
        },
        testedAt: new Date(),
      };
    } catch (error: any) {
      return {
        success: false,
        responseTime: Date.now() - t0,
        error: error?.message || 'Unknown error',
        testedAt: new Date(),
      };
    }
  }

  override async executeQuery(query: string, params?: any[]): Promise<QueryResult> {
    return this.executeWithMetrics(async () => {
      await this.ensureConnected();
      const req = new Request(this.pool!);

      if (params?.length) {
        params.forEach((p, i) => req.input(`p${i}`, p));
        let idx = 0;
        query = query.replace(/\?/g, () => `@p${idx++}`);
      }

      const r = await req.query(query);
      return {
        rows: (r as any).recordset ?? [],
        rowCount: (r as any).rowsAffected?.[0] ?? ((r as any).recordset?.length || 0),
        columns: (r as any).recordset?.columns || {},
      };
    });
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ schema & metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  override async getSchema(): Promise<SchemaInfo> {
    await this.ensureConnected();

    const schemaQuery = `
      SELECT 
        t.TABLE_SCHEMA,
        t.TABLE_NAME,
        t.TABLE_TYPE,
        c.COLUMN_NAME,
        c.DATA_TYPE,
        c.IS_NULLABLE,
        c.COLUMN_DEFAULT,
        c.CHARACTER_MAXIMUM_LENGTH,
        c.NUMERIC_PRECISION,
        c.NUMERIC_SCALE,
        CASE WHEN pk.COLUMN_NAME IS NOT NULL THEN 1 ELSE 0 END AS IS_PRIMARY_KEY
      FROM INFORMATION_SCHEMA.TABLES t
      LEFT JOIN INFORMATION_SCHEMA.COLUMNS c
        ON t.TABLE_NAME = c.TABLE_NAME AND t.TABLE_SCHEMA = c.TABLE_SCHEMA
      LEFT JOIN (
        SELECT ku.TABLE_SCHEMA, ku.TABLE_NAME, ku.COLUMN_NAME
        FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc
        INNER JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE ku
          ON tc.CONSTRAINT_NAME = ku.CONSTRAINT_NAME
          AND tc.TABLE_SCHEMA = ku.TABLE_SCHEMA
        WHERE tc.CONSTRAINT_TYPE = 'PRIMARY KEY'
      ) pk ON c.TABLE_SCHEMA = pk.TABLE_SCHEMA 
         AND c.TABLE_NAME   = pk.TABLE_NAME 
         AND c.COLUMN_NAME  = pk.COLUMN_NAME
      WHERE t.TABLE_TYPE IN ('BASE TABLE', 'VIEW')
      ORDER BY t.TABLE_SCHEMA, t.TABLE_NAME, c.ORDINAL_POSITION
    `;

    const res = await this.executeQuery(schemaQuery);
    return this.buildSchemaInfo(res.rows as any[]);
  }

  private buildSchemaInfo(rows: any[]): SchemaInfo {
    const bySchema = new Map<string, { tables: Map<string, Table>; views: Map<string, View> }>();

    for (const r of rows) {
      const sName = r.TABLE_SCHEMA as string;
      const tName = r.TABLE_NAME as string;
      const isView = (r.TABLE_TYPE as string) === 'VIEW';

      if (!bySchema.has(sName)) bySchema.set(sName, { tables: new Map(), views: new Map() });
      const bucket = bySchema.get(sName)!;

      if (isView) {
        const vmap = bucket.views;
        if (!vmap.has(tName)) {
          vmap.set(tName, { name: tName, schema: sName, columns: [], definition: undefined });
        }
        if (r.COLUMN_NAME) {
          const col: Column = {
            name: r.COLUMN_NAME,
            dataType: r.DATA_TYPE,
            nullable: r.IS_NULLABLE === 'YES',
            defaultValue: r.COLUMN_DEFAULT ?? undefined,
            maxLength: r.CHARACTER_MAXIMUM_LENGTH ?? undefined,
            precision: r.NUMERIC_PRECISION ?? undefined,
            scale: r.NUMERIC_SCALE ?? undefined,
            isPrimaryKey: r.IS_PRIMARY_KEY === 1,
          };
          vmap.get(tName)!.columns.push(col);
        }
      } else {
        const tmap = bucket.tables;
        if (!tmap.has(tName)) {
          tmap.set(tName, { name: tName, schema: sName, columns: [], primaryKeys: [], foreignKeys: [], indexes: [] });
        }
        if (r.COLUMN_NAME) {
          const col: Column = {
            name: r.COLUMN_NAME,
            dataType: r.DATA_TYPE,
            nullable: r.IS_NULLABLE === 'YES',
            defaultValue: r.COLUMN_DEFAULT ?? undefined,
            maxLength: r.CHARACTER_MAXIMUM_LENGTH ?? undefined,
            precision: r.NUMERIC_PRECISION ?? undefined,
            scale: r.NUMERIC_SCALE ?? undefined,
            isPrimaryKey: r.IS_PRIMARY_KEY === 1,
          };
          const tbl = tmap.get(tName)!;
          tbl.columns.push(col);
          if (col.isPrimaryKey) tbl.primaryKeys.push(col.name);
        }
      }
    }

    const schemas: Schema[] = [];
    let totalTables = 0;
    let totalViews = 0;
    let totalColumns = 0;

    for (const [name, { tables, views }] of bySchema) {
      const tablesArr = Array.from(tables.values());
      const viewsArr = Array.from(views.values());
      totalTables += tablesArr.length;
      totalViews += viewsArr.length;
      totalColumns +=
        tablesArr.reduce((n, t) => n + t.columns.length, 0) +
        viewsArr.reduce((n, v) => n + v.columns.length, 0);
      schemas.push({ name, tables: tablesArr, views: viewsArr });
    }

    return { schemas, totalTables, totalViews, totalColumns };
  }

  override async validateQuery(query: string): Promise<{ valid: boolean; error?: string }> {
    try {
      await this.ensureConnected();
      const req = new Request(this.pool!);
      await req.batch('SET PARSEONLY ON;');
      try {
        await req.query(query);
        await req.batch('SET PARSEONLY OFF;');
        return { valid: true };
      } catch (err: any) {
        await req.batch('SET PARSEONLY OFF;');
        return { valid: false, error: err?.message || 'Invalid query' };
      }
    } catch (e: any) {
      return { valid: false, error: e?.message || 'Connection error during validation' };
    }
  }

  override async getSampleData(tableName: string, schemaName: string = 'dbo', limit: number = 100): Promise<QueryResult> {
    await this.ensureConnected();
    const full = `${this.escapeIdentifier(schemaName)}.${this.escapeIdentifier(tableName)}`;
    const q = `SELECT TOP (${limit}) * FROM ${full}`;
    return this.executeQuery(q);
  }

  async getConnectionInfo(): Promise<any> {
    await this.ensureConnected();
    const q = `
      SELECT 
        @@VERSION AS version,
        @@SERVERNAME AS server_name,
        DB_NAME() AS database_name,
        SUSER_NAME() AS login_name,
        @@LANGUAGE AS language,
        GETDATE() AS server_time
    `;
    const r = await this.executeQuery(q);
    return (r.rows as any[])?.[0];
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ internals â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
  private async ensureConnected(): Promise<void> {
    if (!this.pool || !this.pool.connected) await this.connect();
  }

  protected override escapeIdentifier(identifier: string): string {
    return `[${String(identifier).replace(/]/g, ']]')}]`;
  }

  private createConnectionError(error: any): Error {
    const message = error instanceof Error ? error.message : 'Connection failed';
    return new Error(`Azure SQL connection error: ${message}`);
  }

  private createQueryError(error: any, query: string): Error {
    const message = error instanceof Error ? error.message : 'Query failed';
    return new Error(`Azure SQL query error: ${message} (Query: ${query.substring(0, 120)}...)`);
  }

  override getMetrics(): any {
    const p: any = this.pool as any;
    const poolStats = p?.pool ?? p;
    return {
      type: 'azure-sql',
      connected: !!this.pool?.connected,
      server: this.config.server,
      database: this.config.database,
      poolSize: poolStats?.size ?? 0,
      poolAvailable: poolStats?.available ?? 0,
      poolPending: poolStats?.pending ?? 0,
    };
  }
}

export default AzureSqlConnector;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\services\connectors\base.ts
--------------------------------------------------------------------------------
import { EventEmitter } from "events";
import type { ConnectionConfig } from "../../models/Connection";
import type { ConnectionTestResult } from "../../models/DataSource";

export interface ConnectorMetrics {
  type: string;
  connected: boolean;
  totalQueries: number;
  successfulQueries: number;
  failedQueries: number;
  averageQueryTime: number;
  lastQueryAt?: Date;
  lastErrorAt?: Date;
  uptime: number;
}

export interface QueryResult {
  rows: any[];
  rowCount: number;
  columns?: any;
  executionTime?: number;
  metadata?: any;
}

export interface SchemaInfo {
  schemas: Schema[];
  totalTables: number;
  totalViews: number;
  totalColumns: number;
}

export interface Schema {
  name: string;
  tables: Table[];
  views: View[];
}

export interface Table {
  name: string;
  schema: string;
  columns: Column[];
  primaryKeys: string[];
  foreignKeys: ForeignKey[];
  indexes: Index[];
  rowCount?: number;
  sizeBytes?: number;
  lastModified?: Date;
}

export interface View {
  name: string;
  schema: string;
  columns: Column[];
  definition?: string;
}

export interface Column {
  name: string;
  dataType: string;
  nullable: boolean;
  defaultValue?: string;
  maxLength?: number;
  precision?: number;
  scale?: number;
  isPrimaryKey?: boolean;
  isForeignKey?: boolean;
  isUnique?: boolean;
  comment?: string;
}

export interface ForeignKey {
  name: string;
  column: string;
  referencedTable: string;
  referencedColumn: string;
  referencedSchema?: string;
}

export interface Index {
  name: string;
  columns: string[];
  unique: boolean;
  type: string;
  clustered?: boolean;
}

export abstract class BaseConnector<C extends ConnectionConfig = ConnectionConfig> extends EventEmitter {
  protected type: string;
  protected connectionConfig: C;
  protected metrics: ConnectorMetrics;
  protected startTime: Date;

  constructor(type: string, connectionConfig: C) {
    super();
    this.type = type;
    this.connectionConfig = connectionConfig;
    this.startTime = new Date();
    this.metrics = {
      type,
      connected: false,
      totalQueries: 0,
      successfulQueries: 0,
      failedQueries: 0,
      averageQueryTime: 0,
      uptime: 0,
    };
  }

  abstract connect(): Promise<void>;
  abstract disconnect(): Promise<void>;
  abstract testConnection(): Promise<ConnectionTestResult>;
  abstract executeQuery(query: string, params?: any[]): Promise<QueryResult>;
  abstract getSchema(): Promise<SchemaInfo>;

  // Optional lifecycle hook
  cleanup?(): Promise<void> | void;

  async validateQuery(query: string): Promise<{ valid: boolean; error?: string }> {
    try {
      await this.executeQuery(`EXPLAIN ${query}`);
      return { valid: true };
    } catch (error: any) {
      return { valid: false, error: error?.message ?? "Query validation failed" };
    }
  }

  async getTableStats(tableName: string, schemaName?: string): Promise<any> {
    return {
      tableName,
      schemaName,
      rowCount: 0,
      sizeBytes: 0,
      message: "Table statistics not available for this connector type",
    };
  }

  async getSampleData(tableName: string, schemaName?: string, limit = 100): Promise<QueryResult> {
    const full = schemaName ? `${schemaName}.${tableName}` : tableName;
    const q = `SELECT * FROM ${full} LIMIT ${limit}`;
    return this.executeQuery(q);
  }

  async getColumnDistinctValues(
    tableName: string,
    columnName: string,
    schemaName?: string,
    limit = 100
  ): Promise<any[]> {
    const full = schemaName ? `${schemaName}.${tableName}` : tableName;
    const q = `SELECT DISTINCT ${columnName} FROM ${full} LIMIT ${limit}`;
    const result = await this.executeQuery(q);
    return result.rows.map((r) => r[columnName]);
  }

  getMetrics(): ConnectorMetrics {
    return { ...this.metrics, uptime: Date.now() - this.startTime.getTime() };
  }

  protected updateQueryMetrics(executionTime: number, success: boolean): void {
    this.metrics.totalQueries++;
    if (success) {
      this.metrics.successfulQueries++;
      this.metrics.lastQueryAt = new Date();
      const totalTime =
        this.metrics.averageQueryTime * (this.metrics.successfulQueries - 1) + executionTime;
      this.metrics.averageQueryTime = totalTime / this.metrics.successfulQueries;
    } else {
      this.metrics.failedQueries++;
      this.metrics.lastErrorAt = new Date();
    }
  }

  protected async executeWithMetrics<T>(operation: () => Promise<T>): Promise<T> {
    const t0 = Date.now();
    let ok = false;
    try {
      const result = await operation();
      ok = true;
      return result;
    } finally {
      const dt = Date.now() - t0;
      this.updateQueryMetrics(dt, ok);
    }
  }

  protected emitConnectionEvent(event: "connected" | "disconnected" | "error", data?: any): void {
    this.emit(event, { connector: this.type, timestamp: new Date(), data });
  }

  async healthCheck(): Promise<{
    status: "healthy" | "unhealthy" | "degraded";
    latency?: number;
    error?: string;
    details?: any;
  }> {
    try {
      const r = await this.testConnection();
      if (r.success) {
        return {
          status: (r.responseTime ?? 0) > 5000 ? "degraded" : "healthy",
          latency: r.responseTime,
          details: r.details,
        };
      }
      return { status: "unhealthy", error: r.error, details: r.details };
    } catch (e: any) {
      return { status: "unhealthy", error: e?.message ?? "Health check failed" };
    }
  }

  getConnectionConfig(): C {
    const clone: any = { ...this.connectionConfig };
    if (clone.password) clone.password = "[REDACTED]";
    return clone as C;
  }

  updateConnectionConfig(patch: Partial<C>): void {
    this.connectionConfig = { ...this.connectionConfig, ...patch };
  }

  protected escapeIdentifier(identifier: string): string {
    return `"${String(identifier).replace(/"/g, '""')}"`;
  }
  protected escapeLiteral(literal: string): string {
    return `'${String(literal).replace(/'/g, "''")}'`;
  }

  protected parseConnectionString(cs: string): Record<string, string> {
    const out: Record<string, string> = {};
    cs.split(";").forEach((pair) => {
      const [k, v] = pair.split("=");
      if (k && v) out[k.trim().toLowerCase()] = v.trim();
    });
    return out;
  }

  async getPoolStats(): Promise<{ total: number; active: number; idle: number; waiting: number }> {
    return { total: 1, active: this.metrics.connected ? 1 : 0, idle: 0, waiting: 0 };
  }
}

export default BaseConnector;



--------------------------------------------------------------------------------
FILE: backend\data-service\src\services\connectors\factory.ts
--------------------------------------------------------------------------------
// backend/data-service/src/services/connectors/factory.ts - FIXED VERSION

import type { ConnectionConfig } from "../../models/Connection";
import type { DataSourceType } from "../../models/DataSource";
import { logger } from "../../utils/logger";
import { AzureSqlConnector } from "./azureSql";
import { BaseConnector } from "./base";

// Fix: Use generic constraint that works with union types
export interface ConnectorConstructor<TConfig extends ConnectionConfig = ConnectionConfig> {
  new (config: TConfig): BaseConnector<TConfig>;
}

/* Normalize common aliases to canonical DataSourceType values */
const normalizeType = (t: string): DataSourceType => {
  const x = (t || "").toLowerCase();
  if (x === "azure_sql" || x === "azure-sql" || x === "sqlserver" || x === "sql-server") return "mssql";
  return x as DataSourceType;
};

export class ConnectorFactory {
  private static connectors: Map<DataSourceType | string, ConnectorConstructor<any>> = new Map();
  private static instances: Map<string, BaseConnector<any>> = new Map();

  /* Register built-ins with proper type casting */
  static {
    // Fix: Use type assertion to handle the union type compatibility
    ConnectorFactory.registerConnector("mssql", AzureSqlConnector as ConnectorConstructor<any>);
    ConnectorFactory.registerConnector("azure-sql", AzureSqlConnector as ConnectorConstructor<any>);
    ConnectorFactory.registerConnector("azure_sql", AzureSqlConnector as ConnectorConstructor<any>);
  }

  static registerConnector(type: DataSourceType | string, ctor: ConnectorConstructor<any>): void {
    ConnectorFactory.connectors.set(type, ctor);
    logger.info(`Registered connector for type: ${type}`);
  }

  static createConnector(config: ConnectionConfig): BaseConnector<ConnectionConfig> {
    const type = normalizeType(config.type as string);
    const Ctor =
      ConnectorFactory.connectors.get(type) || 
      ConnectorFactory.connectors.get(config.type as string);
    
    if (!Ctor) {
      throw new Error(`No connector available for data source type: ${config.type}`);
    }
    
    try {
      const inst = new Ctor(config);
      logger.info(`Created connector instance for type: ${type}`);
      return inst;
    } catch (err: any) {
      logger.error(`Failed to create connector for ${type}:`, err);
      throw new Error(`Failed to create connector: ${err?.message || "Unknown error"}`);
    }
  }

  static async getConnector(dataSourceId: string, config: ConnectionConfig): Promise<BaseConnector<ConnectionConfig>> {
    let c = ConnectorFactory.instances.get(dataSourceId);
    if (!c) {
      c = ConnectorFactory.createConnector(config);
      ConnectorFactory.instances.set(dataSourceId, c);
      
      // Fix: Add proper event handling with type safety
      c.on("disconnected", () => {
        ConnectorFactory.instances.delete(dataSourceId);
        logger.info(`Connector removed from cache: ${dataSourceId}`);
      });
      
      c.on("error", (e) => {
        logger.error(`Connector error [${dataSourceId}]`, e);
      });
    }
    return c;
  }

  static async removeConnector(dataSourceId: string): Promise<void> {
    const c = ConnectorFactory.instances.get(dataSourceId);
    if (c) {
      try {
        // Fix: Check if cleanup method exists before calling
        if (c && typeof (c as any).cleanup === "function") {
          await (c as any).cleanup();
        }
      } catch (e) {
        logger.error(`Cleanup error for ${dataSourceId}`, e);
      } finally {
        ConnectorFactory.instances.delete(dataSourceId);
      }
    }
  }

  static async testConnection(config: ConnectionConfig): Promise<any> {
    const c = ConnectorFactory.createConnector(config);
    try {
      const result = await c.testConnection();
      await c.disconnect();
      return result;
    } catch (e) {
      try {
        await c.disconnect();
      } catch (disconnectError) {
        logger.warn("Error during connector cleanup after test failure:", disconnectError);
      }
      throw e;
    }
  }

  // Fix: Updated helper methods with better type safety
  static getAvailableTypes(): DataSourceType[] {
    const uniq = new Set<DataSourceType>();
    for (const k of ConnectorFactory.connectors.keys()) {
      const normalized = normalizeType(String(k));
      if (normalized) {
        uniq.add(normalized);
      }
    }
    return Array.from(uniq);
  }

  static isTypeSupported(type: DataSourceType | string): boolean {
    const normalized = normalizeType(String(type));
    return ConnectorFactory.connectors.has(normalized) || ConnectorFactory.connectors.has(type);
  }

  // Fix: More robust config validation with type guards
  static validateConfig(type: DataSourceType | string, config: any): string[] {
    const normalizedType = normalizeType(String(type));
    const errors: string[] = [];

    // Basic validation
    if (!config || typeof config !== 'object') {
      errors.push("Configuration must be an object");
      return errors;
    }

    switch (normalizedType) {
      case "postgresql":
      case "mysql":
      case "mssql": {
        const hasConnStr = config.connectionString && typeof config.connectionString === 'string';
        if (!hasConnStr) {
          if (!config.host || typeof config.host !== 'string') {
            errors.push("Host is required when not using connection string");
          }
          if (!config.database || typeof config.database !== 'string') {
            errors.push("Database is required when not using connection string");
          }
          if (!config.username || typeof config.username !== 'string') {
            errors.push("Username is required when not using connection string");
          }
        }
        if (config.port !== undefined && (typeof config.port !== 'number' || config.port < 1 || config.port > 65535)) {
          errors.push("Port must be a number between 1 and 65535");
        }
        break;
      }
      case "mongodb": {
        const hasConnStr = config.connectionString && typeof config.connectionString === 'string';
        const hasHost = config.host && typeof config.host === 'string';
        if (!hasConnStr && !hasHost) {
          errors.push("Connection string or host is required");
        }
        break;
      }
      case "s3": {
        if (!config.bucket || typeof config.bucket !== 'string') {
          errors.push("Bucket is required");
        }
        if (!config.region || typeof config.region !== 'string') {
          errors.push("Region is required");
        }
        if (!config.accessKeyId || typeof config.accessKeyId !== 'string') {
          errors.push("Access Key ID is required");
        }
        if (!config.secretAccessKey || typeof config.secretAccessKey !== 'string') {
          errors.push("Secret Access Key is required");
        }
        break;
      }
      case "api": {
        if (!config.baseUrl || typeof config.baseUrl !== 'string') {
          errors.push("Base URL is required");
        } else {
          try {
            new URL(config.baseUrl);
          } catch {
            errors.push("Base URL must be a valid URL");
          }
        }
        break;
      }
      default:
        // For unknown types, do basic validation
        if (config.timeout !== undefined && (typeof config.timeout !== 'number' || config.timeout <= 0)) {
          errors.push("Timeout must be a positive number");
        }
        break;
    }

    return errors;
  }

  // Rest of your existing methods remain the same
  static getConfigTemplate(type: DataSourceType | string): any {
    const normalized = normalizeType(String(type));
    switch (normalized) {
      case "postgresql":
        return { 
          host: "localhost", port: 5432, database: "", username: "", password: "", 
          ssl: false, timeout: 30000, maxConnections: 10 
        };
      case "mysql":
        return { 
          host: "localhost", port: 3306, database: "", username: "", password: "", 
          ssl: false, timeout: 30000, maxConnections: 10 
        };
      case "mssql":
        return { 
          host: "localhost", port: 1433, database: "", username: "", password: "", 
          ssl: true, timeout: 30000, maxConnections: 10 
        };
      case "mongodb":
        return { 
          connectionString: "mongodb://localhost:27017", database: "", 
          username: "", password: "", timeout: 30000 
        };
      case "s3":
        return { 
          region: "us-east-1", bucket: "", accessKeyId: "", secretAccessKey: "" 
        };
      case "api":
        return { 
          baseUrl: "https://api.example.com", apiKey: "", timeout: 30000, headers: {} 
        };
      default:
        return { timeout: 30000 };
    }
  }

  static getActiveConnectors(): Map<string, BaseConnector<any>> {
    return new Map(ConnectorFactory.instances);
  }

  static getAllMetrics(): Record<string, any> {
    const out: Record<string, any> = {};
    for (const [id, c] of ConnectorFactory.instances) {
      try {
        out[id] = (c as any).getMetrics?.() || { status: "no metrics available" };
      } catch (e: any) {
        out[id] = { error: e?.message || "Failed to get metrics" };
      }
    }
    return out;
  }

  static async healthCheckAll(): Promise<Record<string, any>> {
    const results: Record<string, any> = {};
    const checks = Array.from(ConnectorFactory.instances.entries()).map(async ([id, c]) => {
      try {
        results[id] = await c.healthCheck();
      } catch (e: any) {
        results[id] = { status: "unhealthy", error: e?.message || "Health check failed" };
      }
    });
    
    await Promise.allSettled(checks);
    return results;
  }

  static async cleanupAll(): Promise<void> {
    const cleanupPromises = Array.from(ConnectorFactory.instances.keys()).map(id => 
      ConnectorFactory.removeConnector(id)
    );
    
    await Promise.allSettled(cleanupPromises);
    ConnectorFactory.instances.clear();
    logger.info("All connectors cleaned up");
  }
}

export default ConnectorFactory;


--------------------------------------------------------------------------------
FILE: backend\data-service\src\services\DatabaseService.ts
--------------------------------------------------------------------------------
// src/services/DatabaseService.ts
import { Pool, PoolClient, QueryResult } from 'pg';
import { config } from '../config/env';
import { logger, loggerUtils } from '../utils/logger';

export interface DatabaseConfig {
  host: string;
  port: number;
  database: string;
  user: string;
  password: string;
  ssl?: boolean | object;
  max?: number;
  min?: number;
  idleTimeoutMillis?: number;
  connectionTimeoutMillis?: number;
}

export interface QueryOptions {
  timeout?: number;
  retries?: number;
  retryDelay?: number;
}

export interface TransactionOptions {
  timeout?: number;
  isolationLevel?: 'READ_UNCOMMITTED' | 'READ_COMMITTED' | 'REPEATABLE_READ' | 'SERIALIZABLE';
}

export interface HealthCheckResult {
  status: 'healthy' | 'unhealthy';
  latency?: number;
  connections?: {
    total: number;
    idle: number;
    waiting: number;
  };
  error?: string;
  timestamp: string;
}

export interface PoolStats {
  total: number;
  idle: number;
  waiting: number;
  max: number;
  min: number;
}

export interface Migration {
  id: number;
  name: string;
  sql: string;
  executedAt?: Date;
}

export class DatabaseService {
  private pool!: Pool; // Use definite assignment assertion
  private isInitialized: boolean = false;
  private migrations: Migration[] = [];

  constructor() {
    this.initializePool();
    this.setupMigrations();
  }

  /**
   * Initialize the database connection pool
   */
  private initializePool(): void {
    const dbConfig: DatabaseConfig = {
      host: config.database.host,
      port: config.database.port,
      database: config.database.name,
      user: config.database.user,
      password: config.database.password,
      ssl: config.database.ssl ? { rejectUnauthorized: false } : false,
      max: config.database.poolMax,
      min: config.database.poolMin,
      idleTimeoutMillis: config.database.idleTimeout,
      connectionTimeoutMillis: config.database.connectionTimeout,
    };

    this.pool = new Pool(dbConfig);

    // Handle pool events
    this.pool.on('connect', (client: PoolClient) => {
      logger.debug('New database client connected', {
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
      });
    });

    this.pool.on('acquire', (client: PoolClient) => {
      logger.debug('Database client acquired from pool', {
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
        waitingClients: this.pool.waitingCount,
      });
    });

    this.pool.on('remove', (client: PoolClient) => {
      logger.debug('Database client removed from pool', {
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
      });
    });

    this.pool.on('error', (err: Error, client: PoolClient) => {
      logger.error('Database pool error:', {
        error: err.message,
        stack: err.stack,
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
      });
    });

    logger.info('Database pool initialized', {
      host: dbConfig.host,
      port: dbConfig.port,
      database: dbConfig.database,
      maxConnections: dbConfig.max,
      minConnections: dbConfig.min,
      ssl: !!dbConfig.ssl,
    });

    this.isInitialized = true;
  }

  /**
   * Setup database migrations
   */
  private setupMigrations(): void {
    this.migrations = [
      {
        id: 1,
        name: '001_create_data_sources_table',
        sql: `
          CREATE TABLE IF NOT EXISTS data_sources (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name VARCHAR(100) NOT NULL,
            type VARCHAR(50) NOT NULL CHECK (type IN ('postgres', 'mysql', 'sqlserver', 'mongodb', 's3', 'api', 'file')),
            host VARCHAR(255),
            port INTEGER CHECK (port > 0 AND port <= 65535),
            database_name VARCHAR(100),
            username VARCHAR(100),
            password_encrypted TEXT,
            ssl BOOLEAN DEFAULT FALSE,
            connection_string TEXT,
            description TEXT,
            tags TEXT[] DEFAULT '{}',
            status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'inactive', 'error')),
            metadata JSONB DEFAULT '{}',
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            last_tested_at TIMESTAMP WITH TIME ZONE,
            last_sync_at TIMESTAMP WITH TIME ZONE
          );

          -- Indexes for performance
          CREATE INDEX IF NOT EXISTS idx_data_sources_type ON data_sources(type);
          CREATE INDEX IF NOT EXISTS idx_data_sources_status ON data_sources(status);
          CREATE INDEX IF NOT EXISTS idx_data_sources_name ON data_sources(name);
          CREATE INDEX IF NOT EXISTS idx_data_sources_tags ON data_sources USING GIN(tags);

          -- Updated at trigger
          CREATE OR REPLACE FUNCTION update_updated_at_column()
          RETURNS TRIGGER AS $$
          BEGIN
            NEW.updated_at = NOW();
            RETURN NEW;
          END;
          $$ language 'plpgsql';

          DROP TRIGGER IF EXISTS update_data_sources_updated_at ON data_sources;
          CREATE TRIGGER update_data_sources_updated_at
            BEFORE UPDATE ON data_sources
            FOR EACH ROW
            EXECUTE FUNCTION update_updated_at_column();
        `
      },
      {
        id: 2,
        name: '002_create_assets_table',
        sql: `
          CREATE TABLE IF NOT EXISTS assets (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name VARCHAR(100) NOT NULL,
            type VARCHAR(50) NOT NULL CHECK (type IN ('table', 'view', 'procedure', 'function', 'schema', 'index', 'trigger')),
            data_source_id UUID NOT NULL REFERENCES data_sources(id) ON DELETE CASCADE,
            schema_name VARCHAR(100),
            table_name VARCHAR(100),
            full_name VARCHAR(255) GENERATED ALWAYS AS (
              CASE 
                WHEN schema_name IS NOT NULL AND table_name IS NOT NULL 
                THEN schema_name || '.' || table_name 
                ELSE name 
              END
            ) STORED,
            description TEXT,
            columns JSONB DEFAULT '[]',
            tags TEXT[] DEFAULT '{}',
            status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'inactive', 'deprecated', 'archived')),
            metadata JSONB DEFAULT '{}',
            quality_score NUMERIC(3,2) DEFAULT 0 CHECK (quality_score >= 0 AND quality_score <= 100),
            sensitivity_level VARCHAR(20) DEFAULT 'public' CHECK (sensitivity_level IN ('public', 'internal', 'confidential', 'restricted')),
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            last_scanned_at TIMESTAMP WITH TIME ZONE,
            last_accessed_at TIMESTAMP WITH TIME ZONE
          );

          -- Indexes for performance
          CREATE INDEX IF NOT EXISTS idx_assets_data_source_id ON assets(data_source_id);
          CREATE INDEX IF NOT EXISTS idx_assets_type ON assets(type);
          CREATE INDEX IF NOT EXISTS idx_assets_status ON assets(status);
          CREATE INDEX IF NOT EXISTS idx_assets_name ON assets(name);
          CREATE INDEX IF NOT EXISTS idx_assets_full_name ON assets(full_name);
          CREATE INDEX IF NOT EXISTS idx_assets_tags ON assets USING GIN(tags);
          CREATE INDEX IF NOT EXISTS idx_assets_sensitivity ON assets(sensitivity_level);
          CREATE INDEX IF NOT EXISTS idx_assets_quality_score ON assets(quality_score);

          -- Full text search index
          CREATE INDEX IF NOT EXISTS idx_assets_search ON assets USING GIN(
            to_tsvector('english', coalesce(name, '') || ' ' || coalesce(description, ''))
          );

          -- Updated at trigger
          DROP TRIGGER IF EXISTS update_assets_updated_at ON assets;
          CREATE TRIGGER update_assets_updated_at
            BEFORE UPDATE ON assets
            FOR EACH ROW
            EXECUTE FUNCTION update_updated_at_column();
        `
      },
      {
        id: 3,
        name: '003_create_asset_lineage_table',
        sql: `
          CREATE TABLE IF NOT EXISTS asset_lineage (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            upstream_asset_id UUID NOT NULL REFERENCES assets(id) ON DELETE CASCADE,
            downstream_asset_id UUID NOT NULL REFERENCES assets(id) ON DELETE CASCADE,
            relationship_type VARCHAR(50) NOT NULL CHECK (relationship_type IN (
              'table_to_view', 'view_to_table', 'procedure_call', 'function_call',
              'data_flow', 'dependency', 'transformation', 'aggregation'
            )),
            confidence_score NUMERIC(3,2) DEFAULT 0 CHECK (confidence_score >= 0 AND confidence_score <= 100),
            description TEXT,
            discovered_by VARCHAR(50) DEFAULT 'manual',
            metadata JSONB DEFAULT '{}',
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            
            -- Prevent self-referencing and duplicate relationships
            CONSTRAINT no_self_reference CHECK (upstream_asset_id != downstream_asset_id),
            CONSTRAINT unique_lineage UNIQUE (upstream_asset_id, downstream_asset_id, relationship_type)
          );

          -- Indexes for lineage queries
          CREATE INDEX IF NOT EXISTS idx_lineage_upstream ON asset_lineage(upstream_asset_id);
          CREATE INDEX IF NOT EXISTS idx_lineage_downstream ON asset_lineage(downstream_asset_id);
          CREATE INDEX IF NOT EXISTS idx_lineage_type ON asset_lineage(relationship_type);
          CREATE INDEX IF NOT EXISTS idx_lineage_confidence ON asset_lineage(confidence_score);

          -- Updated at trigger
          DROP TRIGGER IF EXISTS update_asset_lineage_updated_at ON asset_lineage;
          CREATE TRIGGER update_asset_lineage_updated_at
            BEFORE UPDATE ON asset_lineage
            FOR EACH ROW
            EXECUTE FUNCTION update_updated_at_column();
        `
      },
      {
        id: 4,
        name: '004_create_asset_usage_stats_table',
        sql: `
          CREATE TABLE IF NOT EXISTS asset_usage_stats (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            asset_id UUID NOT NULL REFERENCES assets(id) ON DELETE CASCADE,
            date DATE NOT NULL DEFAULT CURRENT_DATE,
            access_count INTEGER DEFAULT 0 CHECK (access_count >= 0),
            query_count INTEGER DEFAULT 0 CHECK (query_count >= 0),
            avg_query_time_ms NUMERIC(10,2) DEFAULT 0 CHECK (avg_query_time_ms >= 0),
            error_count INTEGER DEFAULT 0 CHECK (error_count >= 0),
            data_volume_mb NUMERIC(12,2) DEFAULT 0 CHECK (data_volume_mb >= 0),
            unique_users INTEGER DEFAULT 0 CHECK (unique_users >= 0),
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            
            -- One record per asset per day
            CONSTRAINT unique_asset_date UNIQUE (asset_id, date)
          );

          -- Indexes for usage analytics
          CREATE INDEX IF NOT EXISTS idx_usage_stats_asset_id ON asset_usage_stats(asset_id);
          CREATE INDEX IF NOT EXISTS idx_usage_stats_date ON asset_usage_stats(date);
          CREATE INDEX IF NOT EXISTS idx_usage_stats_access_count ON asset_usage_stats(access_count);

          -- Updated at trigger
          DROP TRIGGER IF EXISTS update_asset_usage_stats_updated_at ON asset_usage_stats;
          CREATE TRIGGER update_asset_usage_stats_updated_at
            BEFORE UPDATE ON asset_usage_stats
            FOR EACH ROW
            EXECUTE FUNCTION update_updated_at_column();
        `
      },
      {
        id: 5,
        name: '005_create_data_quality_rules_table',
        sql: `
          CREATE TABLE IF NOT EXISTS data_quality_rules (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name VARCHAR(100) NOT NULL,
            description TEXT,
            asset_id UUID REFERENCES assets(id) ON DELETE CASCADE,
            column_name VARCHAR(100),
            rule_type VARCHAR(50) NOT NULL CHECK (rule_type IN (
              'not_null', 'unique', 'range', 'format', 'custom', 'referential_integrity',
              'data_freshness', 'completeness', 'consistency', 'accuracy'
            )),
            rule_definition JSONB NOT NULL,
            severity VARCHAR(20) DEFAULT 'medium' CHECK (severity IN ('low', 'medium', 'high', 'critical')),
            is_active BOOLEAN DEFAULT TRUE,
            threshold_warning NUMERIC(5,2) DEFAULT 90 CHECK (threshold_warning >= 0 AND threshold_warning <= 100),
            threshold_error NUMERIC(5,2) DEFAULT 80 CHECK (threshold_error >= 0 AND threshold_error <= 100),
            last_executed_at TIMESTAMP WITH TIME ZONE,
            last_result JSONB,
            created_by VARCHAR(100),
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
          );

          -- Indexes for quality rules
          CREATE INDEX IF NOT EXISTS idx_quality_rules_asset_id ON data_quality_rules(asset_id);
          CREATE INDEX IF NOT EXISTS idx_quality_rules_type ON data_quality_rules(rule_type);
          CREATE INDEX IF NOT EXISTS idx_quality_rules_active ON data_quality_rules(is_active);
          CREATE INDEX IF NOT EXISTS idx_quality_rules_severity ON data_quality_rules(severity);

          -- Updated at trigger
          DROP TRIGGER IF EXISTS update_data_quality_rules_updated_at ON data_quality_rules;
          CREATE TRIGGER update_data_quality_rules_updated_at
            BEFORE UPDATE ON data_quality_rules
            FOR EACH ROW
            EXECUTE FUNCTION update_updated_at_column();
        `
      }
    ];
  }

  /**
   * Execute a query with optional retries
   */
  public async query(text: string, params?: any[], options: QueryOptions = {}): Promise<QueryResult> {
    const { timeout = 30000, retries = 0, retryDelay = 1000 } = options;
    const start = Date.now();
    const queryId = Math.random().toString(36).substring(7);
    
    let lastError: Error;
    
    for (let attempt = 0; attempt <= retries; attempt++) {
      try {
        if (attempt > 0) {
          logger.warn(`Query retry attempt ${attempt}`, { queryId, attempt });
          await this.sleep(retryDelay * attempt);
        }

        logger.debug('Database query started', {
          queryId,
          query: text.substring(0, 100) + (text.length > 100 ? '...' : ''),
          paramCount: params?.length || 0,
          attempt: attempt + 1,
        });

        const result = await this.pool.query({
          text,
          values: params,
        });
        
        const duration = Date.now() - start;
        
        loggerUtils.logDbOperation('select', 'general', duration, true);
        
        logger.debug('Database query completed', {
          queryId,
          duration: `${duration}ms`,
          rowCount: result.rowCount,
          command: result.command,
          attempt: attempt + 1,
        });
        
        return result;
      } catch (error) {
        lastError = error as Error;
        const duration = Date.now() - start;
        
        logger.error('Database query failed', {
          queryId,
          query: text.substring(0, 100) + (text.length > 100 ? '...' : ''),
          duration: `${duration}ms`,
          error: lastError.message,
          paramCount: params?.length || 0,
          attempt: attempt + 1,
          retriesLeft: retries - attempt,
        });

        if (attempt === retries) {
          loggerUtils.logDbOperation('select', 'general', duration, false);
          throw lastError;
        }
      }
    }

    throw lastError!;
  }

  /**
   * Execute a transaction with proper error handling
   */
  public async transaction<T>(
    callback: (client: PoolClient) => Promise<T>,
    options: TransactionOptions = {}
  ): Promise<T> {
    const { timeout = 30000, isolationLevel } = options;
    const client = await this.pool.connect();
    const transactionId = Math.random().toString(36).substring(7);
    const start = Date.now();
    
    try {
      await client.query('BEGIN');
      
      if (isolationLevel) {
        await client.query(`SET TRANSACTION ISOLATION LEVEL ${isolationLevel}`);
      }
      
      logger.debug('Database transaction started', { 
        transactionId, 
        isolationLevel: isolationLevel || 'default',
        timeout 
      });
      
      const result = await Promise.race([
        callback(client),
        this.createTimeoutPromise<T>(timeout, `Transaction timeout after ${timeout}ms`)
      ]);
      
      await client.query('COMMIT');
      
      const duration = Date.now() - start;
      logger.debug('Database transaction committed', { 
        transactionId, 
        duration: `${duration}ms` 
      });
      
      return result;
    } catch (error) {
      await client.query('ROLLBACK');
      const duration = Date.now() - start;
      
      logger.error('Database transaction rolled back', {
        transactionId,
        duration: `${duration}ms`,
        error: error instanceof Error ? error.message : 'Unknown error'
      });
      
      throw error;
    } finally {
      client.release();
      logger.debug('Database transaction client released', { transactionId });
    }
  }

  /**
   * Health check with comprehensive testing
   */
  public async healthCheck(): Promise<HealthCheckResult> {
    const start = Date.now();
    
    try {
      // Basic connectivity test
      await this.pool.query('SELECT 1 as health_check');
      const latency = Date.now() - start;
      
      return {
        status: 'healthy',
        latency,
        connections: {
          total: this.pool.totalCount,
          idle: this.pool.idleCount,
          waiting: this.pool.waitingCount
        },
        timestamp: new Date().toISOString(),
      };
    } catch (error) {
      const latency = Date.now() - start;
      return {
        status: 'unhealthy',
        latency,
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString(),
      };
    }
  }

  /**
   * Deep health check with write operations
   */
  public async deepHealthCheck(): Promise<{
    status: 'healthy' | 'unhealthy';
    checks: {
      connectivity: boolean;
      writeable: boolean;
      performant: boolean;
    };
    latency: number;
    error?: string;
  }> {
    const start = Date.now();
    const checks = {
      connectivity: false,
      writeable: false,
      performant: false,
    };

    try {
      // Test connectivity
      await this.pool.query('SELECT 1');
      checks.connectivity = true;

      // Test write capability
      const tempTableName = `health_check_${Date.now()}`;
      await this.transaction(async (client) => {
        await client.query(`CREATE TEMP TABLE ${tempTableName} (id INTEGER)`);
        await client.query(`INSERT INTO ${tempTableName} VALUES (1)`);
        await client.query(`SELECT * FROM ${tempTableName}`);
        await client.query(`DROP TABLE ${tempTableName}`);
      });
      checks.writeable = true;

      const latency = Date.now() - start;
      
      // Check if performance is acceptable (under 1 second)
      checks.performant = latency < 1000;

      return {
        status: Object.values(checks).every(Boolean) ? 'healthy' : 'unhealthy',
        checks,
        latency,
      };
    } catch (error) {
      const latency = Date.now() - start;
      return {
        status: 'unhealthy',
        checks,
        latency,
        error: error instanceof Error ? error.message : 'Unknown error',
      };
    }
  }

  /**
   * Get pool statistics
   */
  public getPoolStats(): PoolStats {
    return {
      total: this.pool.totalCount,
      idle: this.pool.idleCount,
      waiting: this.pool.waitingCount,
      max: config.database.poolMax,
      min: config.database.poolMin,
    };
  }

  /**
   * Get detailed database statistics
   */
  public async getDetailedStats(): Promise<{
    pool: PoolStats;
    database: {
      version: string;
      size: string;
      activeConnections: number;
      totalTables: number;
      totalIndexes: number;
    };
  }> {
    try {
      // Get database version
      const versionResult = await this.query('SELECT version()');
      const version = versionResult.rows[0].version;

      // Get database size
      const sizeResult = await this.query(`
        SELECT pg_size_pretty(pg_database_size(current_database())) as size
      `);
      const size = sizeResult.rows[0].size;

      // Get active connections count
      const connectionsResult = await this.query(`
        SELECT count(*) as active_connections 
        FROM pg_stat_activity 
        WHERE state = 'active'
      `);
      const activeConnections = parseInt(connectionsResult.rows[0].active_connections);

      // Get table count
      const tablesResult = await this.query(`
        SELECT count(*) as total_tables
        FROM information_schema.tables
        WHERE table_schema = 'public'
      `);
      const totalTables = parseInt(tablesResult.rows[0].total_tables);

      // Get index count
      const indexesResult = await this.query(`
        SELECT count(*) as total_indexes
        FROM pg_indexes
        WHERE schemaname = 'public'
      `);
      const totalIndexes = parseInt(indexesResult.rows[0].total_indexes);

      return {
        pool: this.getPoolStats(),
        database: {
          version: version.split(' ')[1], // Extract version number
          size,
          activeConnections,
          totalTables,
          totalIndexes,
        },
      };
    } catch (error) {
      logger.error('Error getting detailed database stats:', error);
      throw error;
    }
  }

  /**
   * Run database migrations
   */
  public async runMigrations(): Promise<void> {
    try {
      logger.info('Running database migrations...');

      // Create migrations table if it doesn't exist
      await this.query(`
        CREATE TABLE IF NOT EXISTS migrations (
          id SERIAL PRIMARY KEY,
          name VARCHAR(255) NOT NULL UNIQUE,
          executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        )
      `);

      for (const migration of this.migrations) {
        // Check if migration has already been run
        const result = await this.query(
          'SELECT id FROM migrations WHERE name = $1',
          [migration.name]
        );

        if (result.rows.length === 0) {
          logger.info(`Running migration: ${migration.name}`);
          
          // Run migration in a transaction
          await this.transaction(async (client) => {
            await client.query(migration.sql);
            await client.query(
              'INSERT INTO migrations (name) VALUES ($1)',
              [migration.name]
            );
          });

          logger.info(`Migration completed: ${migration.name}`);
        } else {
          logger.debug(`Migration already applied: ${migration.name}`);
        }
      }

      logger.info('All migrations completed successfully');
    } catch (error) {
      logger.error('Migration failed:', error);
      throw error;
    }
  }

  /**
   * Get migration status
   */
  public async getMigrationStatus(): Promise<{
    total: number;
    executed: number;
    pending: string[];
    executed_migrations: string[];
  }> {
    try {
      const result = await this.query('SELECT name FROM migrations ORDER BY executed_at');
      const executedMigrations = result.rows.map(row => row.name);
      
      const pendingMigrations = this.migrations
        .map(m => m.name)
        .filter(name => !executedMigrations.includes(name));

      return {
        total: this.migrations.length,
        executed: executedMigrations.length,
        pending: pendingMigrations,
        executed_migrations: executedMigrations,
      };
    } catch (error) {
      logger.error('Error getting migration status:', error);
      throw error;
    }
  }

  /**
   * Close database connections
   */
  public async close(): Promise<void> {
    try {
      logger.info('Closing database pool...');
      await this.pool.end();
      this.isInitialized = false;
      logger.info('Database pool closed successfully');
    } catch (error) {
      logger.error('Error closing database pool:', error);
      throw error;
    }
  }

  /**
   * Check if database service is initialized
   */
  public isReady(): boolean {
    return this.isInitialized;
  }

  /**
   * Utility method to create timeout promises
   */
  private createTimeoutPromise<T>(ms: number, message: string): Promise<T> {
    return new Promise((_, reject) => {
      setTimeout(() => reject(new Error(message)), ms);
    });
  }

  /**
   * Utility method for sleep/delay
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Backup database to file
   */
  public async createBackup(options: {
    includeTables?: string[];
    excludeTables?: string[];
  } = {}): Promise<{ success: boolean; message: string; filename?: string }> {
    try {
      // This would typically use pg_dump
      // For now, we'll return a success message
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
      const filename = `backup-${config.database.name}-${timestamp}.sql`;
      
      logger.info('Database backup initiated', { filename, options });
      
      return {
        success: true,
        message: 'Backup completed successfully',
        filename,
      };
    } catch (error) {
      logger.error('Database backup failed:', error);
      return {
        success: false,
        message: error instanceof Error ? error.message : 'Unknown error',
      };
    }
  }

  /**
   * Get database performance metrics
   */
  public async getPerformanceMetrics(): Promise<{
    slowQueries: any[];
    indexUsage: any[];
    tableStats: any[];
  }> {
    try {
      // Get slow queries (requires pg_stat_statements extension)
      const slowQueriesResult = await this.query(`
        SELECT 
          query,
          calls,
          total_time,
          mean_time,
          rows
        FROM pg_stat_statements
        WHERE mean_time > 1000
        ORDER BY mean_time DESC
        LIMIT 10
      `).catch(() => ({ rows: [] })); // Fallback if extension not available

      // Get index usage statistics
      const indexUsageResult = await this.query(`
        SELECT 
          schemaname,
          tablename,
          indexname,
          idx_scan,
          idx_tup_read,
          idx_tup_fetch
        FROM pg_stat_user_indexes
        WHERE idx_scan > 0
        ORDER BY idx_scan DESC
        LIMIT 20
      `);

      // Get table statistics
      const tableStatsResult = await this.query(`
        SELECT 
          schemaname,
          tablename,
          seq_scan,
          seq_tup_read,
          idx_scan,
          idx_tup_fetch,
          n_tup_ins,
          n_tup_upd,
          n_tup_del
        FROM pg_stat_user_tables
        ORDER BY seq_scan + idx_scan DESC
        LIMIT 20
      `);

      return {
        slowQueries: slowQueriesResult.rows,
        indexUsage: indexUsageResult.rows,
        tableStats: tableStatsResult.rows,
      };
    } catch (error) {
      logger.error('Error getting performance metrics:', error);
      throw error;
    }
  }
}


--------------------------------------------------------------------------------
FILE: backend\data-service\src\services\DataSourceService.ts
--------------------------------------------------------------------------------
import { nanoid } from 'nanoid';
import { ConnectionConfig, DataSource, DataSourceStatus, DataSourceType } from '../models/DataSource';
import { logger } from '../utils/logger';
import { DatabaseService } from './DatabaseService';

export interface DataSourceFilters {
  status?: DataSourceStatus;
  type?: DataSourceType;
  createdBy?: string;
}

export interface PaginatedDataSources {
  dataSources: DataSource[];
  total: number;
  page: number;
  limit: number;
  totalPages: number;
}

export interface HealthSummary {
  total: number;
  healthy: number;
  warning: number;
  error: number;
  lastUpdated: Date;
}

export interface SchemaInfo {
  tables: TableInfo[];
  views: ViewInfo[];
  totalTables: number;
  totalColumns: number;
  estimatedSize: string;
}

export interface TableInfo {
  name: string;
  schema?: string;
  rowCount?: number;
  columns: ColumnInfo[];
  indexes: IndexInfo[];
  constraints: ConstraintInfo[];
}

export interface ColumnInfo {
  name: string;
  type: string;
  nullable: boolean;
  primaryKey: boolean;
  foreignKey?: {
    table: string;
    column: string;
  };
  defaultValue?: string;
}

export interface ViewInfo {
  name: string;
  schema?: string;
  definition: string;
}

export interface IndexInfo {
  name: string;
  columns: string[];
  unique: boolean;
  type: string;
}

export interface ConstraintInfo {
  name: string;
  type: 'PRIMARY_KEY' | 'FOREIGN_KEY' | 'UNIQUE' | 'CHECK';
  columns: string[];
  referencedTable?: string;
  referencedColumns?: string[];
}

export interface SyncResult {
  syncId: string;
  status: 'started' | 'completed' | 'failed';
  tablesScanned: number;
  newTables: number;
  updatedTables: number;
  errors: string[];
  startedAt: Date;
  completedAt?: Date;
}

export class DataSourceService {
  private db: DatabaseService;

  constructor() {
    this.db = new DatabaseService();
  }

  // ---- helpers -------------------------------------------------------------

  private makePublicId(): string {
    return `ds_${Date.now()}_${crypto.randomUUID().slice(0, 8)}`;
  }

  private coerceJSON<T>(val: any, fallback: T): T {
    if (val === null || val === undefined) return fallback;
    if (typeof val === 'string') {
      try { return JSON.parse(val) as T; } catch { return fallback; }
    }
    return val as T;
  }

  private coerceStringArray(val: any): string[] {
    if (Array.isArray(val)) return val.map(String);
    if (typeof val === 'string') {
      try {
        const parsed = JSON.parse(val);
        return Array.isArray(parsed) ? parsed.map(String) : [];
      } catch {
        // Node-postgres may return Postgres array as "{a,b}" string in some configs.
        if (/^\{.*\}$/.test(val)) {
          return val
            .slice(1, -1)
            .split(',')
            .map(s => s.replace(/^"(.*)"$/, '$1'))
            .filter(Boolean);
        }
        return [];
      }
    }
    return [];
  }

  /** Coerce raw JSON/DB value into a valid ConnectionConfig.
 *  Falls back to a minimal discriminant only (e.g. { type: 'api' }).
 */
private toConnectionConfig(raw: any, fallbackType?: DataSourceType): ConnectionConfig {
  try {
    const obj: any =
      raw == null
        ? {}
        : (typeof raw === 'string' ? JSON.parse(raw) : raw);

    if (obj && typeof obj === 'object' && typeof obj.type === 'string') {
      // Assume it structurally matches one of the union members:
      return obj as ConnectionConfig;
    }
  } catch {
    // ignore parse errors and use fallback
  }

  // Minimal, valid union member; baseUrl/etc. are optional in APIConnection
  const t = (fallbackType as any) ?? 'api';
  return { type: t } as ConnectionConfig;
}


  private mapRowToDataSource(row: any): DataSource {
    return {
      id: row.id, // UUID (PK)
      name: row.name,
      description: row.description,
      type: row.type,
      status: row.status,
      connectionConfig: this.toConnectionConfig(row.connection_config, row.type), // JSONB
      tags: this.coerceStringArray(row.tags),                                     // text[]
      metadata: this.coerceJSON(row.metadata, {}),                                // JSONB
      createdAt: row.created_at,
      updatedAt: row.updated_at,
      createdBy: row.created_by,
      updatedBy: row.updated_by,
      lastTestAt: row.last_test_at ?? row.last_tested_at ?? null,   // tolerate legacy column
      lastSyncAt: row.last_sync_at,
      lastError: row.last_error ?? null,
      // if your model includes publicId, expose it:
      // @ts-ignore allow extra field if interface doesn't have it yet
      publicId: row.public_id ?? null,
    };
  }

  // ---- queries -------------------------------------------------------------

  async getAllDataSources(options: {
    page: number;
    limit: number;
    filters: DataSourceFilters;
  }): Promise<PaginatedDataSources> {
    try {
      const { page, limit, filters } = options;
      const offset = (page - 1) * limit;

      let whereClause = 'WHERE deleted_at IS NULL';
      const params: any[] = [];
      let i = 1;

      if (filters.status) {
        whereClause += ` AND status = $${i++}`;
        params.push(filters.status);
      }
      if (filters.type) {
        whereClause += ` AND type = $${i++}`;
        params.push(filters.type);
      }
      if (filters.createdBy) {
        whereClause += ` AND created_by = $${i++}`;
        params.push(filters.createdBy);
      }

      const countSQL = `SELECT COUNT(*)::int AS total FROM data_sources ${whereClause}`;
      const { rows: countRows } = await this.db.query(countSQL, params);
      const total = countRows[0]?.total ?? 0;

      const dataSQL = `
        SELECT
          id, name, description, type, status,
          connection_config, tags, metadata,
          created_at, updated_at, created_by, updated_by,
          last_test_at, last_sync_at, last_error,
          public_id
        FROM data_sources
        ${whereClause}
        ORDER BY created_at DESC
        LIMIT $${i++} OFFSET $${i++}
      `;
      const { rows } = await this.db.query(dataSQL, [...params, limit, offset]);
      const dataSources = rows.map(r => this.mapRowToDataSource(r));

      return {
        dataSources,
        total,
        page,
        limit,
        totalPages: Math.ceil(total / limit),
      };
    } catch (error) {
      logger.error('Error in getAllDataSources:', error);
      throw error;
    }
  }

  async getDataSourceById(id: string): Promise<DataSource | null> {
    try {
      const sql = `
        SELECT
          id, name, description, type, status,
          connection_config, tags, metadata,
          created_at, updated_at, created_by, updated_by,
          last_test_at, last_sync_at, last_error,
          public_id
        FROM data_sources
        WHERE id = $1 AND deleted_at IS NULL
      `;
      const { rows } = await this.db.query(sql, [id]);
      if (rows.length === 0) return null;
      return this.mapRowToDataSource(rows[0]);
    } catch (error) {
      logger.error('Error in getDataSourceById:', error);
      throw error;
    }
  }





async createDataSource(dataSourceData: Partial<DataSource>) {
  try {
    const publicId = `ds_${Date.now()}_${nanoid(8)}`;
    const nowTs = new Date();

    // â¬‡ï¸ ensure a minimal valid config gets stored
    const connectionCfg: ConnectionConfig =
      dataSourceData.connectionConfig && typeof (dataSourceData.connectionConfig as any).type === 'string'
        ? (dataSourceData.connectionConfig as ConnectionConfig)
        : ({ type: (dataSourceData.type as any) ?? 'api' } as ConnectionConfig);

    const query = `
      INSERT INTO data_sources (
        name, description, type, status,
        connection_config, tags, metadata,
        created_at, updated_at, created_by, public_id
      )
      VALUES ($1,$2,$3,$4,$5::jsonb,$6::jsonb,$7::jsonb,$8,$9,$10,$11)
      RETURNING *;
    `;

    const params = [
      dataSourceData.name,
      dataSourceData.description ?? null,
      dataSourceData.type,
      dataSourceData.status ?? 'pending',
      JSON.stringify(connectionCfg),                         // â¬…ï¸ here
      JSON.stringify(dataSourceData.tags ?? []),
      JSON.stringify(dataSourceData.metadata ?? {}),
      nowTs, nowTs,
      dataSourceData.createdBy ?? null,
      publicId,
    ];

    const result = await this.db.query(query, params);
    return this.mapRowToDataSource(result.rows[0]);
  } catch (err) {
    logger.error('Error in createDataSource:', err);
    throw err;
  }
}



  async updateDataSource(id: string, updateData: Partial<DataSource>): Promise<DataSource | null> {
    try {
      const existing = await this.getDataSourceById(id);
      if (!existing) return null;

      const fieldMap: Record<string, string> = {
        name: 'name',
        description: 'description',
        type: 'type',
        status: 'status',
        connectionConfig: 'connection_config', // JSONB
        tags: 'tags',                           // text[]
        metadata: 'metadata',                   // JSONB
        lastTestAt: 'last_test_at',
        lastSyncAt: 'last_sync_at',
        lastError: 'last_error',
      };

      const sets: string[] = [];
      const params: any[] = [];
      let i = 1;

      for (const [k, v] of Object.entries(updateData)) {
        const col = fieldMap[k];
        if (!col) continue;

        if (col === 'tags') {
          sets.push(`${col} = $${i++}`);
          params.push(Array.isArray(v) ? v : []);
        } else if (col === 'connection_config') {
          const cfg = this.toConnectionConfig(v, (updateData.type as any) ?? undefined);
          sets.push(`${col} = $${i++}`);
          params.push(cfg);
        } else if (col === 'metadata') {
          sets.push(`${col} = $${i++}`);
          params.push(v ?? {});
        }

      }

      // audit fields
      sets.push(`updated_at = $${i++}`);
      params.push(new Date());

      if (updateData.updatedBy !== undefined) {
        sets.push(`updated_by = $${i++}`);
        params.push(updateData.updatedBy ?? null);
      }

      params.push(id);

      const sql = `
        UPDATE data_sources
        SET ${sets.join(', ')}
        WHERE id = $${i} AND deleted_at IS NULL
        RETURNING *
      `;

      const { rows } = await this.db.query(sql, params);
      if (rows.length === 0) return null;
      return this.mapRowToDataSource(rows[0]);
    } catch (error) {
      logger.error('Error in updateDataSource:', error);
      throw error;
    }
  }

  async deleteDataSource(id: string): Promise<boolean> {
    try {
      const sql = `
        UPDATE data_sources
        SET deleted_at = $1
        WHERE id = $2 AND deleted_at IS NULL
      `;
      const { rowCount } = await this.db.query(sql, [new Date(), id]);
      return (rowCount ?? 0) > 0;
    } catch (error) {
      logger.error('Error in deleteDataSource:', error);
      throw error;
    }
  }

  async getHealthSummary(): Promise<HealthSummary> {
    try {
      const sql = `
        SELECT
          COUNT(*)::int as total,
          COUNT(CASE WHEN status = 'connected' THEN 1 END)::int as healthy,
          COUNT(CASE WHEN status = 'warning' THEN 1 END)::int as warning,
          COUNT(CASE WHEN status IN ('error','disconnected') THEN 1 END)::int as error
        FROM data_sources
        WHERE deleted_at IS NULL
      `;
      const { rows } = await this.db.query(sql);
      const row = rows[0] || { total: 0, healthy: 0, warning: 0, error: 0 };
      return { ...row, lastUpdated: new Date() } as HealthSummary;
    } catch (error) {
      logger.error('Error in getHealthSummary:', error);
      throw error;
    }
  }

  async getDataSourceSchema(_id: string): Promise<SchemaInfo> {
    // stub / unchanged
    return {
      tables: [
        {
          name: 'users',
          schema: 'public',
          rowCount: 10000,
          columns: [
            { name: 'id', type: 'bigint', nullable: false, primaryKey: true },
            { name: 'email', type: 'varchar', nullable: false, primaryKey: false },
            { name: 'created_at', type: 'timestamp', nullable: false, primaryKey: false },
          ],
          indexes: [
            { name: 'users_pkey', columns: ['id'], unique: true, type: 'btree' },
            { name: 'users_email_idx', columns: ['email'], unique: true, type: 'btree' },
          ],
          constraints: [{ name: 'users_pkey', type: 'PRIMARY_KEY', columns: ['id'] }],
        },
      ],
      views: [],
      totalTables: 1,
      totalColumns: 3,
      estimatedSize: '125MB',
    };
  }

  async syncDataSource(id: string, _options: { force: boolean }): Promise<SyncResult> {
    const syncId = this.generateId();
    await this.updateDataSource(id, { lastSyncAt: new Date(), status: 'connected' as any });
    return {
      syncId,
      status: 'completed',
      tablesScanned: 15,
      newTables: 2,
      updatedTables: 3,
      errors: [],
      startedAt: new Date(),
      completedAt: new Date(),
    };
  }

  private generateId(): string {
    return `ds_${Date.now()}_${Math.random().toString(36).slice(2, 10)}`;
  }
}



--------------------------------------------------------------------------------
FILE: backend\data-service\src\utils\logger.ts
--------------------------------------------------------------------------------
// src/utils/logger.ts
import path from 'path';
import winston from 'winston';
import { env } from '../config/env';

// Define custom log levels
const customLevels = {
  levels: {
    error: 0,
    warn: 1,
    info: 2,
    http: 3,
    debug: 4,
  },
  colors: {
    error: 'red',
    warn: 'yellow',
    info: 'green',
    http: 'magenta',
    debug: 'white',
  }
};

// Add colors to winston
winston.addColors(customLevels.colors);

// Define log format
const logFormat = winston.format.combine(
  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss:ms' }),
  winston.format.colorize({ all: true }),
  winston.format.printf(
    (info) => `${info.timestamp} ${info.level}: ${info.message}`,
  ),
);

// Define transports
const transports = [
  // Console transport
  new winston.transports.Console({
    format: logFormat,
  }),
  
  // Error log file
  new winston.transports.File({
    filename: path.join('logs', 'error.log'),
    level: 'error',
    format: winston.format.combine(
      winston.format.timestamp(),
      winston.format.json()
    ),
  }),
  
  // Combined log file
  new winston.transports.File({
    filename: path.join('logs', 'combined.log'),
    format: winston.format.combine(
      winston.format.timestamp(),
      winston.format.json()
    ),
  }),
];

// Create logger instance
export const logger = winston.createLogger({
  level: env.LOG_LEVEL || 'info',
  levels: customLevels.levels,
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json(),
  ),
  transports,
  exitOnError: false,
});

// HTTP Request logger middleware
export const httpLogger = winston.createLogger({
  level: 'http',
  levels: customLevels.levels,
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.colorize(),
    winston.format.printf(
      (info) => `${info.timestamp} ${info.level}: ${info.message}`,
    ),
  ),
  transports: [
    new winston.transports.Console(),
    new winston.transports.File({
      filename: path.join('logs', 'http.log'),
      format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json()
      ),
    }),
  ],
});

// Performance logger for timing operations
export const performanceLogger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json(),
  ),
  transports: [
    new winston.transports.File({
      filename: path.join('logs', 'performance.log'),
    }),
  ],
});

// Security logger for auth and security events
export const securityLogger = winston.createLogger({
  level: 'warn',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json(),
  ),
  transports: [
    new winston.transports.File({
      filename: path.join('logs', 'security.log'),
    }),
  ],
});

// Utility functions for structured logging
export const loggerUtils = {
  // Log HTTP requests
  logRequest: (req: any, res: any, duration: number) => {
    httpLogger.http(`${req.method} ${req.url} - ${res.statusCode} - ${duration}ms`);
  },

  // Log database operations
  logDbOperation: (operation: string, table: string, duration: number, success: boolean) => {
    performanceLogger.info({
      type: 'database',
      operation,
      table,
      duration,
      success,
      timestamp: new Date().toISOString(),
    });
  },

  // Log authentication events
  logAuth: (event: string, userId?: string, ip?: string, userAgent?: string) => {
    securityLogger.warn({
      type: 'authentication',
      event,
      userId,
      ip,
      userAgent,
      timestamp: new Date().toISOString(),
    });
  },

  // Log API calls to external services
  logApiCall: (service: string, endpoint: string, method: string, duration: number, statusCode: number) => {
    performanceLogger.info({
      type: 'external_api',
      service,
      endpoint,
      method,
      duration,
      statusCode,
      timestamp: new Date().toISOString(),
    });
  },

  // Log data source operations
  logDataSource: (operation: string, dataSourceId: string, status: string, details?: any) => {
    logger.info({
      type: 'data_source',
      operation,
      dataSourceId,
      status,
      details,
      timestamp: new Date().toISOString(),
    });
  },

  // Log connection tests
  logConnectionTest: (dataSourceId: string, type: string, success: boolean, duration: number, error?: string) => {
    logger.info({
      type: 'connection_test',
      dataSourceId,
      connectionType: type,
      success,
      duration,
      error,
      timestamp: new Date().toISOString(),
    });
  },

  // Log errors with context
  logError: (error: Error, context?: any) => {
    logger.error({
      message: error.message,
      stack: error.stack,
      context,
      timestamp: new Date().toISOString(),
    });
  },
};

// Create logs directory if it doesn't exist
import fs from 'fs';
const logsDir = path.join(process.cwd(), 'logs');
if (!fs.existsSync(logsDir)) {
  fs.mkdirSync(logsDir, { recursive: true });
}

// Handle uncaught exceptions and unhandled rejections
process.on('uncaughtException', (error) => {
  logger.error('Uncaught Exception:', error);
  process.exit(1);
});

process.on('unhandledRejection', (reason, promise) => {
  logger.error('Unhandled Rejection at:', promise, 'reason:', reason);
});

// Export default logger
export default logger;


--------------------------------------------------------------------------------
FILE: backend\data-service\src\utils\normalizeType.ts
--------------------------------------------------------------------------------
// Normalize external aliases to canonical internal types
export type KnownType =
  | 'postgres' | 'postgresql' | 'mysql' | 'mssql' | 'mongodb' | 'redis'
  | 'snowflake' | 'bigquery' | 'redshift' | 'databricks' | 's3' | 'azure-blob'
  | 'gcs' | 'kafka' | 'api' | 'file' | 'ftp' | 'elasticsearch' | 'oracle';

export function normalizeDataSourceType(t?: string): KnownType | undefined {
  if (!t) return undefined;
  const lower = t.toLowerCase();
  switch (lower) {
    case 'postgres': return 'postgresql';
    default: return lower as KnownType;
  }
}



--------------------------------------------------------------------------------
FILE: backend\data-service\src\utils\pagination.ts
--------------------------------------------------------------------------------
export type PageQuery = { page?: any; pageSize?: any; sort?: any; dir?: any };

export function pageParams(q: PageQuery, allowedSort: string[] = ['created_at']) {
  const page = Math.max(1, Number(q.page) || 1);
  const pageSize = Math.min(100, Math.max(1, Number(q.pageSize) || 20));
  const sort = allowedSort.includes(String(q.sort || '')) ? String(q.sort) : allowedSort[0];
  const dir = (String(q.dir).toLowerCase() === 'desc' ? 'desc' : 'asc') as 'asc'|'desc';
  const offset = (page - 1) * pageSize;
  return { page, pageSize, sort, dir, offset };
}



--------------------------------------------------------------------------------
FILE: backend\data-service\tsconfig.json
--------------------------------------------------------------------------------
{
  "compilerOptions": {
    /* Node 18+ friendly output */
    "target": "ES2022",
    "lib": ["ES2022"],
    "module": "CommonJS",
    "moduleResolution": "Node",

    /* Project structure */
    "rootDir": "src",
    "outDir": "dist",

    /* Build mode support (so `tsc --build` works) */
    "composite": true,
    "tsBuildInfoFile": "dist/.tsbuildinfo",

    /* Interop / JSON */
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "resolveJsonModule": true,

    /* Strictness & hygiene */
    "strict": true,
    "noImplicitOverride": true,
    "forceConsistentCasingInFileNames": true,
    "skipLibCheck": true,
    "noEmitOnError": true,

    /* Node types */
    "types": ["node"],

    /* Nice absolute imports like "@/utils/logger" */
    "baseUrl": "src",
    "paths": {
      "@/*": ["*"]
    },

    /* Useful in dev; keep if you want stack traces mapped to TS */
    "sourceMap": true
  },
  "include": [
    "src/**/*"
  ],
  "exclude": [
    "dist",
    "node_modules"
  ]
}



========================================================================================================================
  BACKEND SERVICE: AI-SERVICE
========================================================================================================================


--------------------------------------------------------------------------------
FILE: backend\ai-service\.env
--------------------------------------------------------------------------------
NODE_ENV=development
PORT=8003
HOST=0.0.0.0

# OpenAI (real)
OPENAI_API_KEY=sk-xxxxxxx
OPENAI_MODEL=gpt-4o-mini
OPENAI_MAX_TOKENS=3000
OPENAI_TEMPERATURE=0.1

# --- Auth (dev) ---
JWT_SECRET=cwic-dev-secret
JWT_ISSUER=cwic
JWT_AUDIENCE=cwic-web
JWT_ALLOWED_ALGS=HS256
AUTH_COOKIE=auth_token

# Rate limits (optional)
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100

# CORS (optional, comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:5173



--------------------------------------------------------------------------------
FILE: backend\ai-service\Dockerfile
--------------------------------------------------------------------------------
# ---------- BUILD STAGE ----------
FROM node:20-alpine3.20 AS build
WORKDIR /app

# Copy manifest files
COPY package*.json ./
COPY tsconfig.json ./

# Install deps (works with or without lockfile)
RUN if [ -f package-lock.json ]; then npm ci; else npm install; fi

# Copy sources
COPY src ./src

# Build TS and rewrite path aliases (requires: "build:prod": "tsc && tsc-alias && npm run copy-assets")
RUN npm run build:prod

# ---------- RUNTIME STAGE ----------
FROM node:20-alpine3.20 AS runtime
WORKDIR /app

ENV NODE_ENV=production
ENV PORT=8003
ENV HOST=0.0.0.0

# Apply security updates and install a tiny init
RUN apk --no-cache upgrade && apk add --no-cache dumb-init

# Copy only what's needed to run
COPY package*.json ./
# Install production deps (works with or without lockfile)
RUN if [ -f package-lock.json ]; then npm ci --omit=dev; else npm install --omit=dev; fi

COPY --from=build /app/dist ./dist

# Run as non-root
RUN addgroup -g 1001 -S nodejs && adduser -S cwic -u 1001
USER cwic

EXPOSE 8003

HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD node -e "require('http').get('http://127.0.0.1:'+ (process.env.PORT||8003) +'/health', r=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))"

ENTRYPOINT ["dumb-init","--"]
CMD ["node","dist/server.js"]



--------------------------------------------------------------------------------
FILE: backend\ai-service\Dockerfile.dev
--------------------------------------------------------------------------------
# ---------- DEV STAGE ----------
FROM node:20-alpine

# System deps for better builds & file watching
RUN apk add --no-cache bash git tini

WORKDIR /app

# Copy only manifests first to leverage layer caching
COPY package*.json ./
COPY tsconfig.json ./

# Install ALL deps (prod + dev) for dev workflow
# Works with or without a lockfile
RUN if [ -f package-lock.json ]; then npm ci; else npm install; fi

# Ensure dev-time tools present (safety in case package.json changes)
RUN npm i -D ts-node tsconfig-paths nodemon

# Copy source last (so we can cache node_modules above)
COPY src ./src

# Environment
ENV NODE_ENV=development \
    PORT=8003 \
    HOST=0.0.0.0 \
    TS_NODE_TRANSPILE_ONLY=1

# Expose port
EXPOSE 8003

# Nodemon config (runs ts-node with tsconfig-paths/register)
# You can also move this into package.json scripts if you prefer.
CMD ["tini", "--", "npx", "nodemon", \
    "--watch", "src", \
    "--ext", "ts,json", \
    "--exec", "ts-node", "-r", "tsconfig-paths/register", "src/server.ts"]



--------------------------------------------------------------------------------
FILE: backend\ai-service\package.json
--------------------------------------------------------------------------------
{
  "name": "cwic-ai-service",
  "version": "1.0.0",
  "description": "AI Service for CWIC Data Governance Platform",
  "main": "dist/server.js",
  "scripts": {
    "start": "node dist/server.js",
    "dev": "cross-env NODE_ENV=development TS_NODE_TRANSPILE_ONLY=1 nodemon --watch src --ext ts,json --exec \"ts-node -r tsconfig-paths/register src/server.ts\"",
    "build": "tsc",
    "build:prod": "tsc && tsc-alias && npm run copy-assets",
    "copy-assets": "node -e \"try{require('fs').cpSync('src/assets','dist/assets',{recursive:true})}catch(e){}\"",
    "test": "jest",
    "test:watch": "jest --watch",
    "test:coverage": "jest --coverage",
    "lint": "eslint \"src/**/*.ts\"",
    "lint:fix": "eslint \"src/**/*.ts\" --fix",
    "format": "prettier --write \"src/**/*.ts\"",
    "typecheck": "tsc --noEmit",
    "clean": "node -e \"try{require('fs').rmSync('dist',{recursive:true,force:true})}catch(e){}\"",
    "docker:build": "docker build -t cwic-ai-service .",
    "docker:run": "docker run -p 8003:8003 cwic-ai-service",
    "docker:dev": "docker-compose -f docker-compose.dev.yml up",
    "docker:prod": "docker-compose up -d",
    "setup": "npm install && npm run build",
    "prepare": "node -e \"try{require('husky').install()}catch(e){process.exit(0)}\""
  },
  "dependencies": {
    "axios": "^1.5.0",
    "bcryptjs": "^2.4.3",
    "compression": "^1.7.4",
    "cors": "^2.8.5",
    "csv-parser": "^3.0.0",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "express-rate-limit": "^6.11.2",
    "express-validator": "^7.0.1",
    "helmet": "^7.2.0",
    "joi": "^17.9.2",
    "jsonwebtoken": "^9.0.2",
    "morgan": "^1.10.1",
    "multer": "^1.4.5-lts.1",
    "node-cron": "^3.0.2",
    "openai": "^4.104.0",
    "pg": "^8.11.3",
    "prom-client": "^14.2.0",
    "redis": "^4.6.8",
    "uuid": "^9.0.1",
    "winston": "^3.10.0",
    "winston-daily-rotate-file": "^4.7.1",
    "xlsx": "^0.18.5",
    "zod": "^3.25.76"
  },
  "devDependencies": {
    "@types/bcryptjs": "^2.4.2",
    "@types/compression": "^1.7.2",
    "@types/cors": "^2.8.13",
    "@types/express": "^5.0.3",
    "@types/helmet": "^0.0.48",
    "@types/jest": "^29.5.4",
    "@types/jsonwebtoken": "^9.0.2",
    "@types/morgan": "^1.9.10",
    "@types/multer": "^1.4.7",
    "@types/node": "^20.19.11",
    "@types/node-cron": "^3.0.8",
    "@types/pg": "^8.10.2",
    "@types/supertest": "^2.0.12",
    "@types/uuid": "^9.0.8",
    "@typescript-eslint/eslint-plugin": "^6.2.1",
    "@typescript-eslint/parser": "^6.2.1",
    "eslint": "^8.46.0",
    "husky": "^8.0.3",
    "jest": "^29.6.2",
    "lint-staged": "^13.2.3",
    "nodemon": "^3.0.1",
    "prettier": "^3.0.1",
    "supertest": "^6.3.3",
    "ts-jest": "^29.1.1",
    "ts-node": "^10.9.1",
    "tsc-alias": "^1.8.16",
    "tsconfig-paths": "^4.2.0",
    "typescript": "^5.1.6"
  },
  "engines": {
    "node": ">=18.0.0",
    "npm": ">=8.0.0"
  },
  "keywords": [
    "ai",
    "data-governance",
    "discovery",
    "openai",
    "typescript",
    "microservice"
  ],
  "author": "CWIC Team",
  "license": "MIT",
  "lint-staged": {
    "*.ts": [
      "eslint --fix",
      "prettier --write",
      "git add"
    ]
  }
}



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\app.ts
--------------------------------------------------------------------------------
// src/app.ts
import compression from 'compression';
import cors from 'cors';
import express, { Application, NextFunction, Request, RequestHandler, Response } from 'express';
import helmet from 'helmet';
import morgan from 'morgan';

class App {
  public app: Application;

  constructor() {
    this.app = express();
    console.log('ðŸ”„ Creating Express app...');
    this.initializeMiddlewares();
    console.log('âœ… Express app created');
    this.initializeRoutes();
    console.log('ðŸ”„ Initializing basic middlewares...');
    this.initializeErrorHandling();
    console.log('âœ… Basic middlewares initialized');
  }

  private initializeMiddlewares(): void {
    // Security middleware
    this.app.use(helmet({
      contentSecurityPolicy: {
        directives: {
          defaultSrc: ["'self'"],
          styleSrc: ["'self'", "'unsafe-inline'"],
          scriptSrc: ["'self'"],
          imgSrc: ["'self'", "data:", "https:"],
        },
      },
      crossOriginEmbedderPolicy: false
    }) as RequestHandler);

    // CORS configuration
    const corsOptions = {
      origin: process.env.NODE_ENV === 'production' 
        ? process.env.CORS_ORIGINS?.split(',') || ['https://your-frontend-domain.com']
        : ['http://localhost:3000', 'http://127.0.0.1:3000', 'http://localhost:5173'],
      credentials: true,
      optionsSuccessStatus: 200
    };
    this.app.use(cors(corsOptions) as RequestHandler);

    // Compression
    this.app.use(compression() as RequestHandler);

    // Body parsing
    this.app.use(express.json({ limit: '10mb' }) as RequestHandler);
    this.app.use(express.urlencoded({ extended: true, limit: '10mb' }) as RequestHandler);

    // Logging
    const morganFormat = process.env.NODE_ENV === 'production' ? 'combined' : 'dev';
    this.app.use(morgan(morganFormat) as RequestHandler);

    // Request ID middleware
    this.app.use((req: Request, _res: Response, next: NextFunction) => {
      (req as any).id = `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      next();
    });

    // Security headers
    this.app.use((_req: Request, res: Response, next: NextFunction) => {
      res.setHeader('X-Content-Type-Options', 'nosniff');
      res.setHeader('X-Frame-Options', 'DENY');
      res.setHeader('X-XSS-Protection', '1; mode=block');
      res.setHeader('Referrer-Policy', 'strict-origin-when-cross-origin');
      res.removeHeader('X-Powered-By');
      next();
    });
  }

  private initializeRoutes(): void {
    console.log('ðŸ”„ Initializing basic routes...');
    
    // Health check (before any middleware that might block it)
    this.app.get('/health', (_req: Request, res: Response) => {
      const memoryUsage = process.memoryUsage();
      res.status(200).json({
        status: 'healthy',
        service: 'ai-service',
        timestamp: new Date().toISOString(),
        version: process.env.APP_VERSION || '1.0.0',
        uptime: Math.floor(process.uptime()),
        environment: process.env.NODE_ENV || 'development',
        memory: {
          used: Math.round(memoryUsage.heapUsed / 1024 / 1024),
          total: Math.round(memoryUsage.heapTotal / 1024 / 1024),
          usage: Math.round((memoryUsage.heapUsed / memoryUsage.heapTotal) * 100)
        }
      });
    });

    // Root endpoint
    this.app.get('/', (_req: Request, res: Response) => {
      res.status(200).json({
        message: 'CWIC AI Service',
        version: process.env.APP_VERSION || '1.0.0',
        status: 'running',
        endpoints: {
          health: '/health',
          api: '/api',
          docs: '/api/docs'
        },
        timestamp: new Date().toISOString()
      });
    });

    // API documentation endpoint
    this.app.get('/api/docs', (_req: Request, res: Response) => {
      res.status(200).json({
        service: 'CWIC AI Service',
        version: process.env.APP_VERSION || '1.0.0',
        description: 'AI-powered data governance and discovery service',
        endpoints: {
          discovery: {
            'POST /api/discovery': 'Start data discovery session',
            'GET /api/discovery/:sessionId': 'Get discovery status',
            'POST /api/discovery/query': 'Natural language queries',
            'POST /api/discovery/quality-rules': 'Generate quality rules'
          },
          analysis: {
            'POST /api/analysis/schema': 'Analyze database schema',
            'POST /api/analysis/quality': 'Data quality analysis'
          },
          health: {
            'GET /health': 'Service health check',
            'GET /health/ready': 'Readiness probe',
            'GET /health/live': 'Liveness probe'
          }
        }
      });
    });

    // API status endpoint (fallback)
    this.app.get('/api/status', (_req: Request, res: Response) => {
      res.status(200).json({
        status: 'AI Service API Ready',
        timestamp: new Date().toISOString(),
        routes: 'Basic routes loaded'
      });
    });

    // Try to load API routes (gracefully handle if not available)
   try {
  // IMPORTANT: prefer require here because ts-node is already running with tsconfig-paths/register
  // If this throws, you'll see the exact error below.
    const routes = require('./routes');
    this.app.use('/api', routes.default || routes);
    console.log('âœ… API routes loaded successfully');
  } catch (routeError: any) {
    console.error('âŒ Failed to load API routes:', {
      message: routeError?.message,
      stack: routeError?.stack,
    });

    // Fallback API route (keep this so app still runs)
    this.app.use('/api', (_req: Request, res: Response) => {
      res.status(503).json({
        error: 'API routes failed to load',
        message: routeError?.message || 'Service is starting up',
        retry: 'Fix the routes import error shown in server logs',
      });
    });
  }

    console.log('âœ… Basic routes initialized');
  }

  private initializeErrorHandling(): void {
    // 404 handler
    this.app.use((_req: Request, res: Response) => {
      res.status(404).json({
        success: false,
        error: {
          code: 'NOT_FOUND',
          message: 'The requested resource was not found',
          path: _req.path
        },
        timestamp: new Date().toISOString()
      });
    });

    // Global error handler
    this.app.use((error: unknown, _req: Request, res: Response, _next: NextFunction) => {
      // Type guard to check if error is an Error object
      const isError = error instanceof Error;
      const errorMessage = isError ? error.message : 'Unknown error occurred';
      const errorStack = isError ? error.stack : undefined;

      // Log error safely
      if (typeof console !== 'undefined') {
        console.error('âŒ Application Error:', {
          message: errorMessage,
          stack: errorStack,
          url: _req.url,
          method: _req.method
        });
      }

      // Determine status code
      let statusCode = 500;
      if (isError && 'statusCode' in error) {
        statusCode = (error as any).statusCode || 500;
      }

      // Send error response
      const response: any = {
        success: false,
        error: {
          code: 'INTERNAL_SERVER_ERROR',
          message: process.env.NODE_ENV === 'development' 
            ? errorMessage 
            : 'An internal server error occurred'
        },
        timestamp: new Date().toISOString()
      };

      // Include stack trace in development
      if (process.env.NODE_ENV === 'development' && errorStack) {
        response.error.stack = errorStack;
      }

      if (!res.headersSent) {
        res.status(statusCode).json(response);
      }
    });
  }

  public getApp(): Application {
    return this.app;
  }

  public async shutdown(): Promise<void> {
    console.log('ðŸ”„ Shutting down application...');
    // Add cleanup logic here if needed
    console.log('âœ… Application shutdown complete');
  }
}

export default App;


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\config\database.ts
--------------------------------------------------------------------------------
import { logger } from '@utils/logger';
import { Pool } from 'pg';

class DatabaseConfig {
  private pool: Pool | null = null;

  constructor() {
    this.initializePool();
  }

  private initializePool(): void {
    try {
      this.pool = new Pool({
        connectionString: process.env.DATABASE_URL,
        min: parseInt(process.env.DATABASE_POOL_MIN || '2'),
        max: parseInt(process.env.DATABASE_POOL_MAX || '10'),
        idleTimeoutMillis: 30000,
        connectionTimeoutMillis: 2000,
        ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false
      });

      // Handle pool errors
      this.pool.on('error', (err) => {
        logger.error('Database pool error:', err);
      });

      // Handle client connection errors
      this.pool.on('connect', () => {
        logger.debug('New database client connected');
      });

    } catch (error) {
      logger.error('Failed to initialize database pool:', error);
      throw error;
    }
  }

  public getPool(): Pool {
    if (!this.pool) {
      throw new Error('Database pool not initialized');
    }
    return this.pool;
  }

  public async query(text: string, params?: any[]): Promise<any> {
    const client = await this.getPool().connect();
    try {
      const start = Date.now();
      const result = await client.query(text, params);
      const duration = Date.now() - start;
      
      logger.debug('Query executed', {
        query: text,
        duration: `${duration}ms`,
        rows: result.rowCount
      });
      
      return result;
    } catch (error) {
      logger.error('Database query error:', { query: text, error });
      throw error;
    } finally {
      client.release();
    }
  }

  public async transaction(callback: (client: any) => Promise<any>): Promise<any> {
    const client = await this.getPool().connect();
    try {
      await client.query('BEGIN');
      const result = await callback(client);
      await client.query('COMMIT');
      return result;
    } catch (error) {
      await client.query('ROLLBACK');
      logger.error('Transaction error:', error);
      throw error;
    } finally {
      client.release();
    }
  }

  public async close(): Promise<void> {
    if (this.pool) {
      await this.pool.end();
      this.pool = null;
      logger.info('Database pool closed');
    }
  }
}

export const db = new DatabaseConfig();

export async function connectDatabase(): Promise<void> {
  try {
    await db.query('SELECT 1');
    logger.info('Database connection verified');
  } catch (error) {
    logger.error('Database connection failed:', error);
    throw error;
  }
}


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\config\openai.ts
--------------------------------------------------------------------------------
// src/config/openai.ts
import { logger } from '@utils/logger';
import OpenAI from 'openai';

type ChatRole = 'system' | 'user' | 'assistant';

export interface ChatMessage {
  role: ChatRole;
  content: string;
}

export interface ChatArgs {
  model?: string;
  messages: ChatMessage[];
  max_tokens?: number;
  temperature?: number;
  response_format?: { type: 'json_object' | 'text' };
}

export interface ChatResponse {
  choices: Array<{ message: { content: string } }>;
}

class OpenAIAdapter {
  private client: OpenAI | null = null;
  private enabled = false;

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY?.trim();
    if (apiKey) {
      try {
        this.client = new OpenAI({ apiKey });
        this.enabled = true;
        logger.info('OpenAI client initialized');
      } catch (err) {
        this.client = null;
        this.enabled = false;
        logger.error('Failed to initialize OpenAI client (using stub):', err as any);
      }
    } else {
      logger.warn('OPENAI_API_KEY not set. Using stub OpenAI client.');
      this.enabled = false;
    }
  }

  public isAvailable(): boolean {
    return this.enabled && !!this.client;
  }

  /**
   * Mirrors the shape your AIService expects:
   * await openai.createChatCompletion({...})
   * -> returns { choices: [{ message: { content: string } }] }
   */
  public async createChatCompletion(args: ChatArgs): Promise<ChatResponse> {
    // Real client path
    if (this.isAvailable() && this.client) {
      const model = args.model || process.env.OPENAI_MODEL || 'gpt-4o-mini';
      const res = await this.client.chat.completions.create({
        model,
        messages: args.messages,
        max_tokens: args.max_tokens,
        temperature: args.temperature,
        // The SDK accepts response_format on chat.completions
        response_format: args.response_format as any,
      });

      // Conform to your expected return type
      return {
        choices: [
          {
            message: {
              content: res.choices?.[0]?.message?.content ?? '',
            },
          },
        ],
      };
    }

    // Stubbed fallback (no API key): return a JSON-ish payload so your services can parse it.
    const stubJson = JSON.stringify({
      // field discovery defaults
      fields: [],
      recommendations: { governance: [], quality: [], compliance: [] },
      confidence: 0.5,
      // nlq defaults
      sql: 'SELECT 1;',
      explanation: 'OpenAI disabled; returning stubbed content.',
      tables: [],
      fieldsUsed: [],
      warnings: ['OpenAI disabled (stub)'],
    });

    logger.warn('OpenAI call intercepted by stub. Set OPENAI_API_KEY to enable real calls.');
    return { choices: [{ message: { content: stubJson } }] };
  }
}

export const openai = new OpenAIAdapter();



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\config\redis.ts
--------------------------------------------------------------------------------
import { logger } from '@utils/logger';
import { createClient, RedisClientType } from 'redis';

class RedisConfig {
  private client: RedisClientType | null = null;

  constructor() {
    this.initializeClient();
  }

  private initializeClient(): void {
    try {
      this.client = createClient({
        url: process.env.REDIS_URL || 'redis://localhost:6379',
        socket: {
          reconnectStrategy: (retries) => Math.min(retries * 50, 1000)
        }
      });

      this.client.on('error', (err) => {
        logger.error('Redis client error:', err);
      });

      this.client.on('connect', () => {
        logger.info('Redis client connected');
      });

      this.client.on('ready', () => {
        logger.info('Redis client ready');
      });

      this.client.on('end', () => {
        logger.info('Redis client disconnected');
      });

    } catch (error) {
      logger.error('Failed to initialize Redis client:', error);
      throw error;
    }
  }

  public getClient(): RedisClientType {
    if (!this.client) {
      throw new Error('Redis client not initialized');
    }
    return this.client;
  }

  public async set(key: string, value: string, ttl?: number): Promise<void> {
    try {
      if (ttl) {
        await this.getClient().setEx(key, ttl, value);
      } else {
        await this.getClient().set(key, value);
      }
    } catch (error) {
      logger.error('Redis SET error:', { key, error });
      throw error;
    }
  }

  public async get(key: string): Promise<string | null> {
    try {
      return await this.getClient().get(key);
    } catch (error) {
      logger.error('Redis GET error:', { key, error });
      throw error;
    }
  }

  public async del(key: string): Promise<void> {
    try {
      await this.getClient().del(key);
    } catch (error) {
      logger.error('Redis DEL error:', { key, error });
      throw error;
    }
  }

  public async exists(key: string): Promise<boolean> {
    try {
      const result = await this.getClient().exists(key);
      return result === 1;
    } catch (error) {
      logger.error('Redis EXISTS error:', { key, error });
      throw error;
    }
  }

  public async close(): Promise<void> {
    if (this.client) {
      await this.client.quit();
      this.client = null;
      logger.info('Redis client closed');
    }
  }
}

export const redis = new RedisConfig();

export async function connectRedis(): Promise<void> {
  try {
    await redis.getClient().connect();
    logger.info('Redis connection established');
  } catch (error) {
    logger.error('Redis connection failed:', error);
    throw error;
  }
}


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\controllers\AnalysisController.ts
--------------------------------------------------------------------------------
import { AnalysisService } from '@/services/AnalysisService';
import { APIError } from '@/utils/errors';
import { logger } from '@/utils/logger';
import { successResponse } from '@/utils/responses';
import { NextFunction, Request, Response } from 'express';

export class AnalysisController {
  private analysisService: AnalysisService;

  constructor() {
    this.analysisService = new AnalysisService();
  }

  public analyzeSchema = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { schema } = req.body;
      const userId = (req as any).user.id;

      if (!schema) {
        throw new APIError('Schema information is required', 400);
      }

      const analysis = await this.analysisService.analyzeSchema(schema, userId);

      logger.info('Schema analysis completed', { 
        userId, 
        schema: schema.name,
        tables: schema.tables?.length || 0
      });

      res.json(successResponse(analysis, 'Schema analysis completed'));

    } catch (error) {
      next(error);
    }
  };

  public analyzeDataSample = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { samples } = req.body;
      const userId = (req as any).user.id;

      if (!samples || !Array.isArray(samples)) {
        throw new APIError('Data samples are required', 400);
      }

      const analysis = await this.analysisService.analyzeDataSample(samples, userId);

      logger.info('Data sample analysis completed', { 
        userId,
        samplesAnalyzed: samples.length
      });

      res.json(successResponse(analysis, 'Data sample analysis completed'));

    } catch (error) {
      next(error);
    }
  };

  public performQualityCheck = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { dataSourceId, rules } = req.body;
      const userId = (req as any).user.id;

      if (!dataSourceId) {
        throw new APIError('Data source ID is required', 400);
      }

      const qualityReport = await this.analysisService.performQualityCheck(dataSourceId, rules, userId);

      logger.info('Quality check completed', { 
        userId,
        dataSourceId,
        rulesChecked: rules?.length || 0
      });

      res.json(successResponse(qualityReport, 'Quality check completed'));

    } catch (error) {
      next(error);
    }
  };
}


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\controllers\DiscoveryController.ts
--------------------------------------------------------------------------------
import { AIService, NaturalLanguageQuery } from '@services/AIService';
import { DiscoveryService, StartDiscoveryRequest } from '@services/DiscoveryService';
import { APIError } from '@utils/errors';
import { logger } from '@utils/logger';
import { successResponse } from '@utils/responses';
import { NextFunction, Request, Response } from 'express';

export class DiscoveryController {
  private discoveryService: DiscoveryService;
  private aiService: AIService;

  constructor() {
    this.discoveryService = new DiscoveryService();
    this.aiService = new AIService();
  }

  public startDiscovery = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { dataSourceId, schemas, tables, options } = req.body;
      const userId = (req as any).user.id;

      if (!dataSourceId) {
        throw new APIError('Data source ID is required', 400);
      }

      const request: StartDiscoveryRequest = {
        userId,
        dataSourceId,
        schemas,
        tables,
        options
      };

      const session = await this.discoveryService.startDiscovery(request);

      logger.info('Discovery started', { sessionId: session.sessionId, userId });

      res.status(201).json(successResponse(session, 'Discovery session started'));

    } catch (error) {
      next(error);
    }
  };

  public getDiscoveryStatus = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { sessionId } = req.params;
      const userId = (req as any).user.id;

      const session = await this.discoveryService.getSession(sessionId);

      if (!session) {
        throw new APIError('Discovery session not found', 404);
      }

      if (session.userId !== userId) {
        throw new APIError('Access denied', 403);
      }

      res.json(successResponse(session));

    } catch (error) {
      next(error);
    }
  };

  public listDiscoverySessions = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const userId = (req as any).user.id;
      const limit = parseInt(req.query.limit as string) || 20;
      const offset = parseInt(req.query.offset as string) || 0;

      const sessions = await this.discoveryService.listSessions(userId, limit, offset);

      res.json(successResponse({
        sessions,
        pagination: {
          limit,
          offset,
          total: sessions.length
        }
      }));

    } catch (error) {
      next(error);
    }
  };

  public deleteDiscoverySession = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { sessionId } = req.params;
      const userId = (req as any).user.id;

      await this.discoveryService.deleteSession(sessionId, userId);

      logger.info('Discovery session deleted', { sessionId, userId });

      res.json(successResponse(null, 'Discovery session deleted'));

    } catch (error) {
      next(error);
    }
  };

  public processNaturalLanguageQuery = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
  try {
    const { query, context } = req.body;
    if (!query) throw new APIError('Query is required', 400);

    const nlQuery: NaturalLanguageQuery = { query, context };
    const result = await this.aiService.processNaturalLanguageQuery(nlQuery);

    // ðŸ”§ Normalize to the frontendâ€™s expected AIResponse.data shape
    const payload = {
      message: "Hereâ€™s the SQL Iâ€™d run based on your request.",
      type: 'query' as const,
      results: {
        sql:          result.sql,
        explanation:  result.explanation,
        tables:       result.tables,
        fields:       result.fields,
        confidence:   result.confidence,
        warnings:     result.warnings,
        isAiGenerated: result.isAiGenerated
      },
      suggestions: [
        'Add a date range filter',
        'Group results by day',
        'Limit to the last 1,000 rows',
        'Explain this query step-by-step'
      ],
      actions: [
        { type: 'view_details', label: 'Copy SQL', payload: { sql: result.sql } },
        { type: 'export_data',  label: 'Export as CSV' }
      ]
    };

    res.json(successResponse(payload, 'Natural language query processed'));
  } catch (error) {
    next(error);
  }
};

  public generateQualityRules = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { fieldInfo } = req.body;

      if (!fieldInfo) {
        throw new APIError('Field information is required', 400);
      }

      const rules = await this.aiService.generateQualityRules(fieldInfo);

      res.json(successResponse({ rules }));

    } catch (error) {
      next(error);
    }
  };

  public explainViolation = async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      const { violation } = req.body;

      if (!violation) {
        throw new APIError('Violation information is required', 400);
      }

      const explanation = await this.aiService.explainViolation(violation);

      res.json(successResponse({ explanation }));

    } catch (error) {
      next(error);
    }
  };
}


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\controllers\HealthController.ts
--------------------------------------------------------------------------------
// src/controllers/HealthController.ts
import type { NextFunction, Request, Response } from 'express';
import { unlink, writeFile } from 'fs/promises';
import os from 'os';
import path from 'path';

import { db } from '@config/database';
import { openai } from '@config/openai';
import { redis } from '@config/redis';
import { APIError } from '@utils/errors';
import { logger } from '@utils/logger';
import { errorResponse, successResponse } from '@utils/responses';

type Status = 'healthy' | 'degraded' | 'unhealthy';

interface HealthCheck {
  name: string;
  status: Status;
  responseTime: number; // ms
  details?: unknown;
  error?: string;
}

interface SystemHealth {
  status: Status;
  service: string;
  version: string;
  timestamp: string;
  uptime: number;
  environment: string;
  checks: HealthCheck[];
  summary: { total: number; healthy: number; degraded: number; unhealthy: number };
  system: {
    memory: NodeJS.MemoryUsage;
    cpu: { loadAverage: number[]; usageMicros: number };
    process: { pid: number; platform: string; nodeVersion: string };
  };
}

// --- bounded timeouts to avoid crazy env values
const clamp = (n: number, min: number, max: number) => Math.max(min, Math.min(max, n));
const DEFAULT_CHECK_TIMEOUT_MS = clamp(Number(process.env.HEALTH_CHECK_TIMEOUT_MS || 2000), 250, 10_000);

// Mask detailed dependency errors in production
const mask = (msg: string) => (process.env.NODE_ENV === 'development' ? msg : 'dependency error');

export class HealthController {
  private healthChecks: Map<string, () => Promise<HealthCheck>> = new Map();

  constructor() {
    this.initializeHealthChecks();
  }

  private initializeHealthChecks(): void {
    this.healthChecks.set('database', this.checkDatabase.bind(this));
    this.healthChecks.set('redis', this.checkRedis.bind(this));
    this.healthChecks.set('openai', this.checkOpenAI.bind(this));
    this.healthChecks.set('memory', this.checkMemory.bind(this));
    this.healthChecks.set('storage', this.checkStorage.bind(this));
  }

  public checkHealth = async (_req: Request, res: Response): Promise<void> => {
    const started = Date.now();
    logger.debug('Health: starting');

    const entries = Array.from(this.healthChecks.entries());
    const checks = await Promise.all(
      entries.map(([name, fn]) => this.runWithTimeout(name, fn, DEFAULT_CHECK_TIMEOUT_MS))
    );

    const summary = {
      total: checks.length,
      healthy: checks.filter(c => c.status === 'healthy').length,
      degraded: checks.filter(c => c.status === 'degraded').length,
      unhealthy: checks.filter(c => c.status === 'unhealthy').length
    };

    const overall: Status =
      summary.unhealthy > 0 ? 'unhealthy' : summary.degraded > 0 ? 'degraded' : 'healthy';

    const system = this.getSystemInfo();
    const payload: SystemHealth = {
      status: overall,
      service: 'ai-service',
      version: process.env.APP_VERSION || '1.0.0',
      timestamp: new Date().toISOString(),
      uptime: process.uptime(),
      environment: process.env.NODE_ENV || 'development',
      checks,
      summary,
      system
    };

    logger.info('Health: completed', { status: overall, durationMs: Date.now() - started, summary });

    // prevent caches from serving stale health
    res.setHeader('Cache-Control', 'no-store, no-cache, must-revalidate, proxy-revalidate');
    res
      .status(overall === 'unhealthy' ? 503 : 200)
      .json(successResponse(payload, 'Health check completed'));
  };

  public checkReadiness = async (_req: Request, res: Response, next: NextFunction): Promise<void> => {
    try {
      logger.debug('Readiness: starting');
      const critical: Array<[string, () => Promise<HealthCheck>]> = [
        ['database', this.checkDatabase.bind(this)],
        ['redis', this.checkRedis.bind(this)],
        ['openai', this.checkOpenAI.bind(this)]
      ];

      const results = await Promise.all(
        critical.map(([name, fn]) => this.runWithTimeout(name, fn, DEFAULT_CHECK_TIMEOUT_MS))
      );

      const failures = results.filter(r => r.status === 'unhealthy');

      if (failures.length) {
        logger.warn('Readiness: failed', { failures });
        res
          .status(503)
          .json(
            errorResponse('Service not ready', 503, {
              status: 'not ready',
              service: 'ai-service',
              timestamp: new Date().toISOString(),
              failures
            })
          );
        return;
      }

      logger.info('Readiness: passed');
      res.setHeader('Cache-Control', 'no-store');
      res.json(
        successResponse(
          { status: 'ready', service: 'ai-service', timestamp: new Date().toISOString(), message: 'Service is ready to accept requests' },
          'Readiness check passed'
        )
      );
    } catch (err) {
      logger.error('Readiness: error', { err });
      next(new APIError('Readiness check failed', 503, err));
    }
  };

  public checkLiveness = async (_req: Request, res: Response): Promise<void> => {
    const status = {
      status: 'alive',
      service: 'ai-service',
      timestamp: new Date().toISOString(),
      uptime: Math.floor(process.uptime()),
      pid: process.pid,
      memory: process.memoryUsage()
    };
    logger.debug('Liveness: ok', status);
    res.setHeader('Cache-Control', 'no-store');
    res.json(successResponse(status, 'Service is alive'));
  };

  public getMetrics = async (_req: Request, res: Response): Promise<void> => {
    try {
      // short sampling window to compute delta CPU
      const snap = process.cpuUsage();
      await new Promise(r => setTimeout(r, 50));
      const delta = process.cpuUsage(snap);
      const usageMicros = delta.user + delta.system;

      const metrics = {
        service: 'ai-service',
        timestamp: new Date().toISOString(),
        uptime: process.uptime(),
        memory: process.memoryUsage(),
        cpu: { loadAverage: os.loadavg(), usageMicros },
        requests: { total: 0, active: 0, errors: 0 }, // wire your counters via middleware
        cache: await this.getCacheMetrics(),
        database: await this.getDatabaseMetrics()
      };

      res.setHeader('Cache-Control', 'no-store');
      res.json(successResponse(metrics, 'Metrics retrieved'));
    } catch (err) {
      logger.error('Metrics: failed', { err });
      res.status(500).json(errorResponse('Failed to retrieve metrics', 500));
    }
  };

  // ----- Individual checks

  private async checkDatabase(): Promise<HealthCheck> {
    const t0 = Date.now();
    try {
      await db.query('SELECT 1');
      return { name: 'database', status: 'healthy', responseTime: Date.now() - t0, details: { type: 'PostgreSQL', connection: 'active' } };
    } catch (err: any) {
      return { name: 'database', status: 'unhealthy', responseTime: Date.now() - t0, error: mask(err?.message || 'db error') };
    }
  }

  private async checkRedis(): Promise<HealthCheck> {
    const t0 = Date.now();
    try {
      const key = `health:check:${Date.now()}`;
      // Redis v4: { EX: seconds }
      await (redis as any).set?.(key, 'ok', { EX: 10 });
      const val = await (redis as any).get?.(key);
      await (redis as any).del?.(key);
      if (val !== 'ok') throw new Error('Redis read/write test failed');
      return { name: 'redis', status: 'healthy', responseTime: Date.now() - t0, details: { type: 'Redis', operation: 'read/write passed' } };
    } catch (err: any) {
      return { name: 'redis', status: 'unhealthy', responseTime: Date.now() - t0, error: mask(err?.message || 'redis error') };
    }
  }

  private async checkOpenAI(): Promise<HealthCheck> {
    const t0 = Date.now();
    try {
      let ok = false;
      if (typeof (openai as any)?.testConnection === 'function') {
        ok = await (openai as any).testConnection();
      } else if ((openai as any)?.models?.list) {
        const models = await (openai as any).models.list();
        ok = Boolean(models);
      }
      return {
        name: 'openai',
        status: ok ? 'healthy' : 'degraded',
        responseTime: Date.now() - t0,
        details: { type: 'OpenAI API', available: ok },
        ...(ok ? {} : { error: 'OpenAI API not verified' })
      };
    } catch (err: any) {
      return { name: 'openai', status: 'degraded', responseTime: Date.now() - t0, error: mask(err?.message || 'OpenAI check failed') };
    }
  }

  private async checkMemory(): Promise<HealthCheck> {
    const t0 = Date.now();
    try {
      const m = process.memoryUsage();
      const pct = (m.heapUsed / Math.max(m.heapTotal, 1)) * 100;
      let status: Status = 'healthy';
      let message = 'Memory usage normal';
      if (pct > 90) { status = 'unhealthy'; message = 'Critical memory usage'; }
      else if (pct > 75) { status = 'degraded'; message = 'High memory usage'; }

      return {
        name: 'memory',
        status,
        responseTime: Date.now() - t0,
        details: {
          usagePercent: Math.round(pct * 100) / 100,
          heapUsedMB: Math.round((m.heapUsed / 1024 / 1024) * 100) / 100,
          heapTotalMB: Math.round((m.heapTotal / 1024 / 1024) * 100) / 100,
          message
        }
      };
    } catch {
      return { name: 'memory', status: 'unhealthy', responseTime: Date.now() - t0, error: 'Failed to evaluate memory' };
    }
  }

  private async checkStorage(): Promise<HealthCheck> {
    const t0 = Date.now();
    const base = process.env.HEALTH_TMP_DIR || os.tmpdir(); // configurable for read-only FS
    const file = path.join(base, `ai-service-health-${process.pid}-${Date.now()}.tmp`);
    try {
      await writeFile(file, 'health check');
      await unlink(file);
      return { name: 'storage', status: 'healthy', responseTime: Date.now() - t0, details: { type: 'filesystem', dir: base, writable: true } };
    } catch (err: any) {
      return { name: 'storage', status: 'degraded', responseTime: Date.now() - t0, error: mask(err?.message || 'Storage write test failed') };
    }
  }

  // ----- Helpers

  private getSystemInfo() {
    const mem = process.memoryUsage();
    const cpu0 = process.cpuUsage();
    const load = os.loadavg();
    return {
      memory: mem,
      cpu: { loadAverage: load, usageMicros: cpu0.user + cpu0.system },
      process: { pid: process.pid, platform: process.platform, nodeVersion: process.version }
    };
  }

  private async getCacheMetrics() {
    try {
      if (typeof (redis as any)?.getStats === 'function') {
        const stats = await (redis as any).getStats();
        return { connected: true, stats };
      }
      const pong = await (redis as any).ping?.();
      return { connected: pong === 'PONG' || pong === true };
    } catch (err: any) {
      return { connected: false, error: mask(err?.message || 'Failed to get Redis stats') };
    }
  }

  private async getDatabaseMetrics() {
    try {
      const result = await db.query(`
        SELECT 
          pg_database_size(current_database()) as database_size,
          (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_connections
      `);
      const row = (result as any).rows?.[0] || {};
      return {
        connected: true,
        size: Number(row.database_size ?? 0),
        activeConnections: Number(row.active_connections ?? 0)
      };
    } catch (err: any) {
      return { connected: false, error: mask(err?.message || 'Failed to get database metrics') };
    }
  }

  private async runWithTimeout(name: string, fn: () => Promise<HealthCheck>, timeoutMs: number): Promise<HealthCheck> {
    const started = Date.now();
    try {
      const result = await Promise.race<Promise<HealthCheck>>([
        fn(),
        new Promise<HealthCheck>((_resolve, reject) => setTimeout(() => reject(new Error(`${name} check timed out after ${timeoutMs}ms`)), timeoutMs))
      ]);
      return result;
    } catch (err: any) {
      logger.warn('Health: check failed', { name, error: err?.message });
      return { name, status: 'unhealthy', responseTime: Date.now() - started, error: mask(err?.message || 'Unknown error') };
    }
  }
}



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\interfaces\analysis.interface.ts
--------------------------------------------------------------------------------
export interface AnalysisRequest {
  dataSourceId: string;
  analysisType: 'schema' | 'data_quality' | 'compliance' | 'performance';
  options?: AnalysisOptions;
  userId: string;
}

export interface AnalysisOptions {
  includeMetadata?: boolean;
  includeSampleData?: boolean;
  sampleSize?: number;
  analysisDepth?: 'basic' | 'detailed' | 'comprehensive';
  targetTables?: string[];
  targetSchemas?: string[];
  complianceFrameworks?: ComplianceFramework[];
}

export interface AnalysisResult {
  analysisId: string;
  dataSourceId: string;
  analysisType: string;
  status: AnalysisStatus;
  progress: number;
  startedAt: Date;
  completedAt?: Date;
  results?: AnalysisData;
  error?: string;
  userId: string;
}

export interface AnalysisData {
  schemaAnalysis?: SchemaAnalysisResult;
  qualityAnalysis?: QualityAnalysisResult;
  complianceAnalysis?: ComplianceAnalysisResult;
  performanceAnalysis?: PerformanceAnalysisResult;
  summary: AnalysisSummary;
}

export interface SchemaAnalysisResult {
  totalTables: number;
  totalColumns: number;
  tableAnalysis: TableAnalysis[];
  relationshipMap: RelationshipAnalysis[];
  dataClassification: DataClassificationSummary;
  recommendations: string[];
}

export interface TableAnalysis {
  schemaName: string;
  tableName: string;
  columnCount: number;
  estimatedRowCount?: number;
  columns: ColumnAnalysis[];
  governance: TableGovernance;
  dataQuality: TableQualityMetrics;
  relationships: TableRelationship[];
}

export interface ColumnAnalysis {
  name: string;
  dataType: string;
  nullable: boolean;
  isPrimaryKey: boolean;
  isForeignKey: boolean;
  classification: DataClassification;
  sensitivity: SensitivityLevel;
  qualityScore: number;
  patterns: DataPattern[];
  statistics?: ColumnStatistics;
  recommendations: string[];
}

export interface TableGovernance {
  classification: DataClassification;
  sensitivity: SensitivityLevel;
  complianceRequirements: ComplianceFramework[];
  accessLevel: AccessLevel;
  retentionPolicy?: RetentionPolicy;
  encryptionRequired: boolean;
  auditRequired: boolean;
}

export interface TableQualityMetrics {
  completeness: number;
  validity: number;
  consistency: number;
  accuracy: number;
  uniqueness: number;
  overallScore: number;
  issues: QualityIssue[];
}

export interface QualityAnalysisResult {
  overallScore: number;
  dimensionScores: QualityDimensionScores;
  issues: QualityIssue[];
  trends: QualityTrend[];
  recommendations: QualityRecommendation[];
}

export interface QualityDimensionScores {
  completeness: number;
  validity: number;
  consistency: number;
  accuracy: number;
  uniqueness: number;
  timeliness: number;
}

export interface QualityIssue {
  id: string;
  type: QualityIssueType;
  severity: 'Critical' | 'High' | 'Medium' | 'Low';
  table: string;
  column?: string;
  description: string;
  affectedRows: number;
  suggestion: string;
  detectedAt: Date;
  status: 'Open' | 'Acknowledged' | 'Resolved' | 'Ignored';
}

export interface ComplianceAnalysisResult {
  overallScore: number;
  frameworkResults: FrameworkComplianceResult[];
  violations: ComplianceViolation[];
  recommendations: ComplianceRecommendation[];
  riskAssessment: RiskAssessment;
}

export interface FrameworkComplianceResult {
  framework: ComplianceFramework;
  score: number;
  status: 'Compliant' | 'Partially Compliant' | 'Non-Compliant';
  requirements: RequirementResult[];
  lastAssessed: Date;
}

export interface PerformanceAnalysisResult {
  queryPerformance: QueryPerformanceMetrics;
  indexAnalysis: IndexAnalysis[];
  storageAnalysis: StorageMetrics;
  recommendations: PerformanceRecommendation[];
}

export interface AnalysisSummary {
  totalTables: number;
  totalColumns: number;
  sensitiveDataTables: number;
  qualityScore: number;
  complianceScore: number;
  criticalIssues: number;
  highPriorityRecommendations: string[];
  estimatedImprovementEffort: 'Low' | 'Medium' | 'High';
}

// Supporting Enums and Types
export enum AnalysisStatus {
  PENDING = 'pending',
  IN_PROGRESS = 'in_progress',
  COMPLETED = 'completed',
  FAILED = 'failed',
  CANCELLED = 'cancelled'
}

export enum DataClassification {
  PUBLIC = 'Public',
  INTERNAL = 'Internal',
  CONFIDENTIAL = 'Confidential',
  RESTRICTED = 'Restricted',
  PII = 'PII',
  PHI = 'PHI',
  FINANCIAL = 'Financial'
}

export enum SensitivityLevel {
  LOW = 'Low',
  MEDIUM = 'Medium',
  HIGH = 'High',
  CRITICAL = 'Critical'
}

export enum ComplianceFramework {
  GDPR = 'GDPR',
  HIPAA = 'HIPAA',
  CCPA = 'CCPA',
  SOX = 'SOX',
  PCI_DSS = 'PCI-DSS',
  ISO_27001 = 'ISO-27001',
  NIST = 'NIST'
}

export enum AccessLevel {
  PUBLIC = 'Public',
  INTERNAL = 'Internal',
  RESTRICTED = 'Restricted',
  CONFIDENTIAL = 'Confidential'
}

export enum QualityIssueType {
  NULL_VALUES = 'null_values',
  DUPLICATES = 'duplicates',
  FORMAT_INCONSISTENCY = 'format_inconsistency',
  OUTLIERS = 'outliers',
  REFERENTIAL_INTEGRITY = 'referential_integrity',
  BUSINESS_RULE_VIOLATION = 'business_rule_violation'
}

// Supporting Interfaces
export interface DataPattern {
  type: string;
  pattern: string;
  confidence: number;
  examples: string[];
}

export interface ColumnStatistics {
  nullCount: number;
  uniqueCount: number;
  minValue?: any;
  maxValue?: any;
  avgValue?: number;
  standardDeviation?: number;
  distribution?: ValueDistribution[];
}

export interface ValueDistribution {
  value: any;
  count: number;
  percentage: number;
}

export interface RelationshipAnalysis {
  sourceTable: string;
  targetTable: string;
  relationshipType: 'one-to-one' | 'one-to-many' | 'many-to-many';
  foreignKeys: ForeignKeyRelation[];
  strength: number;
}

export interface ForeignKeyRelation {
  sourceColumn: string;
  targetColumn: string;
  constraintName?: string;
}

export interface TableRelationship {
  relatedTable: string;
  relationshipType: string;
  foreignKeyColumns: string[];
  isStrong: boolean;
}

export interface DataClassificationSummary {
  byClassification: Record<DataClassification, number>;
  bySensitivity: Record<SensitivityLevel, number>;
  byCompliance: Record<ComplianceFramework, number>;
  totalSensitiveFields: number;
}

export interface RetentionPolicy {
  retentionPeriod: number;
  retentionUnit: 'days' | 'months' | 'years';
  archiveAfter?: number;
  deleteAfter?: number;
  policy: string;
}

export interface QualityTrend {
  date: Date;
  dimension: string;
  score: number;
  change: number;
}

export interface QualityRecommendation {
  id: string;
  priority: 'High' | 'Medium' | 'Low';
  category: 'Data Quality' | 'Governance' | 'Compliance' | 'Performance';
  title: string;
  description: string;
  impact: string;
  effort: 'Low' | 'Medium' | 'High';
  estimatedImprovement: number;
  implementation: string[];
}

export interface ComplianceViolation {
  id: string;
  framework: ComplianceFramework;
  requirement: string;
  severity: 'Critical' | 'High' | 'Medium' | 'Low';
  table: string;
  column?: string;
  description: string;
  remediation: string;
  dueDate?: Date;
  status: 'Open' | 'In Progress' | 'Resolved';
}

export interface ComplianceRecommendation {
  framework: ComplianceFramework;
  requirement: string;
  description: string;
  priority: 'High' | 'Medium' | 'Low';
  implementation: string[];
  timeline: string;
}

export interface RiskAssessment {
  overallRisk: 'Low' | 'Medium' | 'High' | 'Critical';
  riskFactors: RiskFactor[];
  mitigationStrategies: string[];
  businessImpact: string;
}

export interface RiskFactor {
  factor: string;
  level: 'Low' | 'Medium' | 'High' | 'Critical';
  description: string;
  likelihood: number;
  impact: number;
}

export interface RequirementResult {
  requirement: string;
  status: 'Met' | 'Partially Met' | 'Not Met';
  description: string;
  evidence?: string[];
  gaps?: string[];
}

export interface QueryPerformanceMetrics {
  avgQueryTime: number;
  slowQueries: SlowQuery[];
  queryPatterns: QueryPattern[];
  resourceUtilization: ResourceUtilization;
}

export interface SlowQuery {
  query: string;
  executionTime: number;
  frequency: number;
  table: string;
  recommendation: string;
}

export interface QueryPattern {
  pattern: string;
  frequency: number;
  avgExecutionTime: number;
  optimization: string;
}

export interface ResourceUtilization {
  cpuUsage: number;
  memoryUsage: number;
  ioWait: number;
  connectionCount: number;
}

export interface IndexAnalysis {
  table: string;
  existingIndexes: IndexInfo[];
  suggestedIndexes: SuggestedIndex[];
  unusedIndexes: string[];
}

export interface IndexInfo {
  name: string;
  columns: string[];
  type: string;
  size: number;
  usage: number;
}

export interface SuggestedIndex {
  columns: string[];
  type: string;
  rationale: string;
  estimatedImprovement: number;
}

export interface StorageMetrics {
  totalSize: number;
  tablesSizes: TableSize[];
  growthRate: number;
  projectedSize: number;
  optimization: string[];
}

export interface TableSize {
  table: string;
  size: number;
  rowCount: number;
  avgRowSize: number;
}

export interface PerformanceRecommendation {
  type: 'Index' | 'Query' | 'Schema' | 'Configuration';
  description: string;
  impact: 'High' | 'Medium' | 'Low';
  effort: 'Low' | 'Medium' | 'High';
  implementation: string;
}


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\interfaces\discovery.interface.ts
--------------------------------------------------------------------------------
/* eslint-disable @typescript-eslint/consistent-type-definitions */

/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Utility & Brand Types
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */
export type Brand<K, T extends string> = K & { readonly __brand: T };

export type UUID = Brand<string, 'uuid'>;               // e.g., '2f1a2c1e-...'
export type ISODateTime = Brand<string, 'iso-datetime'>; // e.g., new Date().toISOString()
export type ByteSize = Brand<number, 'bytes'>;
export type Percentage = Brand<number, '0..100'>;

export type NonEmptyArray<T> = readonly [T, ...T[]];
export type PositiveInt = Brand<number, 'positive-int'>;

/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Shared Enums / Unions
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */
export type DiscoveryType = 'full' | 'incremental' | 'targeted';

export enum DiscoveryStatus {
  PENDING = 'pending',
  INITIALIZING = 'initializing',
  SCANNING_METADATA = 'scanning_metadata',
  SAMPLING_DATA = 'sampling_data',
  CLASSIFYING = 'classifying',
  AI_ANALYSIS = 'ai_analysis',
  QUALITY_ASSESSMENT = 'quality_assessment',
  GENERATING_INSIGHTS = 'generating_insights',
  RUNNING = 'running',
  COMPLETED = 'completed',
  FAILED = 'failed',
  CANCELLED = 'cancelled',
}

export enum PatternType {
  EMAIL = 'email',
  PHONE = 'phone',
  SSN = 'ssn',
  CREDIT_CARD = 'credit_card',
  IP_ADDRESS = 'ip_address',
  UUID = 'uuid',
  DATE = 'date',
  URL = 'url',
  CUSTOM = 'custom',
}

export type ConstraintKind = 'PRIMARY KEY' | 'FOREIGN KEY' | 'UNIQUE' | 'CHECK' | 'NOT NULL';
export type IndexMethod = 'btree' | 'hash' | 'gist' | 'gin' | 'brin' | 'spgist' | string;
export type TableLike = 'table' | 'view' | 'materialized_view';

export type DBType =
  | 'postgres'
  | 'mysql'
  | 'mariadb'
  | 'mssql'
  | 'oracle'
  | 'snowflake'
  | 'bigquery'
  | 'sqlite'
  | 'redshift'
  | string;

export type QualityDimension = 'Completeness' | 'Validity' | 'Consistency' | 'Accuracy' | 'Uniqueness';
export type DataVolume = 'Small' | 'Medium' | 'Large' | 'Very Large';
export type Criticality = 'Low' | 'Medium' | 'High' | 'Critical';
export type RiskLevel = Criticality;
export type Priority = 'Critical' | 'High' | 'Medium' | 'Low';
export type Timeline = 'Immediate' | 'Short Term' | 'Long Term';

export type GovernanceCategory = 'Access Control' | 'Data Classification' | 'Retention' | 'Compliance';

export type DataClassification =
  | 'PII'
  | 'PHI'
  | 'PCI'
  | 'Confidential'
  | 'Restricted'
  | 'Internal'
  | 'Public'
  | 'Custom';

export type SensitivityLevel = 'Low' | 'Medium' | 'High' | 'Very High';

export type ComplianceFramework =
  | 'GDPR'
  | 'HIPAA'
  | 'PCI-DSS'
  | 'SOX'
  | 'CCPA'
  | 'ISO27001'
  | 'NIST'
  | 'SOC2'
  | 'Custom';

export type ClassificationScope = 'column_name' | 'data_type' | 'data_pattern';

/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Request Models
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */

export interface DiscoveryOptions {
  readonly schemas?: readonly string[];
  readonly tables?: readonly string[];
  readonly includeSystemTables?: boolean;
  readonly includeSampleData?: boolean;
  readonly sampleSize?: PositiveInt; // rows per table when sampling
  readonly analysisDepth?: 'basic' | 'detailed' | 'comprehensive';
  readonly aiAnalysis?: boolean;
  readonly classificationRules?: readonly ClassificationRule[];
  readonly customPatterns?: readonly CustomPattern[];
}

/**
 * Discriminated DiscoveryRequest:
 * - full / incremental: options optional
 * - targeted: requires at least one of schemas/tables, and they cannot be empty
 */
interface BaseDiscoveryRequest {
  readonly dataSourceId: UUID | string;
  readonly userId: UUID | string;
}

export type DiscoveryRequest =
  | (BaseDiscoveryRequest & {
      readonly discoveryType: 'full';
      readonly options?: DiscoveryOptions;
    })
  | (BaseDiscoveryRequest & {
      readonly discoveryType: 'incremental';
      readonly options?: DiscoveryOptions & { readonly since?: ISODateTime };
    })
  | (BaseDiscoveryRequest & {
      readonly discoveryType: 'targeted';
      readonly options: DiscoveryOptions & {
        readonly schemas?: NonEmptyArray<string>;
        readonly tables?: NonEmptyArray<string>;
      };
    });

/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Session / Results
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */

export interface DiscoverySession {
  readonly sessionId: UUID | string;
  readonly userId: UUID | string;
  readonly dataSourceId: UUID | string;
  readonly discoveryType: DiscoveryType;
  readonly status: DiscoveryStatus;
  readonly progress: Percentage; // 0..100
  readonly currentStep?: string;
  readonly results?: DiscoveryResults;
  readonly error?: string;
  readonly startedAt: Date;
  readonly completedAt?: Date;
  readonly estimatedCompletion?: Date;
  readonly options: DiscoveryOptions;
}

export interface DiscoveryResults {
  readonly metadata: DataSourceMetadata;
  readonly classification: ClassificationResults;
  readonly aiInsights: AIInsights;
  readonly qualityAssessment: QualityAssessment;
  readonly summary: DiscoverySummary;
  readonly recommendations: readonly DiscoveryRecommendation[];
}

/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Metadata Shapes
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */

export interface DataSourceMetadata {
  readonly dataSourceId: UUID | string;
  readonly connectionInfo: ConnectionInfo;
  readonly schemas: readonly SchemaMetadata[];
  readonly totalTables: number;
  readonly totalColumns: number;
  readonly totalRows?: number;
  readonly dataSize?: ByteSize;
  readonly lastUpdated: Date;
}

export interface ConnectionInfo {
  readonly type: DBType;
  readonly host?: string;
  readonly port?: number;
  readonly database?: string;
  readonly version?: string;
  readonly charset?: string;
  readonly collation?: string;
}

export interface SchemaMetadata {
  readonly name: string;
  readonly tables: readonly TableMetadata[];
  readonly views: readonly ViewMetadata[];
  readonly procedures?: readonly ProcedureMetadata[];
  readonly functions?: readonly FunctionMetadata[];
}

export interface TableMetadata {
  readonly schema: string;
  readonly name: string;
  readonly type: TableLike;
  readonly columns: readonly ColumnMetadata[];
  readonly primaryKeys: readonly string[];
  readonly foreignKeys: readonly ForeignKeyMetadata[];
  readonly indexes: readonly IndexMetadata[];
  readonly constraints: readonly ConstraintMetadata[];
  readonly rowCount?: number;
  readonly sizeBytes?: ByteSize;
  readonly lastModified?: Date;
  readonly sampleData?: readonly unknown[];
}

export interface ColumnMetadata {
  readonly name: string;
  readonly dataType: string;
  readonly length?: number;
  readonly precision?: number;
  readonly scale?: number;
  readonly nullable: boolean;
  readonly defaultValue?: unknown;
  readonly autoIncrement?: boolean;
  readonly description?: string;
  readonly position: number;
}

export interface ViewMetadata {
  readonly schema: string;
  readonly name: string;
  readonly definition: string;
  readonly columns: readonly ColumnMetadata[];
  readonly dependencies: readonly string[];
}

export interface ProcedureMetadata {
  readonly schema: string;
  readonly name: string;
  readonly parameters: readonly ParameterMetadata[];
  readonly returnType?: string;
  readonly language?: string;
}

export interface FunctionMetadata {
  readonly schema: string;
  readonly name: string;
  readonly parameters: readonly ParameterMetadata[];
  readonly returnType: string;
  readonly language?: string;
}

export interface ParameterMetadata {
  readonly name: string;
  readonly dataType: string;
  readonly direction: 'IN' | 'OUT' | 'INOUT';
  readonly defaultValue?: unknown;
}

export interface ForeignKeyMetadata {
  readonly name: string;
  readonly columns: readonly string[];
  readonly referencedTable: string; // consider "schema.table"
  readonly referencedColumns: readonly string[];
  readonly onDelete?: 'NO ACTION' | 'RESTRICT' | 'CASCADE' | 'SET NULL' | 'SET DEFAULT' | string;
  readonly onUpdate?: 'NO ACTION' | 'RESTRICT' | 'CASCADE' | 'SET NULL' | 'SET DEFAULT' | string;
}

export interface IndexMetadata {
  readonly name: string;
  readonly columns: readonly string[];
  readonly unique: boolean;
  readonly type?: string;   // btree, hash... (engine-specific)
  readonly method?: IndexMethod;
}

export interface ConstraintMetadata {
  readonly name: string;
  readonly type: ConstraintKind;
  readonly columns: readonly string[];
  readonly definition?: string;
}

/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Classification / Compliance
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */

export interface ClassificationResults {
  readonly fieldClassifications: readonly FieldClassification[];
  readonly tableClassifications: readonly TableClassification[];
  readonly sensitivityMap: SensitivityMap;
  readonly complianceMapping: ComplianceMapping;
  readonly riskAssessment: DataRiskAssessment;
}

export interface FieldClassification {
  readonly schema: string;
  readonly table: string;
  readonly column: string;
  readonly dataType: string;
  readonly classification: DataClassification;
  readonly sensitivity: SensitivityLevel;
  readonly confidence: Percentage;
  readonly patterns: readonly DetectedPattern[];
  readonly tags: readonly string[];
  readonly businessContext?: string;
  readonly complianceFlags: readonly ComplianceFlag[];
}

export interface TableClassification {
  readonly schema: string;
  readonly table: string;
  readonly overallClassification: DataClassification;
  readonly overallSensitivity: SensitivityLevel;
  readonly dataVolume: DataVolume;
  readonly businessCriticality: Criticality;
  readonly accessFrequency: 'Rare' | 'Occasional' | 'Regular' | 'High';
  readonly retentionCategory: string;
  readonly complianceScope: readonly ComplianceFramework[];
}

export interface DetectedPattern {
  readonly type: PatternType;
  readonly pattern: string; // e.g., regex or named rule
  readonly confidence: Percentage;
  readonly examples: readonly string[];
  readonly description: string;
}

export interface SensitivityMap {
  readonly bySensitivity: Record<SensitivityLevel, readonly FieldClassification[]>;
  readonly byClassification: Record<DataClassification, readonly FieldClassification[]>;
  readonly sensitiveTableCount: number;
  readonly highRiskFields: readonly FieldClassification[];
}

export interface ComplianceMapping {
  readonly byFramework: Record<ComplianceFramework, ComplianceScope>;
  readonly overallComplexity: 'Low' | 'Medium' | 'High' | 'Very High';
  readonly requiredActions: readonly ComplianceAction[];
}

export interface ComplianceScope {
  readonly applicableTables: readonly string[]; // consider using "schema.table" naming
  readonly applicableFields: readonly FieldClassification[];
  readonly requirements: readonly string[];
  readonly riskLevel: RiskLevel;
}

export interface ComplianceAction {
  readonly framework: ComplianceFramework;
  readonly action: string;
  readonly priority: 'High' | 'Medium' | 'Low';
  readonly timeline: string;
  readonly effort: 'Low' | 'Medium' | 'High';
}

export interface ComplianceFlag {
  readonly framework: ComplianceFramework;
  readonly requirement: string;
  readonly severity: 'Info' | 'Warning' | 'Error';
  readonly description: string;
}

/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Risk / Quality / AI Insights
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 */

export interface DataRiskAssessment {
  readonly overallRisk: RiskLevel;
  readonly riskFactors: readonly DataRiskFactor[];
  readonly mitigationPriorities: readonly string[];
  readonly businessImpact: BusinessImpact;
}

export interface DataRiskFactor {
  readonly factor: string;
  readonly level: RiskLevel;
  readonly description: string;
  readonly affectedTables: readonly string[];
  readonly mitigationStrategy: string;
}

export interface BusinessImpact {
  readonly reputational: RiskLevel;
  readonly financial: RiskLevel;
  readonly operational: RiskLevel;
  readonly regulatory: RiskLevel;
}

export interface AIInsights {
  readonly fieldRecommendations: readonly AIFieldRecommendation[];
  readonly schemaInsights: readonly SchemaInsight[];
  readonly qualityPredictions: readonly QualityPrediction[];
  readonly governanceRecommendations: readonly GovernanceRecommendation[];
  readonly anomalies: readonly DataAnomaly[];
}

export interface AIFieldRecommendation {
  readonly schema: string;
  readonly table: string;
  readonly column: string;
  readonly recommendation: string;
  readonly reasoning: string;
  readonly confidence: Percentage;
  readonly impact: 'Low' | 'Medium' | 'High';
  readonly category: 'Classification' | 'Quality' | 'Governance' | 'Security';
}

export interface SchemaInsight {
  readonly type: 'Pattern' | 'Relationship' | 'Optimization' | 'Risk';
  readonly insight: string;
  readonly confidence: Percentage;
  readonly tables: readonly string[];
  readonly recommendation: string;
  readonly priority: 'Low' | 'Medium' | 'High';
}

export interface QualityPrediction {
  readonly table: string;
  readonly column?: string;
  readonly qualityDimension: QualityDimension;
  readonly predictedScore: Percentage; // 0..100
  readonly confidence: Percentage;     // 0..100
  readonly factors: readonly string[];
  readonly recommendations: readonly string[];
}

export interface GovernanceRecommendation {
  readonly scope: 'Field' | 'Table' | 'Schema' | 'Database';
  readonly target: string; // e.g., "schema.table.column"
  readonly recommendation: string;
  readonly category: GovernanceCategory;
  readonly priority: 'High' | 'Medium' | 'Low';
  readonly effort: 'Low' | 'Medium' | 'High';
  readonly benefit: string;
}

export interface DataAnomaly {
  readonly type: 'Statistical' | 'Pattern' | 'Relationship' | 'Quality';
  readonly table: string;
  readonly column?: string;
  readonly description: string;
  readonly severity: 'Info' | 'Warning' | 'Error';
  readonly confidence: Percentage;
  readonly suggestion: string;
}

export interface QualityAssessment {
  readonly overallScore: Percentage;
  readonly tableScores: readonly TableQualityScore[];
  readonly dimensionScores: QualityDimensionScores;
  readonly issues: readonly QualityIssue[];
  readonly trends?: readonly QualityTrend[];
}

export interface TableQualityScore {
  readonly schema: string;
  readonly table: string;
  readonly overallScore: Percentage;
  readonly completeness: Percentage;
  readonly validity: Percentage;
  readonly consistency: Percentage;
  readonly accuracy: Percentage;
  readonly uniqueness: Percentage;
  readonly issueCount: number;
  readonly recommendation: string;
}

export interface QualityDimensionScores {
  readonly [dimension: string]: Percentage | number; // allow dynamic dimensions
}

export interface QualityIssue {
  readonly id?: UUID | string;
  readonly schema: string;
  readonly table: string;
  readonly column?: string;
  readonly dimension: QualityDimension;
  readonly severity: 'Low' | 'Medium' | 'High';
  readonly description: string;
  readonly recommendation: string;
}

export interface QualityTrend {
  readonly timestamp: ISODateTime | string;
  readonly dimension: QualityDimension;
  readonly score: Percentage;
}

export interface DiscoverySummary {
  readonly totalTablesAnalyzed: number;
  readonly totalColumnsAnalyzed: number;
  readonly classificationsApplied: number;
  readonly sensitiveDataFound: number;
  readonly complianceFlags: number;
  readonly qualityIssues: number;
  readonly aiRecommendations: number;
  readonly executionTime: number; // ms
  readonly dataVolumeProcessed: ByteSize | number;
}

export interface DiscoveryRecommendation {
  readonly id: UUID | string;
  readonly category: Timeline;
  readonly priority: Priority;
  readonly type: 'Security' | 'Compliance' | 'Quality' | 'Governance' | 'Performance';
  readonly title: string;
  readonly description: string;
  readonly affectedTables: readonly string[];
  readonly businessImpact: string;
  readonly effort: 'Low' | 'Medium' | 'High';
  readonly timeline: string;
  readonly implementation: readonly ImplementationStep[];
}

export interface ImplementationStep {
  readonly step: number;
  readonly description: string;
  readonly estimatedTime: string;
  readonly dependencies?: readonly string[];
  readonly tools?: readonly string[];
}

export interface ClassificationRule {
  readonly name: string;
  readonly pattern: string; // regex or named rule
  readonly classification: DataClassification;
  readonly sensitivity: SensitivityLevel;
  readonly scope: ClassificationScope;
  readonly priority: number;
}

export interface CustomPattern {
  readonly name: string;
  readonly regex: string; // serialized regex
  readonly classification: DataClassification;
  readonly description: string;
  readonly examples: readonly string[];
}



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\interfaces\index.ts
--------------------------------------------------------------------------------
// src/interfaces/index.ts

// Namespace-style re-exports (import as `Analysis.*` or `Discovery.*`)
export * as Analysis from './analysis.interface';
export * as Discovery from './discovery.interface';

// Flatten specific items so you can `import { DiscoveryStatus } from '@/interfaces'`
export { DiscoveryStatus, DiscoveryType } from './discovery.interface';

// NOTE: Removed this because we define JobStatus/JobPriority below.
// export { JobStatus, JobPriority } from './job.interface';

// Import only for typing
import type { Request as ExpressRequest } from 'express';

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Utility & Brand Types
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export type Brand<K, T extends string> = K & { readonly __brand: T };

export type UUID = Brand<string, 'uuid'>;
export type ISODateTime = Brand<string, 'iso-datetime'>;

export type Maybe<T> = T | null | undefined;
export type Result<T, E = ServiceError> =
  | Readonly<{ ok: true; value: T }>
  | Readonly<{ ok: false; error: E }>;
export type NonEmptyArray<T> = readonly [T, ...T[]];

export type DeepReadonly<T> =
  T extends (...args: any[]) => any ? T :
  T extends Array<infer U> ? ReadonlyArray<DeepReadonly<U>> :
  T extends object ? { readonly [K in keyof T]: DeepReadonly<T[K]> } :
  T;

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Shared entities & pagination
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export interface BaseEntity {
  readonly id: UUID | string;
  readonly createdAt: Date;
  readonly updatedAt: Date;
  readonly createdBy?: UUID | string;
  readonly updatedBy?: UUID | string;
}

export type SortOrder = 'ASC' | 'DESC';

export interface PaginationParams {
  readonly page?: number;
  readonly limit?: number;
  readonly offset?: number; // (page-1)*limit (server-calculated preferred)
  readonly sortBy?: string;
  readonly sortOrder?: SortOrder;
}

export interface PaginationInfo {
  readonly page: number;
  readonly limit: number;
  readonly total: number;
  readonly totalPages: number;
  readonly hasNext: boolean;
  readonly hasPrev: boolean;
}

export interface PaginationResult<T> {
  readonly data: readonly T[];
  readonly pagination: PaginationInfo;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Filtering / query params
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export interface DateRange {
  readonly from: Date | ISODateTime | string;
  readonly to: Date | ISODateTime | string;
}

export interface FilterParams<F extends Record<string, unknown> = Record<string, unknown>> {
  readonly search?: string;
  readonly filters?: F;
  readonly dateRange?: DateRange;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * ApiRequest & Auth
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export enum UserRole {
  SUPER_ADMIN = 'super_admin',
  ADMIN = 'admin',
  DATA_STEWARD = 'data_steward',
  ANALYST = 'analyst',
  VIEWER = 'viewer'
}

export enum Permission {
  // Discovery
  DISCOVERY_READ = 'discovery:read',
  DISCOVERY_WRITE = 'discovery:write',
  DISCOVERY_DELETE = 'discovery:delete',
  // Analysis
  ANALYSIS_READ = 'analysis:read',
  ANALYSIS_WRITE = 'analysis:write',
  ANALYSIS_DELETE = 'analysis:delete',
  // Data source
  DATASOURCE_READ = 'datasource:read',
  DATASOURCE_WRITE = 'datasource:write',
  DATASOURCE_DELETE = 'datasource:delete',
  DATASOURCE_CONNECT = 'datasource:connect',
  // Admin
  USER_MANAGEMENT = 'user:management',
  SYSTEM_CONFIG = 'system:config',
  AUDIT_LOGS = 'audit:logs',
  // AI
  AI_QUERY = 'ai:query',
  AI_TRAINING = 'ai:training'
}

export interface NotificationSettings {
  readonly email: boolean;
  readonly inApp: boolean;
  readonly sms: boolean;
  readonly slack: boolean;
  readonly webhooks: boolean;
}

export interface UserPreferences {
  readonly timezone: string;
  readonly language: string;
  readonly dateFormat: string;
  readonly notifications: NotificationSettings;
}

export interface AuthenticatedUser {
  readonly id: UUID | string;
  readonly email: string;
  readonly role: UserRole;
  readonly permissions: readonly Permission[];
  readonly organizationId?: UUID | string;
  readonly preferences?: UserPreferences;
}

/** Express Request typed with body/params/query plus user */
export type ApiRequest<
  B = unknown,
  P extends Record<string, string> = Record<string, string>,
  Q extends Record<string, unknown> = Record<string, unknown>
> = ExpressRequest<P, any, B, Q> & { user?: AuthenticatedUser };

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Service response envelopes
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export interface ServiceError {
  readonly code: string; // e.g., 'VALIDATION_ERROR', 'DB_TIMEOUT'
  readonly message: string;
  readonly details?: unknown;
  readonly stack?: string; // do not expose in prod
}

export interface ResponseMeta {
  readonly timestamp: ISODateTime | string;
  readonly requestId?: string;
  readonly version: string;
  readonly processingTime?: number; // ms
}

export interface ServiceResponse<T = unknown> {
  readonly success: boolean;
  readonly data?: T;
  readonly message?: string;
  readonly error?: ServiceError;
  readonly meta?: ResponseMeta;
}

/** Generic pagination wrapper using ApiResponse pattern */
export type PaginatedResponse<T> =
  | Readonly<{
      success: true;
      data: { items: readonly T[]; pagination: PaginationInfo };
      message?: string;
      meta?: ResponseMeta;
    }>
  | Readonly<{
      success: false;
      error: ServiceError;
      meta?: ResponseMeta;
    }>;

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Health & system info
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export type HealthState = 'healthy' | 'degraded' | 'unhealthy';

export interface HealthCheck {
  readonly name: string;
  readonly status: HealthState;
  readonly responseTime: number;
  readonly details?: unknown;
  readonly error?: string;
}

export interface SystemInfo {
  readonly memory: NodeJS.MemoryUsage;
  readonly cpu: {
    readonly loadAverage: readonly number[];
    readonly usage: number; // your implementation units
  };
  readonly process: {
    readonly pid: number;
    readonly platform: string;
    readonly nodeVersion: string;
  };
}

export interface HealthStatus {
  readonly status: HealthState;
  readonly service: string;
  readonly version: string;
  readonly timestamp: ISODateTime | string;
  readonly uptime: number;
  readonly checks: readonly HealthCheck[];
  readonly system: SystemInfo;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Data sources & connection
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export enum DataSourceType {
  POSTGRESQL = 'postgresql',
  MYSQL = 'mysql',
  SQL_SERVER = 'sql_server',
  ORACLE = 'oracle',
  SNOWFLAKE = 'snowflake',
  BIGQUERY = 'bigquery',
  REDSHIFT = 'redshift',
  MONGODB = 'mongodb',
  CASSANDRA = 'cassandra',
  ELASTICSEARCH = 'elasticsearch'
}

export enum ConnectionStatus {
  CONNECTED = 'connected',
  DISCONNECTED = 'disconnected',
  ERROR = 'error',
  TESTING = 'testing',
  PENDING = 'pending',
  TIMEOUT = 'timeout'
}

export interface ConnectionConfig {
  readonly host?: string;
  readonly port?: number;
  readonly database?: string;
  readonly username?: string;
  readonly password?: string; // encrypted at rest
  readonly ssl?: boolean;
  readonly connectionTimeout?: number;
  readonly queryTimeout?: number;
  readonly poolSize?: number;
  readonly additionalProperties?: Record<string, unknown>;
}

export interface DataSource {
  readonly id: UUID | string;
  readonly name: string;
  readonly type: DataSourceType;
  readonly connectionConfig: ConnectionConfig;
  readonly status: ConnectionStatus;
  readonly lastConnected?: Date;
  readonly tags?: readonly string[];
  readonly description?: string;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Logging / audit / notifications
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export type LogLevel = 'error' | 'warn' | 'info' | 'debug';

export interface LogEntry {
  readonly level: LogLevel;
  readonly message: string;
  readonly timestamp: Date;
  readonly service: string;
  readonly requestId?: string;
  readonly userId?: UUID | string;
  readonly metadata?: Record<string, unknown>;
  readonly stack?: string;
}

export interface AuditLog extends BaseEntity {
  readonly action: string;
  readonly resource: string;
  readonly resourceId?: string;
  readonly userId: UUID | string;
  readonly ipAddress?: string;
  readonly userAgent?: string;
  readonly details?: Record<string, unknown>;
  readonly result: 'success' | 'failure';
}

export enum NotificationType {
  DISCOVERY_COMPLETED = 'discovery_completed',
  ANALYSIS_COMPLETED = 'analysis_completed',
  QUALITY_ALERT = 'quality_alert',
  COMPLIANCE_VIOLATION = 'compliance_violation',
  SYSTEM_ALERT = 'system_alert',
  SECURITY_ALERT = 'security_alert',
  DATA_BREACH = 'data_breach',
  MAINTENANCE = 'maintenance'
}

export enum NotificationChannel {
  EMAIL = 'email',
  IN_APP = 'in_app',
  SMS = 'sms',
  SLACK = 'slack',
  TEAMS = 'teams',
  WEBHOOK = 'webhook',
  PUSH = 'push'
}

export enum NotificationStatus {
  PENDING = 'pending',
  SENT = 'sent',
  DELIVERED = 'delivered',
  READ = 'read',
  FAILED = 'failed',
  CANCELLED = 'cancelled'
}

export interface Notification extends BaseEntity {
  readonly type: NotificationType;
  readonly title: string;
  readonly message: string;
  readonly userId?: UUID | string;
  readonly organizationId?: UUID | string;
  readonly channel: NotificationChannel;
  readonly status: NotificationStatus;
  readonly scheduledAt?: Date;
  readonly sentAt?: Date;
  readonly metadata?: Record<string, unknown>;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Jobs (defined here; no external file needed)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export enum JobStatus {
  PENDING = 'pending',
  RUNNING = 'running',
  COMPLETED = 'completed',
  FAILED = 'failed',
  CANCELLED = 'cancelled',
  TIMEOUT = 'timeout'
}

export enum JobPriority {
  LOW = 'low',
  NORMAL = 'normal',
  HIGH = 'high',
  CRITICAL = 'critical'
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Events
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export enum EventType {
  USER_LOGIN = 'user_login',
  USER_LOGOUT = 'user_logout',
  DATA_ACCESS = 'data_access',
  DISCOVERY_START = 'discovery_start',
  DISCOVERY_COMPLETE = 'discovery_complete',
  ANALYSIS_START = 'analysis_start',
  ANALYSIS_COMPLETE = 'analysis_complete',
  COMPLIANCE_CHECK = 'compliance_check',
  QUALITY_CHECK = 'quality_check',
  SYSTEM_ERROR = 'system_error',
  SECURITY_EVENT = 'security_event'
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * JSON types
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export type JSONValue = string | number | boolean | null | JSONObject | JSONArray;
export type JSONObject = { readonly [key: string]: JSONValue };
export type JSONArray = readonly JSONValue[];

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Forms (UI helpers)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export interface ValidationRule {
  readonly required?: boolean;
  readonly minLength?: number;
  readonly maxLength?: number;
  readonly pattern?: RegExp;
  readonly custom?: (value: unknown) => boolean | string;
}

export interface FormField {
  readonly name: string;
  readonly label: string;
  readonly type: 'text' | 'email' | 'password' | 'number' | 'select' | 'checkbox' | 'textarea';
  readonly placeholder?: string;
  readonly defaultValue?: unknown;
  readonly options?: ReadonlyArray<{ label: string; value: unknown }>;
  readonly validation?: ValidationRule;
  readonly disabled?: boolean;
  readonly required?: boolean;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Configuration
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export interface DatabaseConfig {
  readonly host: string;
  readonly port: number;
  readonly database: string;
  readonly username: string;
  readonly password: string;
  readonly ssl: boolean;
  readonly poolMin: number;
  readonly poolMax: number;
  readonly timeout: number;
}

export interface RedisConfig {
  readonly host: string;
  readonly port: number;
  readonly password?: string;
  readonly db: number;
  readonly ttl: number;
}

export interface AIConfig {
  readonly provider: 'openai' | 'azure' | 'anthropic';
  readonly apiKey: string;
  readonly model: string;
  readonly maxTokens: number;
  readonly temperature: number;
  readonly timeout: number;
}

export interface SecurityConfig {
  readonly jwtSecret: string;
  readonly jwtExpiry: string;
  readonly bcryptRounds: number;
  readonly rateLimitWindow: number;
  readonly rateLimitMax: number;
  readonly corsOrigins: readonly string[];
}

export interface MonitoringConfig {
  readonly enabled: boolean;
  readonly metricsPort: number;
  readonly healthCheckInterval: number;
  readonly logLevel: LogLevel;
  readonly logRetention: number;
}

export interface ServiceConfig {
  readonly name: string;
  readonly version: string;
  readonly environment: string;
  readonly port: number;
  readonly database: DatabaseConfig;
  readonly redis: RedisConfig;
  readonly ai: AIConfig;
  readonly security: SecurityConfig;
  readonly monitoring: MonitoringConfig;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Friendly named export aliases
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export type {
  AIConfig as TAIConfig,
  DatabaseConfig as TDatabaseConfig,
  FormField as TFormField,
  ISODateTime as TISODateTime,
  JSONArray as TJSONArray,
  JSONObject as TJSONObject,
  JSONValue as TJSONValue,
  MonitoringConfig as TMonitoringConfig,
  PaginatedResponse as TPaginatedResponse,
  RedisConfig as TRedisConfig,
  SecurityConfig as TSecurityConfig,
  ServiceConfig as TServiceConfig,
  UUID as TUUID
};

// Re-export ApiResponse type from utils (type-only to avoid runtime deps)
  export type { ApiResponse } from '../utils/responses';




--------------------------------------------------------------------------------
FILE: backend\ai-service\src\jobs\analysisJob.ts
--------------------------------------------------------------------------------
// src/jobs/analysisJob.ts
import { db } from '@/config/database';
import { redis } from '@/config/redis';
import { JobPriority, JobStatus } from '@/interfaces';
import { AnalysisService } from '@/services/AnalysisService';
import { APIError } from '@/utils/errors';
import { logger } from '@/utils/logger';

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Types
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

type AnalysisKind = 'schema' | 'data_quality' | 'compliance' | 'performance';

export interface AnalysisJobData {
  analysisId: string;
  userId: string;
  dataSourceId: string;
  analysisType: AnalysisKind;
  options?: SchemaOptions | QualityOptions | ComplianceOptions | PerformanceOptions;
  priority: JobPriority;
  retryCount?: number;
  maxRetries?: number;
}

export interface AnalysisJobResult {
  analysisId: string;
  status: JobStatus;
  result?: SchemaResult | QualityResult | ComplianceResult | PerformanceResult;
  error?: string;
  duration: number;
  completedAt: Date;
}

/** Per-type options */
export interface SchemaOptions {
  schemas?: string[];
  includeSystem?: boolean;
}
export interface QualityOptions {
  sampleSize?: number; // rows per column/table
}
export interface ComplianceOptions {
  frameworks?: Array<'GDPR' | 'HIPAA' | 'PCI-DSS' | 'SOX' | 'CCPA' | 'ISO27001' | 'NIST' | 'SOC2'>;
}
export interface PerformanceOptions {
  timeWindowMinutes?: number;
}

/** Public result contracts (mutable arrays) */
export interface SchemaResult {
  name: string;
  tables: Array<{
    schema: string;
    name: string;
    columns: Array<{ name: string; type: string; nullable: boolean }>;
  }>;
}
export interface QualityResult {
  issues: Array<{ columnName: string; type: 'format' | 'range' | 'nulls'; count: number }>;
  scores: Record<string, number>;
}
export interface ComplianceResult {
  passed: boolean;
  violations: Array<{ id: string; framework: string; description: string }>;
}
export interface PerformanceResult {
  windowMinutes: number;
  queryPerformance: {
    avgQueryTime: number;
    slowQueries: string[];
    queryPatterns: string[];
    resourceUtilization: {
      cpuUsage: number;
      memoryUsage: number;
      ioWait: number;
      connectionCount: number;
    };
  };
  recommendations: Array<{
    type: string;
    description: string;
    impact: 'Low' | 'Medium' | 'High';
    effort: 'Low' | 'Medium' | 'High';
    implementation?: string;
  }>;
}

/** DataSource rows we actually use */
interface DataSource {
  id: string;
  name: string;
  type?: string;
  config?: unknown;
}

/* Shapes coming back from services (loose, then normalized) */
type SchemaAnalysisResult = {
  name?: unknown;
  tables?: ReadonlyArray<{
    schema?: unknown;
    name?: unknown;
    columns?: ReadonlyArray<{
      name?: unknown;
      type?: unknown;
      nullable?: unknown;
    }>;
  }>;
};

type AnalyzeDataSampleResponse = {
  qualityIssues?: ReadonlyArray<{ columnName?: unknown; type?: unknown; count?: unknown }>;
  analysis?: Record<string, unknown> | ReadonlyArray<Record<string, unknown>>;
};

type QualityCheckResult = {
  results?: ReadonlyArray<{
    ruleId?: unknown;
    ruleName?: unknown;
    status?: unknown;
  }>;
};

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Config
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const DEFAULT_MAX_RETRIES = 3;
const BASE_RETRY_DELAY_MS = 5_000; // 5s
const MAX_RETRY_DELAY_MS = 60_000; // 60s cap
const REDIS_STATUS_TTL_SEC = 300; // 5m

/** Where we check for a cancel flag (optional) */
const CANCEL_KEY = (analysisId: string) => `analysis:cancel:${analysisId}`;
const STATUS_KEY = (analysisId: string) => `analysis:status:${analysisId}`;

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Utility helpers
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

function assertNever(x: never): never {
  throw new APIError(`Unsupported analysis type: ${String(x)}`, 400);
}

const backoffWithJitter = (attempt: number) => {
  const exp = Math.min(BASE_RETRY_DELAY_MS * Math.pow(2, attempt - 1), MAX_RETRY_DELAY_MS);
  const jitter = Math.floor(Math.random() * Math.min(1000, Math.floor(exp * 0.1))); // up to 10% jitter, max 1s
  return exp + jitter;
};

/** Keep log payloads small to avoid huge log lines */
const safeLogSize = (value: unknown, max = 4_096) => {
  try {
    const s = JSON.stringify(value);
    return s.length <= max ? s : s.slice(0, max) + `â€¦ (truncated ${s.length - max} chars)`;
  } catch {
    return '[unserializable]';
  }
};

/** Heuristic transient error classifier for retries */
const isTransientError = (err: unknown): boolean => {
  const anyErr = err as { message?: string; code?: string } | undefined;
  const msg = (anyErr?.message ?? '').toLowerCase();
  const code = anyErr?.code ?? '';
  return (
    code === 'ETIMEDOUT' ||
    code === 'ECONNRESET' ||
    code === 'ECONNREFUSED' ||
    msg.includes('timeout') ||
    msg.includes('deadlock') ||
    msg.includes('connection') ||
    msg.includes('too many connections') ||
    msg.includes('rate limit') ||
    msg.includes('temporarily') ||
    msg.includes('try again')
  );
};

/** Build a parameterized UPDATE statement dynamically and safely */
const buildUpdateQuery = (
  table: string,
  whereClause: string,
  whereParams: unknown[],
  data: Record<string, unknown>
) => {
  const keys = Object.keys(data);
  const setFragments = keys.map((k, i) => `${k} = $${i + whereParams.length + 1}`);
  const values = [...whereParams, ...keys.map((k) => data[k])];
  const text = `UPDATE ${table} SET ${setFragments.join(', ')} ${whereClause}`;
  return { text, values };
};

/** Deep clone a readonly schema array to a mutable one */
function cloneTablesToMutable(
  tables: ReadonlyArray<{
    schema?: unknown;
    name?: unknown;
    columns?: ReadonlyArray<{ name?: unknown; type?: unknown; nullable?: unknown }>;
  }>
): SchemaResult['tables'] {
  return tables.map((t) => ({
    schema: typeof t?.schema === 'string' ? t.schema : 'public',
    name: typeof t?.name === 'string' ? t.name : 'unknown',
    columns: Array.isArray(t?.columns)
      ? t.columns.map((c) => ({
          name: typeof c?.name === 'string' ? c.name : 'unknown',
          type: typeof c?.type === 'string' ? c.type : 'text',
          nullable: typeof c?.nullable === 'boolean' ? c.nullable : true
        }))
      : []
  }));
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Job executor
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

export class AnalysisJob {
  private readonly analysisService: AnalysisService;
  private readonly maxRetries = DEFAULT_MAX_RETRIES;

  constructor() {
    this.analysisService = new AnalysisService();
  }

  public async execute(data: AnalysisJobData): Promise<AnalysisJobResult> {
    const start = Date.now();
    const { analysisId, userId, dataSourceId, analysisType, options } = data;

    try {
      logger.info('Analysis job: start', {
        analysisId,
        userId,
        dataSourceId,
        analysisType,
        priority: data.priority,
        retryCount: data.retryCount ?? 0
      });

      await this.updateJobStatus(analysisId, JobStatus.RUNNING);
      await this.validateJobData(data);

      if (await this.isCancelled(analysisId)) {
        throw new APIError('Job cancelled', 499);
      }

      let result: SchemaResult | QualityResult | ComplianceResult | PerformanceResult;

      switch (analysisType) {
        case 'schema':
          result = await this.executeSchemaAnalysis(
            dataSourceId,
            (options as SchemaOptions | undefined) ?? undefined,
            userId
          );
          break;
        case 'data_quality':
          result = await this.executeQualityAnalysis(
            dataSourceId,
            (options as QualityOptions | undefined) ?? undefined,
            userId
          );
          break;
        case 'compliance':
          result = await this.executeComplianceAnalysis(
            dataSourceId,
            (options as ComplianceOptions | undefined) ?? undefined,
            userId
          );
          break;
        case 'performance':
          result = await this.executePerformanceAnalysis(
            dataSourceId,
            (options as PerformanceOptions | undefined) ?? undefined,
            userId
          );
          break;
        default:
          return assertNever(analysisType as never);
      }

      await this.updateJobStatus(analysisId, JobStatus.COMPLETED, result);
      await this.storeAnalysisResult(analysisId, result);
      await this.sendCompletionNotification(userId, analysisId, analysisType, true);

      const duration = Date.now() - start;
      logger.info('Analysis job: completed', {
        analysisId,
        durationMs: duration,
        resultPreview: safeLogSize(result)
      });

      return { analysisId, status: JobStatus.COMPLETED, result, duration, completedAt: new Date() };
    } catch (err) {
      const anyErr = err as { message?: string } | undefined;
      const duration = Date.now() - start;

      logger.error('Analysis job: failed', {
        analysisId,
        durationMs: duration,
        error: anyErr?.message,
        retryCount: data.retryCount ?? 0
      });

      if (await this.shouldRetryJob(data, err)) {
        return this.scheduleRetry(data);
      }

      await this.updateJobStatus(analysisId, JobStatus.FAILED, undefined, anyErr?.message);
      await this.sendCompletionNotification(userId, analysisId, analysisType, false, anyErr?.message);

      return { analysisId, status: JobStatus.FAILED, error: anyErr?.message ?? 'failed', duration, completedAt: new Date() };
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Per-type handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async executeSchemaAnalysis(
    dataSourceId: string,
    _options: SchemaOptions | undefined,
    userId: string
  ): Promise<SchemaResult> {
    try {
      const dataSource = await this.getDataSource(dataSourceId);
      if (!dataSource) throw new APIError('Data source not found', 404);

      // Baseline candidate
      const schemaCandidate = {
        name: String(dataSource.name ?? 'unknown'),
        tables: [
          {
            schema: 'public',
            name: 'users',
            columns: [
              { name: 'id', type: 'integer', nullable: false },
              { name: 'email', type: 'varchar', nullable: false },
              { name: 'first_name', type: 'varchar', nullable: true },
              { name: 'last_name', type: 'varchar', nullable: true },
              { name: 'created_at', type: 'timestamp', nullable: false }
            ]
          }
        ] as const
      } as const;

      // Call service; cast via unknown first (TS suggestion) then narrow with our loose type
      const analyzed = (await this.analysisService.analyzeSchema(
        { name: schemaCandidate.name, tables: schemaCandidate.tables },
        userId
      )) as unknown as SchemaAnalysisResult;

      const name =
        typeof analyzed?.name === 'string' ? analyzed.name : schemaCandidate.name;

      let tables: SchemaResult['tables'];
      if (Array.isArray(analyzed?.tables)) {
        tables = cloneTablesToMutable(analyzed.tables);
      } else {
        // clone readonly candidate â†’ mutable
        tables = cloneTablesToMutable(schemaCandidate.tables);
      }

      return { name, tables };
    } catch (err) {
      logger.error('Schema analysis failed', {
        dataSourceId,
        error: (err as { message?: string } | undefined)?.message
      });
      throw err;
    }
  }

  private async executeQualityAnalysis(
    _dataSourceId: string,
    options: QualityOptions | undefined,
    userId: string
  ): Promise<QualityResult> {
    try {
      const sampleSize = Math.max(1, Math.min(options?.sampleSize ?? 100, 10_000));

      const dataSamples = [
        { columnName: 'email', values: ['user1@example.com', 'user2@example.com', 'invalid-email', null] },
        { columnName: 'age', values: [25, 30, -5, 150, null, 28] }
      ].map((c) => ({ ...c, values: c.values.slice(0, sampleSize) }));

      const serviceRes = (await this.analysisService.analyzeDataSample(
        dataSamples,
        userId
      )) as unknown as AnalyzeDataSampleResponse;

      const issues: QualityResult['issues'] = Array.isArray(serviceRes?.qualityIssues)
        ? serviceRes.qualityIssues.map((q) => ({
            columnName: typeof q?.columnName === 'string' ? q.columnName : 'unknown',
            type:
              q?.type === 'format' || q?.type === 'range' || q?.type === 'nulls'
                ? q.type
                : ('format' as const),
            count: Number.isFinite(q?.count as number) ? Number(q!.count) : 0
          }))
        : [];

      const scores: Record<string, number> = {};
      const a = serviceRes?.analysis;

      if (a && !Array.isArray(a)) {
        for (const [k, v] of Object.entries(a)) {
          scores[k] = typeof v === 'number' && Number.isFinite(v) ? v : 0;
        }
      } else if (Array.isArray(a)) {
        a.forEach((row, idx) => {
          for (const [k, v] of Object.entries(row)) {
            const key = `${k}#${idx}`;
            scores[key] = typeof v === 'number' && Number.isFinite(v) ? v : 0;
          }
        });
      }

      return { issues, scores };
    } catch (err) {
      logger.error('Quality analysis failed', { error: (err as { message?: string } | undefined)?.message });
      throw err;
    }
  }

  private async executeComplianceAnalysis(
    dataSourceId: string,
    options: ComplianceOptions | undefined,
    userId: string
  ): Promise<ComplianceResult> {
    try {
      const frameworks = (options?.frameworks?.length ? options.frameworks : ['GDPR', 'HIPAA']) as readonly string[];

      const rules = [
        { id: 'gdpr_1', name: 'PII Data Encryption', framework: 'GDPR' },
        { id: 'hipaa_1', name: 'PHI Access Controls', framework: 'HIPAA' }
      ].filter((r) => frameworks.includes(r.framework));

      const serviceRes = (await this.analysisService.performQualityCheck(
        dataSourceId,
        rules,
        userId
      )) as unknown as QualityCheckResult;

      const violations: ComplianceResult['violations'] = Array.isArray(serviceRes?.results)
        ? serviceRes.results
            .filter((r) => r?.status === 'failed')
            .map((r) => ({
              id: typeof r?.ruleId === 'string' ? r.ruleId : 'unknown',
              framework: String(rules.find((x) => x.id === r?.ruleId)?.framework ?? 'Unknown'),
              description: typeof r?.ruleName === 'string' ? r.ruleName : 'Violation detected'
            }))
        : [];

      const passed = violations.length === 0;
      return { passed, violations };
    } catch (err) {
      logger.error('Compliance analysis failed', {
        dataSourceId,
        error: (err as { message?: string } | undefined)?.message
      });
      throw err;
    }
  }

  private async executePerformanceAnalysis(
    _dataSourceId: string,
    options: PerformanceOptions | undefined,
    _userId: string
  ): Promise<PerformanceResult> {
    try {
      const windowMins = Math.max(1, Math.min(options?.timeWindowMinutes ?? 15, 1440));

      const result: PerformanceResult = {
        windowMinutes: windowMins,
        queryPerformance: {
          avgQueryTime: 250,
          slowQueries: [],
          queryPatterns: [],
          resourceUtilization: { cpuUsage: 45, memoryUsage: 60, ioWait: 5, connectionCount: 15 }
        },
        recommendations: [
          {
            type: 'Index',
            description: 'Add index on frequently queried columns',
            impact: 'High',
            effort: 'Low',
            implementation: 'CREATE INDEX idx_user_email ON users(email);'
          }
        ]
      };

      return result;
    } catch (err) {
      logger.error('Performance analysis failed', { error: (err as { message?: string } | undefined)?.message });
      throw err;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async validateJobData(d: AnalysisJobData): Promise<void> {
    if (!d.analysisId) throw new APIError('Analysis ID is required', 400);
    if (!d.userId) throw new APIError('User ID is required', 400);
    if (!d.dataSourceId) throw new APIError('Data source ID is required', 400);

    const validTypes: AnalysisKind[] = ['schema', 'data_quality', 'compliance', 'performance'];
    if (!validTypes.includes(d.analysisType)) throw new APIError('Invalid analysis type', 400);

    const ds = await this.getDataSource(d.dataSourceId);
    if (!ds) throw new APIError('Data source not found', 404);
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Persistence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async updateJobStatus(
    analysisId: string,
    status: JobStatus,
    result?: unknown,
    error?: string
  ): Promise<void> {
    try {
      const data: Record<string, unknown> = { status, updated_at: new Date() };
      if (result !== undefined) data.results = JSON.stringify(result);
      if (error !== undefined) data.error = error;
      if (status === JobStatus.COMPLETED || status === JobStatus.FAILED) data.completed_at = new Date();

      const { text, values } = buildUpdateQuery('analysis_jobs', 'WHERE analysis_id = $1', [analysisId], data);
      await db.query(text, values);

      await (redis as any).set?.(
        STATUS_KEY(analysisId),
        JSON.stringify({ status, updated_at: Date.now() }),
        { EX: REDIS_STATUS_TTL_SEC }
      );
    } catch (err) {
      logger.error('Job status update failed', {
        analysisId,
        status,
        error: (err as { message?: string } | undefined)?.message
      });
    }
  }

  private async storeAnalysisResult(analysisId: string, result: unknown): Promise<void> {
    try {
      const resultStr = JSON.stringify(result);
      await db.query(
        `UPDATE analysis_jobs 
           SET results = $2, result_size = $3, completed_at = NOW()
         WHERE analysis_id = $1`,
        [analysisId, resultStr, resultStr.length]
      );

      await db.query(
        `INSERT INTO analysis_results (analysis_id, result_data, created_at)
         VALUES ($1, $2, NOW())
         ON CONFLICT (analysis_id) DO UPDATE SET
           result_data = EXCLUDED.result_data,
           updated_at = NOW()`,
        [analysisId, resultStr]
      );
    } catch (err) {
      logger.error('Persisting result failed', {
        analysisId,
        error: (err as { message?: string } | undefined)?.message
      });
      throw err;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Retry & Cancel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async shouldRetryJob(data: AnalysisJobData, err: unknown): Promise<boolean> {
    const retryCount = data.retryCount ?? 0;
    const maxRetries = data.maxRetries ?? this.maxRetries;

    if (err instanceof APIError && err.statusCode >= 400 && err.statusCode < 500) return false;

    const anyErr = err as { statusCode?: number } | undefined;
    if (anyErr?.statusCode === 499 || (await this.isCancelled(data.analysisId))) return false;

    if (retryCount >= maxRetries) return false;

    return isTransientError(err);
  }

  private async scheduleRetry(data: AnalysisJobData): Promise<AnalysisJobResult> {
    const retryCount = (data.retryCount ?? 0) + 1;
    const delay = backoffWithJitter(retryCount);

    logger.info('Analysis job: scheduling retry', {
      analysisId: data.analysisId,
      retryCount,
      delayMs: delay
    });

    const retryData: AnalysisJobData = { ...data, retryCount };

    setTimeout(async () => {
      try {
        if (await this.isCancelled(retryData.analysisId)) {
          await this.updateJobStatus(retryData.analysisId, JobStatus.FAILED, undefined, 'Job cancelled');
          return;
        }
        await this.execute(retryData);
      } catch (err) {
        logger.error('Retry execution failed', {
          analysisId: retryData.analysisId,
          error: (err as { message?: string } | undefined)?.message
        });
      }
    }, delay);

    return {
      analysisId: data.analysisId,
      status: JobStatus.PENDING,
      duration: 0,
      completedAt: new Date()
    };
  }

  private async isCancelled(analysisId: string): Promise<boolean> {
    try {
      const val = await (redis as any).get?.(CANCEL_KEY(analysisId));
      return val === '1' || val === 'true';
    } catch {
      return false;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Notifications â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async sendCompletionNotification(
    userId: string,
    analysisId: string,
    analysisType: string,
    success: boolean,
    error?: string
  ): Promise<void> {
    try {
      const metadata = { analysisId, analysisType, success };
      const type = success ? 'analysis_completed' : 'analysis_failed';
      const title = success ? 'Analysis Completed' : 'Analysis Failed';
      const message = success
        ? `Your ${analysisType} analysis has completed successfully.`
        : `Your ${analysisType} analysis failed: ${error ?? 'unknown error'}`;

      await db.query(
        `INSERT INTO notifications (user_id, type, title, message, metadata, created_at)
         VALUES ($1, $2, $3, $4, $5, NOW())`,
        [userId, type, title, message, JSON.stringify(metadata)]
      );
    } catch (err) {
      logger.error('Notification emit failed', {
        userId,
        analysisId,
        error: (err as { message?: string } | undefined)?.message
      });
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lookups & Cleanup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async getDataSource(dataSourceId: string): Promise<DataSource | null> {
    try {
      const result = await db.query(
        'SELECT id, name, type, config FROM data_sources WHERE id = $1',
        [dataSourceId]
      );
      const row = result.rows[0];
      if (!row) return null;

      // exactOptionalPropertyTypes-safe construction
      const dsBase: { id: string; name: string } = {
        id: String(row.id),
        name: String(row.name)
      };
      const ds: DataSource = {
        ...dsBase,
        ...(row.type !== undefined && row.type !== null ? { type: String(row.type) } : {}),
        ...(row.config !== undefined ? { config: row.config } : {})
      };

      return ds;
    } catch (err) {
      logger.error('Data source lookup failed', {
        dataSourceId,
        error: (err as { message?: string } | undefined)?.message
      });
      throw err;
    }
  }

  public async cleanup(analysisId: string): Promise<void> {
    try {
      await (redis as any).del?.(STATUS_KEY(analysisId));
      logger.debug('Job cleanup completed', { analysisId });
    } catch (err) {
      logger.error('Job cleanup failed', {
        analysisId,
        error: (err as { message?: string } | undefined)?.message
      });
    }
  }
}

/* Singleton export */
export const analysisJob = new AnalysisJob();



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\jobs\discoveryJob.ts
--------------------------------------------------------------------------------
import { db } from '@/config/database';
import { redis } from '@/config/redis';
import { DiscoveryStatus, JobPriority, JobStatus } from '@/interfaces';
import { APIError } from '@/utils/errors';
import { logger } from '@/utils/logger';

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Types
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

type DiscoveryKind = 'full' | 'incremental' | 'targeted';

export interface DiscoveryJobData {
  sessionId: string;
  userId: string;
  dataSourceId: string;
  discoveryType: DiscoveryKind;
  options?: DiscoveryOptions;
  priority: JobPriority;
  retryCount?: number;
  maxRetries?: number;
}

export interface DiscoveryJobResult {
  sessionId: string;
  status: JobStatus;
  result?: DiscoveryResults;
  error?: string;
  duration: number;
  completedAt: Date;
}

/** Tighten options you actually use */
export interface DiscoveryOptions {
  includeSampleData?: boolean;
  sampleSize?: number;
  aiAnalysis?: boolean; // default true
  schemas?: string[];
  tables?: string[];
}

/** Minimal result shape; swap with your domain interfaces if you prefer */
export interface DiscoveryResults {
  metadata: any;         // replace with Discovery.DataSourceMetadata if wired
  classification: any;   // replace with Discovery.ClassificationResults
  aiInsights?: any;      // replace with Discovery.AIInsights
  summary: any;          // replace with Discovery.DiscoverySummary
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Config & Keys
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const DEFAULT_MAX_RETRIES = 3;
const BASE_RETRY_DELAY_MS = 5_000;
const MAX_RETRY_DELAY_MS = 60_000;
const REDIS_STATUS_TTL_SEC = 300;

const STATUS_KEY = (sessionId: string) => `discovery:status:${sessionId}`;
const CANCEL_KEY = (sessionId: string) => `discovery:cancel:${sessionId}`;

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Utils
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const backoffWithJitter = (attempt: number) => {
  const exp = Math.min(BASE_RETRY_DELAY_MS * Math.pow(2, attempt - 1), MAX_RETRY_DELAY_MS);
  const jitter = Math.floor(Math.random() * Math.min(1000, exp * 0.1));
  return exp + jitter;
};

const safeLogSize = (val: unknown, max = 4096) => {
  try {
    const s = JSON.stringify(val);
    return s.length <= max ? s : s.slice(0, max) + `â€¦ (truncated ${s.length - max} chars)`;
  } catch {
    return '[unserializable]';
  }
};

const isTransientError = (err: unknown): boolean => {
  const msg = (err as any)?.message?.toLowerCase?.() || '';
  const code = (err as any)?.code || '';
  return (
    code === 'ETIMEDOUT' ||
    code === 'ECONNRESET' ||
    code === 'ECONNREFUSED' ||
    msg.includes('timeout') ||
    msg.includes('connection') ||
    msg.includes('deadlock') ||
    msg.includes('too many connections') ||
    msg.includes('rate limit') ||
    msg.includes('temporarily') ||
    msg.includes('try again')
  );
};

/** Parameterized UPDATE builder to avoid index mistakes */
const buildUpdateQuery = (table: string, whereClause: string, whereParams: unknown[], data: Record<string, unknown>) => {
  const keys = Object.keys(data);
  const setFragments = keys.map((k, i) => `${k} = $${i + whereParams.length + 1}`);
  const values = [...whereParams, ...keys.map(k => data[k])];
  const text = `UPDATE ${table} SET ${setFragments.join(', ')} ${whereClause}`;
  return { text, values };
};

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Job
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

export class DiscoveryJob {
  private maxRetries = DEFAULT_MAX_RETRIES;

  constructor() {}

  public async execute(data: DiscoveryJobData): Promise<DiscoveryJobResult> {
    const started = Date.now();
    const { sessionId, userId, dataSourceId, discoveryType, options } = data;

    try {
      logger.info('Discovery job: start', {
        sessionId, userId, dataSourceId, discoveryType, priority: data.priority, retryCount: data.retryCount ?? 0,
      });

      await this.updateJobStatus(sessionId, DiscoveryStatus.INITIALIZING, 0);

      await this.validateJobData(data);

      // Respect cancellation early
      if (await this.isCancelled(sessionId)) {
        throw new APIError('Job cancelled', 499);
      }

      const result = await this.executeDiscoveryPhases(data);

      await this.updateJobStatus(sessionId, DiscoveryStatus.COMPLETED, 100, result);

      await this.sendCompletionNotification(userId, sessionId, discoveryType, true);

      const duration = Date.now() - started;
      logger.info('Discovery job: completed', {
        sessionId,
        durationMs: duration,
        resultPreview: safeLogSize({
          summary: result?.summary,
          metadata: { totalTables: result?.metadata?.totalTables, totalColumns: result?.metadata?.totalColumns },
        })
      });

      return {
        sessionId,
        status: JobStatus.COMPLETED,
        result,
        duration,
        completedAt: new Date(),
      };
    } catch (err: any) {
      const duration = Date.now() - started;
      logger.error('Discovery job: failed', {
        sessionId,
        durationMs: duration,
        error: err?.message,
        retryCount: data.retryCount ?? 0,
      });

      if (await this.shouldRetryJob(data, err)) {
        return this.scheduleRetry(data);
      }

      await this.updateJobStatus(sessionId, DiscoveryStatus.FAILED, 0, undefined, err?.message);
      await this.sendCompletionNotification(userId, sessionId, discoveryType, false, err?.message);

      return {
        sessionId,
        status: JobStatus.FAILED,
        error: err?.message ?? 'failed',
        duration,
        completedAt: new Date(),
      };
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Phases â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async executeDiscoveryPhases(data: DiscoveryJobData): Promise<DiscoveryResults> {
    const { sessionId, dataSourceId, options } = data;
    let progress = 0;

    try {
      // Phase 1: metadata
      await this.updateJobStatus(sessionId, DiscoveryStatus.SCANNING_METADATA, 10);
      const metadata = await this.scanMetadata(dataSourceId, options);
      progress = 20;
      await this.updateJobStatus(sessionId, DiscoveryStatus.SCANNING_METADATA, progress);

      // Phase 2: sampling
      if (options?.includeSampleData) {
        await this.updateJobStatus(sessionId, DiscoveryStatus.SAMPLING_DATA, progress);
        const sampleData = await this.sampleData(dataSourceId, metadata, options);
        (metadata as any).sampleData = sampleData;
        progress = 40;
        await this.updateJobStatus(sessionId, DiscoveryStatus.SAMPLING_DATA, progress);
      }

      // Phase 3: classification
      await this.updateJobStatus(sessionId, DiscoveryStatus.CLASSIFYING, progress);
      const classification = await this.classifyData(metadata, options);
      progress = 60;
      await this.updateJobStatus(sessionId, DiscoveryStatus.CLASSIFYING, progress);

      // Phase 4: AI analysis (default on)
      let aiInsights: unknown | undefined;
      if (options?.aiAnalysis !== false) {
        await this.updateJobStatus(sessionId, DiscoveryStatus.AI_ANALYSIS, progress);
        aiInsights = await this.performAIAnalysis(metadata, classification);
        progress = 80;
        await this.updateJobStatus(sessionId, DiscoveryStatus.AI_ANALYSIS, progress);
      }

      const summary = this.generateSummary(metadata, classification, aiInsights);

      return { metadata, classification, aiInsights, summary };
    } catch (err) {
      logger.error('Discovery phases: error', { sessionId, progress, error: (err as any)?.message });
      throw err;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Implementations (mocked) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async scanMetadata(dataSourceId: string, _options?: DiscoveryOptions): Promise<any> {
    void _options;
    const dataSource = await this.getDataSource(dataSourceId);
    if (!dataSource) throw new APIError('Data source not found', 404);

    // Replace this mock with real extractor when ready
    const metadata = {
      dataSourceId,
      connectionInfo: {
        type: (dataSource as any).type,
        host: (dataSource as any).connectionConfig?.host,
        database: (dataSource as any).connectionConfig?.database,
        version: '14.5',
      },
      schemas: [
        {
          name: 'public',
          tables: [
            {
              schema: 'public',
              name: 'users',
              type: 'table',
              columns: [
                { name: 'id', dataType: 'integer', nullable: false, isPrimaryKey: true },
                { name: 'email', dataType: 'varchar', nullable: false },
                { name: 'first_name', dataType: 'varchar', nullable: true },
                { name: 'last_name', dataType: 'varchar', nullable: true },
                { name: 'phone', dataType: 'varchar', nullable: true },
                { name: 'date_of_birth', dataType: 'date', nullable: true },
                { name: 'created_at', dataType: 'timestamp', nullable: false },
              ],
              primaryKeys: ['id'],
              foreignKeys: [],
              indexes: [
                { name: 'users_pkey', columns: ['id'], unique: true },
                { name: 'users_email_idx', columns: ['email'], unique: true },
              ],
              rowCount: 15_000,
              sizeBytes: 2_048_000,
            },
            {
              schema: 'public',
              name: 'orders',
              type: 'table',
              columns: [
                { name: 'id', dataType: 'integer', nullable: false, isPrimaryKey: true },
                { name: 'user_id', dataType: 'integer', nullable: false, isForeignKey: true },
                { name: 'total_amount', dataType: 'decimal', nullable: false },
                { name: 'status', dataType: 'varchar', nullable: false },
                { name: 'created_at', dataType: 'timestamp', nullable: false },
              ],
              primaryKeys: ['id'],
              foreignKeys: [
                { name: 'fk_orders_user', columns: ['user_id'], referencedTable: 'users', referencedColumns: ['id'] },
              ],
              indexes: [
                { name: 'orders_pkey', columns: ['id'], unique: true },
                { name: 'orders_user_id_idx', columns: ['user_id'], unique: false },
              ],
              rowCount: 45_000,
              sizeBytes: 3_072_000,
            },
          ],
          views: [],
          procedures: [],
        },
      ],
      totalTables: 2,
      totalColumns: 12,
      totalRows: 60_000,
      dataSize: 5_120_000,
      lastUpdated: new Date(),
    };

    return metadata;
  }

  private async sampleData(_dataSourceId: string, _metadata: any, options?: DiscoveryOptions): Promise<any> {
    void _metadata;
    const size = Math.max(1, Math.min(options?.sampleSize ?? 100, 10_000));
    void size; // for now
    return {
      'public.users': [
        { id: 1, email: 'john.doe@example.com', first_name: 'John', last_name: 'Doe', phone: '+1-555-0123', date_of_birth: '1990-05-15' },
        { id: 2, email: 'jane.smith@example.com', first_name: 'Jane', last_name: 'Smith', phone: '+1-555-0124', date_of_birth: '1985-12-08' },
        { id: 3, email: 'invalid-email', first_name: 'Bob', last_name: 'Johnson', phone: '555-0125', date_of_birth: '1992-03-22' },
      ],
      'public.orders': [
        { id: 1, user_id: 1, total_amount: 99.99, status: 'completed', created_at: '2024-01-15T10:30:00Z' },
        { id: 2, user_id: 2, total_amount: 149.50, status: 'pending', created_at: '2024-01-16T14:20:00Z' },
        { id: 3, user_id: 1, total_amount: -10.00, status: 'refunded', created_at: '2024-01-17T09:15:00Z' },
      ],
    };
  }

  private async classifyData(metadata: any, _options?: DiscoveryOptions): Promise<any> {
    void _options;
    const fieldClassifications: any[] = [];
    const tableClassifications: any[] = [];

    const piiPatterns = ['email', 'phone', 'ssn', 'social_security', 'first_name', 'last_name', 'full_name'];
    const phiPatterns = ['medical', 'health', 'diagnosis', 'patient', 'dob', 'date_of_birth'];
    const finPatterns = ['credit_card', 'bank_account', 'salary', 'income', 'total_amount', 'price'];

    for (const schema of metadata.schemas) {
      for (const table of schema.tables) {
        let hasPII = false; let hasPHI = false; let hasFIN = false;

        for (const col of table.columns) {
          const name = String(col.name).toLowerCase();
          let classification = 'General';
          let sensitivity = 'Low';
          let confidence = 0.7;
          const patterns: string[] = [];
          const tags: string[] = [];
          const complianceFlags: any[] = [];

          if (piiPatterns.some(p => name.includes(p))) {
            classification = 'PII';
            sensitivity = name.includes('ssn') ? 'High' : 'Medium';
            confidence = 0.9;
            patterns.push('PII identifier pattern');
            tags.push('personal_data');
            complianceFlags.push({ framework: 'GDPR', requirement: 'Data subject rights', severity: 'Warning' });
            hasPII = true;
          } else if (phiPatterns.some(p => name.includes(p))) {
            classification = 'PHI';
            sensitivity = 'High';
            confidence = 0.9;
            patterns.push('PHI identifier pattern');
            tags.push('health_data');
            complianceFlags.push({ framework: 'HIPAA', requirement: 'PHI protection', severity: 'Error' });
            hasPHI = true;
          } else if (finPatterns.some(p => name.includes(p))) {
            classification = 'Financial';
            sensitivity = 'Medium';
            confidence = 0.8;
            patterns.push('Financial data pattern');
            tags.push('financial_data');
            complianceFlags.push({ framework: 'PCI-DSS', requirement: 'Payment data security', severity: 'Warning' });
            hasFIN = true;
          }

          fieldClassifications.push({
            schema: schema.name,
            table: table.name,
            column: col.name,
            dataType: col.dataType,
            classification,
            sensitivity,
            confidence,
            patterns,
            tags,
            complianceFlags,
          });
        }

        let overallClassification = 'General';
        let overallSensitivity = 'Low';
        if (hasPHI) { overallClassification = 'PHI'; overallSensitivity = 'High'; }
        else if (hasPII) { overallClassification = 'PII'; overallSensitivity = 'Medium'; }
        else if (hasFIN) { overallClassification = 'Financial'; overallSensitivity = 'Medium'; }

        tableClassifications.push({
          schema: schema.name,
          table: table.name,
          overallClassification,
          overallSensitivity,
          dataVolume: table.rowCount > 100_000 ? 'Large' : table.rowCount > 10_000 ? 'Medium' : 'Small',
          businessCriticality: 'Medium',
          accessFrequency: 'Regular',
          retentionCategory: 'Standard',
          complianceScope: this.getComplianceScope(overallClassification),
        });
      }
    }

    return {
      fieldClassifications,
      tableClassifications,
      sensitivityMap: this.createSensitivityMap(fieldClassifications),
      complianceMapping: this.createComplianceMapping(fieldClassifications),
      riskAssessment: this.assessRisk(fieldClassifications, tableClassifications),
    };
  }

  private async performAIAnalysis(metadata: any, classification: any): Promise<any> {
    // Stubbed AI; integrate your AIService usage once ready
    const aiInsights = {
      fieldRecommendations: [] as any[],
      schemaInsights: [] as any[],
      qualityPredictions: [] as any[],
      governanceRecommendations: [] as any[],
      anomalies: [] as any[],
    };

    for (const f of classification.fieldClassifications) {
      if (f.sensitivity === 'High' || f.classification !== 'General') {
        aiInsights.fieldRecommendations.push({
          schema: f.schema,
          table: f.table,
          column: f.column,
          recommendation: `Encrypt and restrict access to ${f.classification} field`,
          reasoning: `Contains ${f.classification} with ${f.sensitivity} sensitivity`,
          confidence: 0.9,
          impact: f.sensitivity === 'High' ? 'High' : 'Medium',
          category: 'Security',
        });
      }
    }

    aiInsights.schemaInsights.push({
      type: 'Pattern',
      insight: 'Detected user-order relationship; consider normalization/indexing',
      confidence: 0.8,
      tables: ['users', 'orders'],
      recommendation: 'Add FKs and composite indexes for key joins',
      priority: 'Medium',
    });

    aiInsights.qualityPredictions.push({
      table: 'users',
      column: 'email',
      qualityDimension: 'Validity',
      predictedScore: 85,
      confidence: 0.9,
      factors: ['Invalid format occurrences'],
      recommendations: ['Add email validation', 'Clean existing invalids'],
    });

    aiInsights.governanceRecommendations.push({
      scope: 'Table',
      target: 'users',
      recommendation: 'Adopt data retention for PII',
      category: 'Retention',
      priority: 'High',
      effort: 'Medium',
      benefit: 'Compliance & cost control',
    });

    aiInsights.anomalies.push({
      type: 'Quality',
      table: 'orders',
      column: 'total_amount',
      description: 'Negative amounts detected',
      severity: 'Warning',
      confidence: 0.95,
      suggestion: 'Review refund logic; add validation',
    });

    return aiInsights;
  }

  private generateSummary(metadata: any, classification: any, aiInsights?: any): any {
    const summary = {
      totalTablesAnalyzed: metadata.totalTables,
      totalColumnsAnalyzed: metadata.totalColumns,
      classificationsApplied: classification.fieldClassifications.length,
      sensitiveDataFound: classification.fieldClassifications.filter((f: any) => f.sensitivity !== 'Low').length,
      complianceFlags: classification.fieldClassifications.reduce((sum: number, f: any) => sum + f.complianceFlags.length, 0),
      qualityIssues: 0,
      aiRecommendations: 0,
      executionTime: 0,
      dataVolumeProcessed: metadata.totalRows || 0,
    };

    if (aiInsights) {
      summary.qualityIssues = aiInsights.anomalies.length;
      summary.aiRecommendations = aiInsights.fieldRecommendations.length + aiInsights.governanceRecommendations.length;
    }
    return summary;
  }

  private createSensitivityMap(fieldClassifications: any[]) {
    const bySensitivity: Record<string, any[]> = {
      Low: fieldClassifications.filter(f => f.sensitivity === 'Low'),
      Medium: fieldClassifications.filter(f => f.sensitivity === 'Medium'),
      High: fieldClassifications.filter(f => f.sensitivity === 'High'),
      Critical: fieldClassifications.filter(f => f.sensitivity === 'Critical'),
    };

    const byClassification: Record<string, any[]> = {
      General: fieldClassifications.filter(f => f.classification === 'General'),
      PII: fieldClassifications.filter(f => f.classification === 'PII'),
      PHI: fieldClassifications.filter(f => f.classification === 'PHI'),
      Financial: fieldClassifications.filter(f => f.classification === 'Financial'),
    };

    return {
      bySensitivity,
      byClassification,
      sensitiveTableCount: new Set(
        fieldClassifications.filter(f => f.sensitivity !== 'Low').map(f => `${f.schema}.${f.table}`)
      ).size,
      highRiskFields: fieldClassifications.filter(f => f.sensitivity === 'High' || f.sensitivity === 'Critical'),
    };
  }

  private createComplianceMapping(fieldClassifications: any[]) {
    const frameworkMap: Record<string, { applicableTables: Set<string>; applicableFields: any[]; requirements: Set<string>; riskLevel: 'Low' | 'Medium' | 'High' }> = {};

    fieldClassifications.forEach(field => {
      field.complianceFlags.forEach((flag: any) => {
        if (!frameworkMap[flag.framework]) {
          frameworkMap[flag.framework] = {
            applicableTables: new Set<string>(),
            applicableFields: [],
            requirements: new Set<string>(),
            riskLevel: 'Low',
          };
        }
        const f = frameworkMap[flag.framework];
        f.applicableTables.add(`${field.schema}.${field.table}`);
        f.applicableFields.push(field);
        f.requirements.add(flag.requirement);
        if (flag.severity === 'Error') f.riskLevel = 'High';
        else if (flag.severity === 'Warning' && f.riskLevel === 'Low') f.riskLevel = 'Medium';
      });
    });

    // Convert Sets
    const byFramework: Record<string, any> = {};
    for (const [fw, data] of Object.entries(frameworkMap)) {
      byFramework[fw] = {
        applicableTables: Array.from(data.applicableTables),
        applicableFields: data.applicableFields,
        requirements: Array.from(data.requirements),
        riskLevel: data.riskLevel,
      };
    }

    return {
      byFramework,
      overallComplexity: Object.keys(byFramework).length > 2 ? 'High' : Object.keys(byFramework).length > 1 ? 'Medium' : 'Low',
      requiredActions: this.generateComplianceActions(byFramework),
    };
  }

  private generateComplianceActions(byFramework: Record<string, any>) {
    const actions: any[] = [];
    for (const [framework, data] of Object.entries(byFramework)) {
      if (data.riskLevel === 'High') {
        actions.push({ framework, action: `Immediate ${framework} compliance review`, priority: 'High', timeline: '30 days', effort: 'High' });
      } else if (data.riskLevel === 'Medium') {
        actions.push({ framework, action: `${framework} compliance assessment`, priority: 'Medium', timeline: '90 days', effort: 'Medium' });
      }
    }
    return actions;
  }

  private assessRisk(fieldClassifications: any[], tableClassifications: any[]) {
    const highRiskFields = fieldClassifications.filter(f => f.sensitivity === 'High' || f.sensitivity === 'Critical');
    const sensitiveTableCount = tableClassifications.filter(t => t.overallSensitivity !== 'Low').length;

    let overallRisk: 'Low' | 'Medium' | 'High' = 'Low';
    if (highRiskFields.length > 10 || sensitiveTableCount > 5) overallRisk = 'High';
    else if (highRiskFields.length > 5 || sensitiveTableCount > 2) overallRisk = 'Medium';

    return {
      overallRisk,
      riskFactors: [
        {
          factor: 'High sensitivity data fields',
          level: highRiskFields.length > 10 ? 'High' : highRiskFields.length > 5 ? 'Medium' : 'Low',
          description: `${highRiskFields.length} fields contain highly sensitive data`,
          affectedTables: [...new Set(highRiskFields.map(f => `${f.schema}.${f.table}`))],
          mitigationStrategy: 'Implement encryption and access controls',
        },
      ],
      mitigationPriorities: [
        'Implement data classification policies',
        'Set up access controls for sensitive data',
        'Regular compliance audits',
      ],
      businessImpact: {
        reputational: overallRisk === 'High' ? 'High' : 'Medium',
        financial: overallRisk === 'High' ? 'High' : 'Low',
        operational: 'Medium',
        regulatory: overallRisk === 'High' ? 'Critical' : 'Medium',
      },
    };
  }

  private getComplianceScope(classification: string): string[] {
    switch (classification) {
      case 'PII': return ['GDPR', 'CCPA'];
      case 'PHI': return ['HIPAA'];
      case 'Financial': return ['PCI-DSS', 'SOX'];
      default: return [];
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async validateJobData(d: DiscoveryJobData): Promise<void> {
    if (!d.sessionId) throw new APIError('Session ID is required', 400);
    if (!d.userId) throw new APIError('User ID is required', 400);
    if (!d.dataSourceId) throw new APIError('Data source ID is required', 400);

    const valid: DiscoveryKind[] = ['full', 'incremental', 'targeted'];
    if (!valid.includes(d.discoveryType)) throw new APIError('Invalid discovery type', 400);

    // basic existence
    const ds = await this.getDataSource(d.dataSourceId);
    if (!ds) throw new APIError('Data source not found', 404);

    // Targeted must include at least one target
    if (d.discoveryType === 'targeted') {
      const hasTargets = Boolean(d.options?.schemas?.length || d.options?.tables?.length);
      if (!hasTargets) throw new APIError('Targeted discovery requires schemas or tables', 400);
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Persistence & Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async updateJobStatus(
    sessionId: string,
    status: DiscoveryStatus,
    progress?: number,
    result?: unknown,
    error?: string
  ): Promise<void> {
    try {
      const patch: Record<string, unknown> = {
        status,
        updated_at: new Date(),
      };
      if (progress !== undefined) patch.progress = progress;
      if (result !== undefined) patch.results = JSON.stringify(result);
      if (error !== undefined) patch.error = error;
      if (status === DiscoveryStatus.COMPLETED || status === DiscoveryStatus.FAILED || status === DiscoveryStatus.CANCELLED) {
        patch.completed_at = new Date();
      }

      const { text, values } = buildUpdateQuery('discovery_sessions', 'WHERE session_id = $1', [sessionId], patch);
      await db.query(text, values);

      await (redis as any).set?.(
        STATUS_KEY(sessionId),
        JSON.stringify({ status, progress: progress ?? null, updated_at: Date.now() }),
        { EX: REDIS_STATUS_TTL_SEC }
      );
    } catch (err) {
      logger.error('Discovery status update failed', { sessionId, status, error: (err as any)?.message });
      // non-fatal
    }
  }

  private async getDataSource(dataSourceId: string): Promise<any> {
    try {
      const result = await db.query('SELECT * FROM data_sources WHERE id = $1', [dataSourceId]);
      return result.rows[0] ?? null;
    } catch (err) {
      logger.error('Discovery data source lookup failed', { dataSourceId, error: (err as any)?.message });
      throw err;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Retry & Cancel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async shouldRetryJob(d: DiscoveryJobData, err: unknown): Promise<boolean> {
    const retryCount = d.retryCount ?? 0;
    const maxRetries = d.maxRetries ?? this.maxRetries;

    if (err instanceof APIError && err.statusCode >= 400 && err.statusCode < 500) return false;
    if ((err as any)?.statusCode === 499) return false; // cancelled
    if (await this.isCancelled(d.sessionId)) return false;
    if (retryCount >= maxRetries) return false;

    return isTransientError(err);
  }

  private async scheduleRetry(d: DiscoveryJobData): Promise<DiscoveryJobResult> {
    const retryCount = (d.retryCount ?? 0) + 1;
    const delay = backoffWithJitter(retryCount);

    logger.info('Discovery job: scheduling retry', { sessionId: d.sessionId, retryCount, delayMs: delay });

    const retryData: DiscoveryJobData = { ...d, retryCount };

    setTimeout(async () => {
      try {
        if (await this.isCancelled(retryData.sessionId)) {
          await this.updateJobStatus(retryData.sessionId, DiscoveryStatus.FAILED, 0, undefined, 'Job cancelled');
          return;
        }
        await this.execute(retryData);
      } catch (err) {
        logger.error('Discovery retry execution failed', { sessionId: retryData.sessionId, error: (err as any)?.message });
      }
    }, delay);

    return {
      sessionId: d.sessionId,
      status: JobStatus.PENDING,
      duration: 0,
      completedAt: new Date(),
    };
  }

  private async isCancelled(sessionId: string): Promise<boolean> {
    try {
      const val = await (redis as any).get?.(CANCEL_KEY(sessionId));
      return val === '1' || val === 'true';
    } catch {
      return false;
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Notifications â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async sendCompletionNotification(
    userId: string,
    sessionId: string,
    discoveryType: string,
    success: boolean,
    error?: string
  ): Promise<void> {
    try {
      const type = success ? 'discovery_completed' : 'discovery_failed';
      const title = success ? 'Discovery Completed' : 'Discovery Failed';
      const message = success
        ? `Your ${discoveryType} discovery completed successfully.`
        : `Your ${discoveryType} discovery failed: ${error ?? 'unknown error'}`;
      const metadata = { sessionId, discoveryType, success };

      await db.query(
        `INSERT INTO notifications (user_id, type, title, message, metadata, created_at)
         VALUES ($1, $2, $3, $4, $5, NOW())`,
        [userId, type, title, message, JSON.stringify(metadata)]
      );
      // push via websocket/event bus as needed
    } catch (err) {
      logger.error('Discovery notification failed', { userId, sessionId, error: (err as any)?.message });
      // swallow
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lifecycle helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  public async cleanup(sessionId: string): Promise<void> {
    try {
      await (redis as any).del?.(STATUS_KEY(sessionId));
      logger.debug('Discovery cleanup completed', { sessionId });
    } catch (err) {
      logger.error('Discovery cleanup failed', { sessionId, error: (err as any)?.message });
    }
  }

  public async cancel(sessionId: string): Promise<void> {
    try {
      // A controller can also set CANCEL_KEY(sessionId) in Redis, if you want
      await this.updateJobStatus(sessionId, DiscoveryStatus.CANCELLED, 0);
      await this.cleanup(sessionId);
      logger.info('Discovery job cancelled', { sessionId });
    } catch (err) {
      logger.error('Discovery cancel failed', { sessionId, error: (err as any)?.message });
      throw err;
    }
  }

  public async getProgress(sessionId: string): Promise<any> {
    try {
      const cached = await (redis as any).get?.(STATUS_KEY(sessionId));
      if (cached) {
        try { return JSON.parse(cached); } catch { /* fall through */ }
      }

      const result = await db.query(
        'SELECT status, progress, updated_at FROM discovery_sessions WHERE session_id = $1',
        [sessionId]
      );
      if (result.rows.length === 0) throw new APIError('Discovery session not found', 404);

      return {
        status: result.rows[0].status,
        progress: result.rows[0].progress,
        updated_at: result.rows[0].updated_at,
      };
    } catch (err) {
      logger.error('Discovery progress fetch failed', { sessionId, error: (err as any)?.message });
      throw err;
    }
  }
}

/** Singleton */
export const discoveryJob = new DiscoveryJob();



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\middleware\auth.ts
--------------------------------------------------------------------------------
// src/middleware/auth.ts
import { APIError } from '@/utils/errors';
import { logger } from '@/utils/logger';
import { NextFunction, Request, Response } from 'express';
import jwt, { JwtPayload, VerifyOptions } from 'jsonwebtoken';

/** Extend Express Request with an authenticated user */
export interface AuthenticatedRequest extends Request {
  user?: {
    id: string;
    email: string;
    role: string;
    permissions?: string[];
    sub?: string;
    iat?: number;
    exp?: number;
  };
}

/** The JWT claims your system issues/accepts */
interface AppJwtPayload extends JwtPayload {
  id: string;
  email: string;
  role: string;
  permissions?: string[];
}

/* ---------------------------- Helpers & Config ---------------------------- */

const readEnv = () => {
  const {
    JWT_SECRET,
    JWT_ISSUER,
    JWT_AUDIENCE,
    JWT_ALLOWED_ALGS,
    JWT_ALG = 'HS256',
    JWT_CLOCK_TOLERANCE_SEC = '60',
    AUTH_COOKIE = 'auth_token',
  } = process.env;

  if (!JWT_SECRET) {
    // fail closed; this is a deployment/config error
    throw new Error('JWT_SECRET is not configured');
  }

  const allowedAlgs = (JWT_ALLOWED_ALGS?.split(',').map(s => s.trim()).filter(Boolean)) || [JWT_ALG];

  const verifyOptions: VerifyOptions = {
    algorithms: allowedAlgs as jwt.Algorithm[],
    issuer: JWT_ISSUER || undefined,
    audience: JWT_AUDIENCE || undefined,
    clockTolerance: Number.isFinite(+JWT_CLOCK_TOLERANCE_SEC) ? Number(JWT_CLOCK_TOLERANCE_SEC) : 60,
  };

  return { JWT_SECRET, verifyOptions, AUTH_COOKIE };
};

/** Safely extract a token from Authorization header, x-access-token, or a cookie */
const extractToken = (req: Request, cookieName: string): string | null => {
  const hdr = req.headers.authorization || '';
  if (hdr) {
    const [scheme, token] = hdr.split(' ');
    if (scheme?.toLowerCase() === 'bearer' && token) return token.trim();
  }
  const alt = (req.headers['x-access-token'] as string) || '';
  if (alt) return alt.trim();

  // minimal cookie parse (avoid extra deps)
  const cookieHeader = req.headers.cookie || '';
  if (cookieHeader && cookieName) {
    const parts = cookieHeader.split(';').map(c => c.trim());
    for (const p of parts) {
      const [k, v] = p.split('=');
      if (k === cookieName && v) return decodeURIComponent(v);
    }
  }
  return null;
};

/** Central token verification (HMAC secret) */
const verifyToken = (token: string): AppJwtPayload => {
  const { JWT_SECRET, verifyOptions } = readEnv();
  const decoded = jwt.verify(token, JWT_SECRET, verifyOptions);
  if (typeof decoded === 'string') {
    throw new APIError('Invalid token payload', 401);
  }
  const payload = decoded as AppJwtPayload;
  if (!payload.id || !payload.email || !payload.role) {
    throw new APIError('Token missing required claims', 401);
  }
  return payload;
};

/** include optional field only if defined (for exactOptionalPropertyTypes) */
type Defined<T> = Exclude<T, undefined>;
const includeIfDefined = <K extends string, V>(key: K, value: V | undefined) =>
  (value !== undefined ? { [key]: value } as Record<K, Defined<V>> : {});

/** normalize string or string[] input */
const toArray = <T>(v: T | T[]) => (Array.isArray(v) ? v : [v]);

/* -------------------------------- Middlewares ----------------------------- */

/** Strict authentication. Requires a valid token and attaches user to req. */
export const authenticateToken = async (
  req: AuthenticatedRequest,
  _res: Response,
  next: NextFunction
): Promise<void> => {
  try {
    const { AUTH_COOKIE } = readEnv();
    const token = extractToken(req, AUTH_COOKIE);
    if (!token) throw new APIError('Access token required', 401);

    const payload = verifyToken(token);

    // only include optional fields if they exist
    req.user = {
      id: payload.id,
      email: payload.email,
      role: payload.role,
      permissions: payload.permissions ?? [],
      ...includeIfDefined('sub', payload.sub),
      ...includeIfDefined('iat', payload.iat),
      ...includeIfDefined('exp', payload.exp),
    };

    logger.debug('Auth: user authenticated', { userId: payload.id, role: payload.role });
    next();
  } catch (err: any) {
    if (err instanceof APIError) return next(err);
    if (err instanceof jwt.TokenExpiredError) return next(new APIError('Token expired', 401));
    if (err instanceof jwt.NotBeforeError) return next(new APIError('Token not active yet', 401));
    if (err instanceof jwt.JsonWebTokenError) return next(new APIError('Invalid token', 401));
    logger.warn('Auth: unexpected verification error', { message: err?.message });
    next(new APIError('Authentication failed', 401));
  }
};

/** Optional auth. If a valid token is present, attaches user; never blocks. */
export const optionalAuth = async (
  req: AuthenticatedRequest,
  _res: Response,
  next: NextFunction
): Promise<void> => {
  try {
    const { AUTH_COOKIE } = readEnv();
    const token = extractToken(req, AUTH_COOKIE);
    if (!token) return next();

    try {
      const payload = verifyToken(token);
      req.user = {
        id: payload.id,
        email: payload.email,
        role: payload.role,
        permissions: payload.permissions ?? [],
        ...includeIfDefined('sub', payload.sub),
        ...includeIfDefined('iat', payload.iat),
        ...includeIfDefined('exp', payload.exp),
      };
      logger.debug('Auth: optional user attached', { userId: payload.id, role: payload.role });
    } catch (e: any) {
      // Do not block optional routes; just log at debug
      logger.debug('Auth: optional token rejected', { reason: e?.message });
    }
    next();
  } catch {
    next();
  }
};

/** Require one of the given roles. */
export const requireRole = (roles: string[] | string) => {
  const required = new Set(toArray(roles).map(r => r.toLowerCase()));
  return (req: AuthenticatedRequest, _res: Response, next: NextFunction): void => {
    try {
      if (!req.user) throw new APIError('Authentication required', 401);
      const userRole = (req.user.role || '').toLowerCase();
      if (!required.has(userRole)) throw new APIError('Insufficient permissions', 403);
      next();
    } catch (err) {
      next(err);
    }
  };
};

/** Require a specific permission (exact match). */
export const requirePermission = (permission: string) => {
  return (req: AuthenticatedRequest, _res: Response, next: NextFunction): void => {
    try {
      if (!req.user) throw new APIError('Authentication required', 401);
      const perms = new Set(req.user.permissions || []);
      if (!perms.has(permission)) throw new APIError('Insufficient permissions', 403);
      next();
    } catch (err) {
      next(err);
    }
  };
};



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\middleware\errorHandler.ts
--------------------------------------------------------------------------------
import { APIError } from '@/utils/errors';
import { logger } from '@/utils/logger';
import { NextFunction, Request, Response } from 'express';

// Extended Error interface for database and system errors
interface ExtendedError extends Error {
  code?: string;
  errno?: number;
  sqlState?: string;
  sqlMessage?: string;
  statusCode?: number;
}

export const errorHandler = (
  error: ExtendedError,
  req: Request,
  res: Response,
  next: NextFunction
): void => {
  void next; // keep Express signature, silence TS unused

  // Log the error with full context
  logger.error('Request error:', {
    error: error.message,
    stack: error.stack,
    code: error.code,
    statusCode: error.statusCode,
    url: req.url,
    method: req.method,
    ip: req.ip,
    userAgent: req.get('User-Agent'),
    requestId: (req as any).id,
    body: req.method !== 'GET' ? req.body : undefined,
    query: req.query,
    params: req.params
  });

  // Handle API errors (our custom errors)
  if (error instanceof APIError) {
    res.status(error.statusCode).json({
      success: false,
      error: {
        message: error.message,
        code: error.statusCode,
        type: 'APIError',
        ...(error.details && { details: error.details }),
        ...(process.env.NODE_ENV === 'development' && { stack: error.stack })
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle Joi validation errors
  if (error.name === 'ValidationError') {
    res.status(400).json({
      success: false,
      error: {
        message: 'Validation failed',
        code: 400,
        type: 'ValidationError',
        details: error.message
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle JWT errors
  if (error.name === 'JsonWebTokenError') {
    res.status(401).json({
      success: false,
      error: {
        message: 'Invalid token',
        code: 401,
        type: 'AuthenticationError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  if (error.name === 'TokenExpiredError') {
    res.status(401).json({
      success: false,
      error: {
        message: 'Token expired',
        code: 401,
        type: 'AuthenticationError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle database connection errors
  if (error.message?.includes('connect ECONNREFUSED') || 
      error.code === 'ECONNREFUSED' ||
      error.code === 'ENOTFOUND' ||
      error.code === 'ETIMEDOUT') {
    res.status(503).json({
      success: false,
      error: {
        message: 'Database connection failed',
        code: 503,
        type: 'DatabaseConnectionError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle PostgreSQL specific errors
  if (error.code?.startsWith('23')) { // PostgreSQL constraint violations
    let message = 'Database constraint violation';
    let statusCode = 400;

    switch (error.code) {
      case '23505': // unique_violation
        message = 'Duplicate entry - record already exists';
        break;
      case '23503': // foreign_key_violation
        message = 'Referenced record does not exist';
        break;
      case '23502': // not_null_violation
        message = 'Required field cannot be empty';
        break;
      case '23514': // check_violation
        message = 'Data does not meet validation requirements';
        break;
    }

    res.status(statusCode).json({
      success: false,
      error: {
        message,
        code: statusCode,
        type: 'DatabaseConstraintError',
        dbCode: error.code,
        ...(process.env.NODE_ENV === 'development' && { 
          sqlMessage: (error as any).sqlMessage || error.message 
        })
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle Redis connection errors
  if (error.message?.includes('Redis') || error.code === 'ECONNRESET') {
    res.status(503).json({
      success: false,
      error: {
        message: 'Cache service temporarily unavailable',
        code: 503,
        type: 'CacheConnectionError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle OpenAI API errors
  if (error.message?.includes('OpenAI') || error.message?.includes('AI service')) {
    res.status(503).json({
      success: false,
      error: {
        message: 'AI service temporarily unavailable',
        code: 503,
        type: 'AIServiceError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle rate limiting errors
  if ((error.message?.includes('rate limit') || (error as any).statusCode === 429)) {
    res.status(429).json({
      success: false,
      error: {
        message: 'Too many requests - please try again later',
        code: 429,
        type: 'RateLimitError',
        retryAfter: '15 minutes'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle file upload errors
  if ((error as any).code === 'LIMIT_FILE_SIZE') {
    res.status(413).json({
      success: false,
      error: {
        message: 'File too large',
        code: 413,
        type: 'FileUploadError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  if ((error as any).code === 'LIMIT_UNEXPECTED_FILE') {
    res.status(400).json({
      success: false,
      error: {
        message: 'Invalid file upload',
        code: 400,
        type: 'FileUploadError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle syntax errors (malformed JSON, etc.)
  if (error instanceof SyntaxError) {
    res.status(400).json({
      success: false,
      error: {
        message: 'Invalid request format',
        code: 400,
        type: 'SyntaxError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle timeout errors
  if (error.message?.includes('timeout') || error.code === 'ETIMEDOUT') {
    res.status(408).json({
      success: false,
      error: {
        message: 'Request timeout',
        code: 408,
        type: 'TimeoutError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle permission errors
  if (error.code === 'EACCES' || error.code === 'EPERM') {
    res.status(403).json({
      success: false,
      error: {
        message: 'Access denied',
        code: 403,
        type: 'PermissionError'
      },
      requestId: (req as any).id,
      timestamp: new Date().toISOString()
    });
    return;
  }

  // Handle default errors
  const isDevelopment = process.env.NODE_ENV === 'development';
  const statusCode = (error as any).statusCode || 500;

  res.status(statusCode).json({
    success: false,
    error: {
      message: isDevelopment ? error.message : 'Internal server error',
      code: statusCode,
      type: 'InternalServerError',
      ...(isDevelopment && { 
        stack: error.stack,
        originalError: {
          name: error.name,
          message: error.message,
          code: error.code
        }
      })
    },
    requestId: (req as any).id,
    timestamp: new Date().toISOString()
  });
};

export const notFoundHandler = (req: Request, res: Response): void => {
  const error = {
    success: false,
    error: {
      message: `Route ${req.method} ${req.path} not found`,
      code: 404,
      type: 'NotFoundError',
      availableRoutes: [
        'GET /api/health',
        'GET /api/docs',
        'POST /api/discovery',
        'POST /api/analysis/schema'
      ]
    },
    requestId: (req as any).id,
    timestamp: new Date().toISOString()
  };

  logger.warn('Route not found', {
    method: req.method,
    path: req.path,
    ip: req.ip,
    userAgent: req.get('User-Agent')
  });

  res.status(404).json(error);
};

// Async error wrapper utility
export const asyncHandler = (
  fn: (req: Request, res: Response, next: NextFunction) => Promise<any>
) => {
  return (req: Request, res: Response, next: NextFunction): void => {
    Promise.resolve(fn(req, res, next)).catch(next);
  };
};

// Global unhandled error handlers
export const setupGlobalErrorHandlers = (): void => {
  // Handle uncaught exceptions
  process.on('uncaughtException', (error: Error) => {
    logger.error('Uncaught Exception - Server shutting down:', {
      error: error.message,
      stack: error.stack,
      pid: process.pid
    });
    process.exit(1);
  });

  // Handle unhandled promise rejections
  process.on('unhandledRejection', (reason: any, promise: Promise<any>) => {
    logger.error('Unhandled Promise Rejection:', {
      reason: reason?.message || reason,
      stack: reason?.stack,
      promise: promise.toString()
    });
    process.exit(1);
  });

  // Handle SIGTERM (graceful shutdown)
  process.on('SIGTERM', () => {
    logger.info('SIGTERM received - starting graceful shutdown');
    process.exit(0);
  });

  // Handle SIGINT (Ctrl+C)
  process.on('SIGINT', () => {
    logger.info('SIGINT received - starting graceful shutdown');
    process.exit(0);
  });
};

// Error types for better error handling
export enum ErrorTypes {
  API_ERROR = 'APIError',
  VALIDATION_ERROR = 'ValidationError',
  AUTHENTICATION_ERROR = 'AuthenticationError',
  AUTHORIZATION_ERROR = 'AuthorizationError',
  DATABASE_ERROR = 'DatabaseError',
  CACHE_ERROR = 'CacheError',
  AI_SERVICE_ERROR = 'AIServiceError',
  RATE_LIMIT_ERROR = 'RateLimitError',
  FILE_UPLOAD_ERROR = 'FileUploadError',
  TIMEOUT_ERROR = 'TimeoutError',
  NOT_FOUND_ERROR = 'NotFoundError',
  INTERNAL_SERVER_ERROR = 'InternalServerError'
}

// Helper function to create standardized error responses
export const createErrorResponse = (
  message: string,
  code: number,
  type: ErrorTypes,
  details?: any,
  requestId?: string
) => {
  return {
    success: false,
    error: {
      message,
      code,
      type,
      ...(details && { details })
    },
    requestId: requestId || 'unknown',
    timestamp: new Date().toISOString()
  };
};



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\middleware\rateLimit.ts
--------------------------------------------------------------------------------
// src/middleware/rateLimit.ts
import { redis } from '@/config/redis';
import type { NextFunction, Request, RequestHandler, Response } from 'express';
import rateLimit, {
  type ClientRateLimitInfo,
  type RateLimitRequestHandler,
  type Store as RateLimitStore,
} from 'express-rate-limit';

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Central config (can move to config/env)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const RATE_LIMIT_CONFIG = {
  ai: { windowMs: 15 * 60 * 1000, max: 10, prefix: 'rl:ai:' },         // 10 per 15m
  discovery: { windowMs: 5 * 60 * 1000, max: 3, prefix: 'rl:disc:' },   // 3 per 5m
  api: { windowMs: 15 * 60 * 1000, max: 100, prefix: 'rl:api:' },       // 100 per 15m
};

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Minimal request shape (avoid importing Express Request types here)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
type MaybeAuthed = {
  user?: { id?: string | null | undefined; role?: string | null | undefined };
  ip?: string;
  originalUrl?: string;
};

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Redis-backed store for express-rate-limit v7
 * (Do NOT `implements Store` to avoid cross-package type conflicts)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
class RateLimitRedisStore {
  private readonly _prefix: string;
  private readonly _windowMs: number;

  constructor(windowMs: number, prefix = 'rl:') {
    this._prefix = prefix;
    this._windowMs = windowMs;
  }

  async increment(key: string): Promise<ClientRateLimitInfo> {
    const redisKey = `${this._prefix}${key}`;
    try {
      const client = redis.getClient();
      const totalHits = await client.incr(redisKey);

      // set expiry on first hit
      if (totalHits === 1) {
        await client.expire(redisKey, Math.ceil(this._windowMs / 1000));
      }

      const ttlSec = await client.ttl(redisKey);
      // resetTime is REQUIRED by ClientRateLimitInfo
      const resetTime =
        ttlSec > 0
          ? new Date(Date.now() + ttlSec * 1000)
          : new Date(Date.now() + this._windowMs);

      return { totalHits, resetTime };
    } catch {
      // If Redis is unavailable, be permissive but still provide a resetTime
      return { totalHits: 1, resetTime: new Date(Date.now() + this._windowMs) };
    }
  }

  async decrement(key: string): Promise<void> {
    try {
      await redis.getClient().decr(`${this._prefix}${key}`);
    } catch {
      // swallow
    }
  }

  async resetKey(key: string): Promise<void> {
    try {
      await redis.getClient().del(`${this._prefix}${key}`);
    } catch {
      // swallow
    }
  }

  // Optional: some versions may call resetAll; implement as no-op
  async resetAll(): Promise<void> {
    // Implement SCAN/DEL by prefix if you need global resets
  }
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Factory (uses `unknown` to avoid type-forks between @types/express trees)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export function createRateLimiter(options: {
  windowMs: number;
  max: number | ((req: unknown) => number);
  message?: string;
  skipSuccessfulRequests?: boolean;
  keyGenerator?: (req: unknown, res: unknown) => string;
  prefix?: string;
}): RateLimitRequestHandler {
  const {
    windowMs,
    max,
    message = 'Too many requests',
    skipSuccessfulRequests = false,
    keyGenerator,
    prefix = 'rl:',
  } = options;

  return rateLimit({
    windowMs,

    // ValueDeterminingMiddleware<number> compatible
    max: (req: unknown) => {
      // functional max support
      if (typeof max === 'function') {
        try {
          return max(req);
        } catch {
          return 100;
        }
      }
      // super_admin bypass (unlimited)
      const r = req as MaybeAuthed;
      if (r?.user?.role === 'super_admin') return Number.MAX_SAFE_INTEGER;
      return max;
    },

    standardHeaders: true,
    legacyHeaders: false,

    // Cast through unknown to satisfy possible distinct type trees
    store: new RateLimitRedisStore(windowMs, prefix) as unknown as RateLimitStore,

    keyGenerator: (req: unknown, res: unknown): string => {
      if (keyGenerator) {
        const k = keyGenerator(req, res);
        // Must always return a string
        return (k ?? '').toString() || (req as MaybeAuthed).ip || 'unknown';
      }
      const r = req as MaybeAuthed;
      const id = (r?.user?.id ?? '').toString().trim();
      return id || r?.ip || 'unknown';
    },

    skipSuccessfulRequests,

    handler: (req: unknown, res: unknown): void => {
      const _res = res as {
        setHeader: (k: string, v: string) => void;
        status: (c: number) => { json?: (b: unknown) => void };
      };
      const _req = req as MaybeAuthed;

      _res.setHeader('Retry-After', Math.ceil(windowMs / 1000).toString());
      _res
        .status(429)
        .json?.({
          success: false,
          error: { code: 'RATE_LIMIT_EXCEEDED', message },
          meta: { timestamp: new Date().toISOString(), path: _req?.originalUrl || '' },
        });
    },
  });
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Preconfigured limiters (raw RateLimitRequestHandler)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export const aiRateLimit: RateLimitRequestHandler = createRateLimiter({
  ...RATE_LIMIT_CONFIG.ai,
  message: 'AI request limit exceeded',
  prefix: RATE_LIMIT_CONFIG.ai.prefix,
  // Admins can do more AI calls
  max: (req) => ((req as MaybeAuthed).user?.role === 'admin' ? 50 : RATE_LIMIT_CONFIG.ai.max),
  keyGenerator: (req) => {
    const r = req as MaybeAuthed;
    const id = (r?.user?.id ?? '').toString().trim();
    return id || r?.ip || 'unknown';
  },
});

export const discoveryRateLimit: RateLimitRequestHandler = createRateLimiter({
  ...RATE_LIMIT_CONFIG.discovery,
  message: 'Discovery request limit exceeded',
  prefix: RATE_LIMIT_CONFIG.discovery.prefix,
  keyGenerator: (req) => {
    const r = req as MaybeAuthed;
    const id = (r?.user?.id ?? '').toString().trim();
    return id || r?.ip || 'unknown';
  },
});

export const apiRateLimit: RateLimitRequestHandler = createRateLimiter({
  ...RATE_LIMIT_CONFIG.api,
  message: 'API request limit exceeded',
  prefix: RATE_LIMIT_CONFIG.api.prefix,
  skipSuccessfulRequests: true,
  keyGenerator: (req) => {
    const r = req as MaybeAuthed;
    const id = (r?.user?.id ?? '').toString().trim();
    return id || r?.ip || 'unknown';
  },
});

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Wrappers to plain Express RequestHandler (fix type tree mismatches)
 * Use these in your routers: aiRateLimitMw/discoveryRateLimitMw/apiRateLimitMw
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const asMiddleware = (rl: RateLimitRequestHandler): RequestHandler =>
  (req: Request, res: Response, next: NextFunction) =>
    (rl as unknown as (req: Request, res: Response, next: NextFunction) => void)(req, res, next);

export const aiRateLimitMw: RequestHandler = asMiddleware(aiRateLimit);
export const discoveryRateLimitMw: RequestHandler = asMiddleware(discoveryRateLimit);
export const apiRateLimitMw: RequestHandler = asMiddleware(apiRateLimit);



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\middleware\validation.ts
--------------------------------------------------------------------------------
import { APIError } from '@/utils/errors';
import { NextFunction, Request, Response } from 'express';
import Joi from 'joi';

const discoveryRequestSchema = Joi.object({
  dataSourceId: Joi.string().uuid().required(),
  schemas: Joi.array().items(Joi.string()).optional(),
  tables: Joi.array().items(Joi.string()).optional(),
  options: Joi.object({
    sampleSize: Joi.number().min(10).max(10000).default(100),
    includeData: Joi.boolean().default(true),
    analysisDepth: Joi.string().valid('basic', 'detailed', 'comprehensive').default('detailed')
  }).optional()
});

const nlQuerySchema = Joi.object({
  query: Joi.string().min(3).max(500).required(),
  context: Joi.object({
    schemas: Joi.array().items(Joi.string()).optional(),
    tables: Joi.array().items(Joi.string()).optional(),
    fields: Joi.array().items(Joi.string()).optional()
  }).optional()
});

const analysisRequestSchema = Joi.object({
  schema: Joi.object({
    name: Joi.string().required(),
    tables: Joi.array().items(Joi.object({
      schema: Joi.string().required(),
      name: Joi.string().required(),
      columns: Joi.array().items(Joi.object({
        name: Joi.string().required(),
        type: Joi.string().required(),
        nullable: Joi.boolean().required(),
        description: Joi.string().optional()
      })).required()
    })).required()
  }).required()
});

export const validateDiscoveryRequest = (req: Request, res: Response, next: NextFunction): void => {
  void res;
  const { error } = discoveryRequestSchema.validate(req.body);
  if (error) {
    next(new APIError(`Validation error: ${error.details[0].message}`, 400));
    return;
  }
  next();
};

export const validateNLQuery = (req: Request, res: Response, next: NextFunction): void => {
  void res;
  const { error } = nlQuerySchema.validate(req.body);
  if (error) {
    next(new APIError(`Validation error: ${error.details[0].message}`, 400));
    return;
  }
  next();
};

export const validateAnalysisRequest = (req: Request, res: Response, next: NextFunction): void => {
  void res;
  const { error } = analysisRequestSchema.validate(req.body);
  if (error) {
    next(new APIError(`Validation error: ${error.details[0].message}`, 400));
    return;
  }
  next();
};



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\models\Analysis.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\models\DataField.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\models\Discovery.ts
--------------------------------------------------------------------------------
// src/models/Discovery.ts
import { db } from '@/config/database';
import { logger } from '@/utils/logger';

/** Domain model */
export interface DiscoverySession {
  sessionId: string;
  userId: string;
  dataSourceId: string;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  progress: number;
  results?: unknown; // if present, must not be undefined (exactOptionalPropertyTypes)
  error?: string;    // if present, must not be undefined
  createdAt: Date;
  updatedAt: Date;
}

/** Shape returned by Postgres (snake_case) */
interface DbDiscoveryRow {
  session_id: string;
  user_id: string;
  data_source_id: string;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  progress: number;
  results: unknown | null;
  error: string | null;
  created_at: Date;
  updated_at: Date;
}

export class DiscoveryModel {
  static async create(session: Partial<DiscoverySession>): Promise<DiscoverySession> {
    try {
      const result = await db.query(
        `INSERT INTO discovery_sessions 
           (session_id, user_id, data_source_id, status, progress, created_at, updated_at)
         VALUES ($1,        $2,     $3,            $4,    $5,      NOW(),     NOW())
         RETURNING *`,
        [
          session.sessionId,
          session.userId,
          session.dataSourceId,
          session.status,
          session.progress,
        ]
      );

      const row = (result.rows as unknown as DbDiscoveryRow[])[0];
      return this.mapFromDb(row);
    } catch (error) {
      logger.error('Failed to create discovery session:', error);
      throw error;
    }
  }

  static async findById(sessionId: string): Promise<DiscoverySession | null> {
    try {
      const result = await db.query(
        'SELECT * FROM discovery_sessions WHERE session_id = $1',
        [sessionId]
      );

      const rows = result.rows as unknown as DbDiscoveryRow[];
      if (rows.length === 0) return null;
      return this.mapFromDb(rows[0]);
    } catch (error) {
      logger.error('Failed to find discovery session:', error);
      throw error;
    }
  }

  static async findByUserId(
    userId: string,
    limit = 20,
    offset = 0
  ): Promise<DiscoverySession[]> {
    try {
      const result = await db.query(
        `SELECT * FROM discovery_sessions
          WHERE user_id = $1
          ORDER BY created_at DESC
          LIMIT $2 OFFSET $3`,
        [userId, limit, offset]
      );

      const rows = result.rows as unknown as DbDiscoveryRow[];
      return rows.map((row) => this.mapFromDb(row));
    } catch (error) {
      logger.error('Failed to find discovery sessions by user:', error);
      throw error;
    }
  }

  static async update(
    sessionId: string,
    updates: Partial<DiscoverySession>
  ): Promise<DiscoverySession | null> {
    try {
      const setClause: string[] = [];
      const values: unknown[] = [];
      let paramIndex = 1;

      // Build dynamic SET list (convert camelCase -> snake_case)
      for (const [key, value] of Object.entries(updates)) {
        if (key === 'sessionId') continue; // never update PK here
        const dbKey = key.replace(/[A-Z]/g, (ltr) => `_${ltr.toLowerCase()}`);
        setClause.push(`${dbKey} = $${paramIndex}`);
        values.push(value);
        paramIndex++;
      }

      if (setClause.length === 0) return null;

      // always set updated_at
      setClause.push(`updated_at = NOW()`);

      // WHERE param
      values.push(sessionId);

      const result = await db.query(
        `UPDATE discovery_sessions
            SET ${setClause.join(', ')}
          WHERE session_id = $${paramIndex}
          RETURNING *`,
        values
      );

      const rows = result.rows as unknown as DbDiscoveryRow[];
      if (rows.length === 0) return null;
      return this.mapFromDb(rows[0]);
    } catch (error) {
      logger.error('Failed to update discovery session:', error);
      throw error;
    }
  }

  static async delete(sessionId: string, userId: string): Promise<boolean> {
    try {
      const result = await db.query(
        'DELETE FROM discovery_sessions WHERE session_id = $1 AND user_id = $2',
        [sessionId, userId]
      );
      return result.rowCount > 0;
    } catch (error) {
      logger.error('Failed to delete discovery session:', error);
      throw error;
    }
  }

  /** Map DB row -> domain object, respecting exactOptionalPropertyTypes */
  private static mapFromDb(row: DbDiscoveryRow): DiscoverySession {
    const base: DiscoverySession = {
      sessionId: row.session_id,
      userId: row.user_id,
      dataSourceId: row.data_source_id,
      status: row.status,
      progress: row.progress,
      createdAt: row.created_at,
      updatedAt: row.updated_at,
    };

    // Only assign optional fields if they are non-null/defined
    if (row.results !== null && row.results !== undefined) {
      (base as { results: unknown }).results = row.results;
    }
    if (row.error !== null && row.error !== undefined) {
      (base as { error: string }).error = row.error;
    }

    return base;
  }
}



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\processors\DataProcessor.ts
--------------------------------------------------------------------------------
// src/processors/DataProcessor.ts
import type {
  DataSample,
  IDataProcessor,
  QualityIssue,
} from '@/services/AnalysisService';

export class DataProcessor implements IDataProcessor {
  async analyzeSampleData(
    samples: DataSample[],
  ): Promise<{ analysis: Record<string, unknown>[]; qualityIssues: QualityIssue[] }> {
    const analysis: Record<string, unknown>[] = [];
    const qualityIssues: QualityIssue[] = [];

    for (const s of samples) {
      const values = Array.isArray(s.values) ? s.values : [];
      const nonNull = values.filter((v) => v !== null && v !== undefined);
      const nulls = values.length - nonNull.length;

      const uniqueCount = new Set(nonNull.map((v) => JSON.stringify(v))).size;

      analysis.push({
        column: s.columnName,
        count: values.length,
        nulls,
        nullRate: values.length ? +(nulls / values.length * 100).toFixed(2) : 0,
        uniqueCount,
        sample: nonNull.slice(0, 5),
      });

      if (nulls > 0) {
        qualityIssues.push({
          column: s.columnName,
          type: 'null_values',
          severity: nulls / (values.length || 1) > 0.2 ? 'High' : 'Low',
          count: nulls,
          message: 'Column contains null/undefined values',
          sample: values.slice(0, 5),
        });
      }

      const lower = s.columnName.toLowerCase();

      if (lower.includes('email')) {
        const bad = nonNull.filter(
          (v) => typeof v === 'string' && !/^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(v),
        );
        if (bad.length) {
          qualityIssues.push({
            column: s.columnName,
            type: 'format_inconsistency',
            severity: bad.length / (nonNull.length || 1) > 0.05 ? 'Medium' : 'Low',
            count: bad.length,
            message: 'Invalid email addresses detected',
            sample: bad.slice(0, 5),
          });
        }
      }

      if (lower.includes('phone')) {
        const bad = nonNull.filter(
          (v) => typeof v === 'string' && !/^\+?[0-9().\- ]{7,}$/.test(v),
        );
        if (bad.length) {
          qualityIssues.push({
            column: s.columnName,
            type: 'format_inconsistency',
            severity: bad.length / (nonNull.length || 1) > 0.05 ? 'Medium' : 'Low',
            count: bad.length,
            message: 'Invalid phone numbers detected',
            sample: bad.slice(0, 5),
          });
        }
      }
    }

    return { analysis, qualityIssues };
  }
}

export default DataProcessor;



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\processors\FieldProcessor.ts
--------------------------------------------------------------------------------
// src/processors/FieldProcessor.ts
export interface FieldInfo {
  name: string;
  type: string;
  nullable: boolean;
  description?: string;
}

export type Classification = 'PII' | 'PHI' | 'Financial' | 'General';
export type Sensitivity = 'Low' | 'Medium' | 'High';

export interface ProcessedField extends FieldInfo {
  classification: Classification;
  sensitivity: Sensitivity;
  patterns: string[];
}

/** Very lightweight field classifier used by SchemaProcessor */
export class FieldProcessor {
  processFields(fields: FieldInfo[]): ProcessedField[] {
    return fields.map((f) => {
      const classification = this.classify(f);
      const sensitivity = this.sensitivityFromClass(classification);
      const patterns = this.detectPatterns(f);

      return { ...f, classification, sensitivity, patterns };
    });
  }

  private classify(f: FieldInfo): Classification {
    const n = f.name.toLowerCase();
    if (/(email|first_?name|last_?name|phone|ssn|address)/.test(n)) return 'PII';
    if (/(health|medical|patient|diagnosis)/.test(n)) return 'PHI';
    if (/(payment|amount|price|card|invoice|billing)/.test(n)) return 'Financial';
    return 'General';
    }

  private sensitivityFromClass(c: Classification): Sensitivity {
    if (c === 'PHI') return 'High';
    if (c === 'PII' || c === 'Financial') return 'Medium';
    return 'Low';
  }

  private detectPatterns(f: FieldInfo): string[] {
    const out: string[] = [];
    const n = f.name.toLowerCase();
    if (n.includes('email')) out.push('email');
    if (n.includes('phone')) out.push('phone');
    if (/(date|timestamp)/.test(f.type.toLowerCase())) out.push('date');
    return out;
  }
}

export default FieldProcessor;



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\processors\SchemaProcessor.ts
--------------------------------------------------------------------------------
// src/processors/SchemaProcessor.ts
import { logger } from '@utils/logger';
import FieldProcessor, { FieldInfo } from './FieldProcessor';

export interface SchemaInfo {
  name: string;
  tables: TableInfo[];
}

export interface TableInfo {
  schema: string;
  name: string;
  columns: FieldInfo[];
  rowCount?: number;
  relationships?: RelationshipInfo[];
}

export interface RelationshipInfo {
  type: 'foreign_key' | 'one_to_many' | 'many_to_many';
  targetTable: string;
  targetColumn: string;
  sourceColumn: string;
}

export interface ProcessedTable extends TableInfo {
  processedColumns: Array<
    FieldInfo & { classification: string; sensitivity: string; patterns: string[] }
  >;
  governance: {
    classification: string;
    sensitivity: string;
    complianceFrameworks: string[];
    suggestedPolicies: string[];
  };
}

export interface SchemaSummary {
  totalTables: number;
  totalColumns: number;
  sensitiveDataTables: number;
  complianceRequirements: string[];
  recommendations: string[];
}

export interface ProcessedSchema {
  name: string;
  tables: ProcessedTable[];
  summary: SchemaSummary;
}

export class SchemaProcessor {
  private readonly fieldProcessor: FieldProcessor;

  constructor() {
    this.fieldProcessor = new FieldProcessor();
  }

  public async processSchema(schema: SchemaInfo): Promise<ProcessedSchema> {
    try {
      logger.info('Processing schema', { schema: schema.name, tables: schema.tables.length });

      const processedTables = await Promise.all(
        schema.tables.map((table) => this.processTable(table))
      );

      const summary = this.generateSchemaSummary(processedTables);

      return { name: schema.name, tables: processedTables, summary };
    } catch (error) {
      logger.error('Schema processing failed:', error);
      throw error;
    }
  }

  private async processTable(table: TableInfo): Promise<ProcessedTable> {
    try {
      const processedColumns = this.fieldProcessor.processFields(table.columns);
      const governance = this.generateTableGovernance(processedColumns);

      return { ...table, processedColumns, governance };
    } catch (error) {
      logger.error('Table processing failed:', { table: table.name, error });
      throw error;
    }
  }

  private generateTableGovernance(
    processedColumns: ProcessedTable['processedColumns']
  ): ProcessedTable['governance'] {
    const classifications = processedColumns.map((c) => c.classification);
    const sensitivities = processedColumns.map((c) => c.sensitivity);

    const hasPhiData = classifications.includes('PHI');
    const hasPiiData = classifications.includes('PII');
    const hasFinancialData = classifications.includes('Financial');

    let classification = 'General';
    if (hasPhiData) classification = 'PHI';
    else if (hasPiiData) classification = 'PII';
    else if (hasFinancialData) classification = 'Financial';

    let sensitivity = 'Low';
    if (sensitivities.includes('High')) sensitivity = 'High';
    else if (sensitivities.includes('Medium')) sensitivity = 'Medium';

    const complianceFrameworks: string[] = [];
    if (hasPhiData) complianceFrameworks.push('HIPAA');
    if (hasPiiData) complianceFrameworks.push('GDPR', 'CCPA');
    if (hasFinancialData) complianceFrameworks.push('SOX', 'PCI-DSS');

    const suggestedPolicies: string[] = [];
    if (sensitivity === 'High') {
      suggestedPolicies.push('Data encryption required', 'Access approval required', 'Regular access reviews');
    }
    if (hasPiiData || hasPhiData) {
      suggestedPolicies.push('Data retention policy', 'Right to be forgotten procedures');
    }

    return { classification, sensitivity, complianceFrameworks, suggestedPolicies };
  }

  private generateSchemaSummary(tables: ProcessedTable[]): SchemaSummary {
    const totalTables = tables.length;
    const totalColumns = tables.reduce((sum, t) => sum + t.columns.length, 0);
    const sensitiveDataTables = tables.filter(
      (t) => t.governance.sensitivity === 'High' || t.governance.sensitivity === 'Medium'
    ).length;

    const complianceSet = new Set<string>();
    for (const t of tables) {
      for (const f of t.governance.complianceFrameworks) complianceSet.add(f);
    }

    const recommendations: string[] = [
      `${sensitiveDataTables} of ${totalTables} tables contain sensitive data`,
      'Implement role-based access controls',
      'Regular compliance audits recommended',
      'Consider data classification labels',
    ];
    if (complianceSet.has('GDPR')) recommendations.push('GDPR compliance review required');
    if (complianceSet.has('HIPAA')) recommendations.push('HIPAA security controls needed');

    return {
      totalTables,
      totalColumns,
      sensitiveDataTables,
      complianceRequirements: Array.from(complianceSet),
      recommendations,
    };
  }
}

// export both ways so AnalysisService can `new (mod.SchemaProcessor ?? mod.default)()`
export default SchemaProcessor;



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\prompts\fieldDiscovery.ts
--------------------------------------------------------------------------------
// src/prompts/fieldDiscovery.ts

/* eslint-disable max-lines */
type Sensitivity =
  | 'Low'
  | 'Medium'
  | 'High'
  | 'Critical';

type Classification =
  | 'General'
  | 'PII'
  | 'PHI'
  | 'Financial';

export interface FieldDiscoveryColumn {
  name: string;
  type: string;
  nullable: boolean;
  description?: string | null | undefined;
  /** Optional small set of example values for extra context */
  exampleValues?: readonly unknown[] | null | undefined;
}

export interface FieldDiscoveryResultField {
  name: string;
  type: string;
  classification: Classification;
  sensitivity: Sensitivity;
  description: string;
  suggestedRules: string[];
  dataPatterns: string[];
  businessContext: string;
}

export interface FieldDiscoveryRequest {
  schema: string;
  tableName: string;
  /** Optional business context of the table */
  context?: string | null | undefined;
  /** Table columns */
  columns: readonly FieldDiscoveryColumn[];
  /** Optional sample data (records/rows) */
  sampleData?: readonly Record<string, unknown>[] | null | undefined;
  /** Optional region for compliance nuance */
  region?: 'US' | 'EU' | 'Global';
  /** Optional hint for how deep the analysis should go */
  aiAnalysisDepth?: 'basic' | 'detailed' | 'comprehensive';
}

export const FIELD_DISCOVERY_SYSTEM_PROMPT = `
You are an expert data governance analyst specializing in:
- Field classification and data sensitivity analysis
- Compliance requirements (GDPR, HIPAA, CCPA, SOX, PCI-DSS)
- Data quality and governance best practices
- Business context understanding

GENERAL PRINCIPLES
- Be precise, practical, and implementation-oriented.
- Prefer least-privilege and data minimization.
- Recommend enforceable validation and indexing strategies when useful.

RESPONSE FORMAT (STRICT)
- Respond with STRICT JSON only. Do NOT include markdown, prose, or comments.
- Use these enums exactly:
  classification: "General" | "PII" | "PHI" | "Financial"
  sensitivity: "Low" | "Medium" | "High" | "Critical"
- Confidence is 0..1 (number).
- Recommendations arrays must be concise and actionable.
`.trim();

/**
 * Build a robust user prompt for field discovery.
 * - Strong typing (no implicit any)
 * - Sanitizes and truncates inputs to control token size
 * - Adds light heuristics to give the model helpful hints
 */
export function buildFieldDiscoveryPrompt(
  request: FieldDiscoveryRequest,
  opts?: {
    maxSampleRows?: number;
    maxCharsPerValue?: number;
    maxColumns?: number;
  }
): string {
  const maxSampleRows = Number.isFinite(opts?.maxSampleRows) ? Math.max(0, Number(opts?.maxSampleRows)) : 5;
  const maxCharsPerValue = Number.isFinite(opts?.maxCharsPerValue) ? Math.max(16, Number(opts?.maxCharsPerValue)) : 120;
  const maxColumns = Number.isFinite(opts?.maxColumns) ? Math.max(1, Number(opts?.maxColumns)) : 200;

  const cleanContext = sanitizeText(request.context);
  const cleanRegion = request.region ?? 'Global';
  const depth = request.aiAnalysisDepth ?? 'detailed';

  const columns = (request.columns ?? []).slice(0, maxColumns).map(safeColumn);
  const hints = buildHeuristicHints(columns);

  const sample = pruneSampleData(request.sampleData ?? [], maxSampleRows, maxCharsPerValue);

  const columnsBlock = columns
    .map((col) => {
      const nullable = col.nullable ? 'NULLABLE' : 'NOT NULL';
      const desc = col.description ? ` - ${sanitizeInline(col.description)}` : '';
      const examples = (col.exampleValues && col.exampleValues.length)
        ? ` (examples: ${stringifyInline(col.exampleValues.slice(0, 3), maxCharsPerValue)})`
        : '';
      return `- ${col.name} (${col.type}) ${nullable}${desc}${examples}`;
    })
    .join('\n');

  const sampleBlock = sample.length
    ? `Sample Data (first ${sample.length} rows):
${JSON.stringify(sample, null, 2)}`
    : '';

  const hintsBlock = hints.length
    ? `Heuristic Hints:
${hints.map((h) => `- ${h}`).join('\n')}`
    : '';

  // Final strict instructions to shape output for downstream services
  const strictJsonContract = `
RETURN STRICT JSON ONLY (no markdown, no comments) with this structure:
{
  "fields": [
    {
      "name": "field_name",
      "type": "data_type",
      "classification": "PII|PHI|Financial|General",
      "sensitivity": "High|Medium|Low|Critical",
      "description": "clear business meaning and purpose",
      "suggestedRules": ["specific quality rule 1", "specific quality rule 2"],
      "dataPatterns": ["observed pattern 1", "observed pattern 2"],
      "businessContext": "business context and usage"
    }
  ],
  "recommendations": {
    "governance": ["actionable governance recommendation 1", "recommendation 2"],
    "quality": ["actionable quality recommendation 1", "recommendation 2"],
    "compliance": ["specific compliance requirement 1", "requirement 2"]
  },
  "confidence": 0.0-1.0
}

CONSTRAINTS
- Keep arrays concise (max 5 items each) and prioritize high impact actions.
- Prefer column-specific recommendations over generic advice.
- Match enum values exactly as specified.
- If uncertain about a field, mark classification "General" with lower confidence.
`.trim();

  const userPrompt = `
Analyze the following database table and provide comprehensive field classification.

Context:
- Schema: ${sanitizeInline(request.schema)}
- Table: ${sanitizeInline(request.tableName)}
- Region: ${cleanRegion}
- Depth: ${depth}
${cleanContext ? `- Business Context: ${sanitizeInline(cleanContext)}` : ''}

Columns:
${columnsBlock}

${sampleBlock ? `\n${sampleBlock}\n` : ''}

${hintsBlock ? `\n${hintsBlock}\n` : ''}

${strictJsonContract}

FOCUS AREAS
1) Accurate classification based on names, types, and sample data.
2) Practical quality rules (validation, uniqueness, ranges, referential integrity).
3) Specific compliance requirements (GDPR/HIPAA/PCI-DSS/CCPA/SOX) relevant to the region.
4) Clear business value and usage context for each field.
5) Actionable, high-impact recommendations.
`.trim();

  return userPrompt;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Helpers
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

function sanitizeText(input?: string | null | undefined): string | undefined {
  if (!input) return undefined;
  const trimmed = input.trim();
  if (!trimmed) return undefined;
  // very light sanitization to cut obvious prompt-injection vectors
  return trimmed.replace(/[<>]/g, '');
}

function sanitizeInline(input: string): string {
  return sanitizeText(input) ?? '';
}

function safeColumn(col: FieldDiscoveryColumn): FieldDiscoveryColumn {
  return {
    name: String(col.name).trim(),
    type: String(col.type).trim(),
    nullable: Boolean(col.nullable),
    description: col.description ?? undefined,
    exampleValues: col.exampleValues ?? undefined,
  };
}

function stringifyInline(values: readonly unknown[], maxCharsPerValue: number): string {
  return values
    .map((v) => compactValue(v, maxCharsPerValue))
    .join(', ');
}

function compactValue(v: unknown, maxCharsPerValue: number): string {
  try {
    const s = typeof v === 'string' ? v : JSON.stringify(v);
    const trimmed = (s ?? '').toString();
    if (trimmed.length <= maxCharsPerValue) return trimmed;
    return trimmed.slice(0, maxCharsPerValue) + 'â€¦';
  } catch {
    return '[unserializable]';
  }
}

function pruneSampleData(
  rows: readonly Record<string, unknown>[],
  maxRows: number,
  maxCharsPerValue: number
): Record<string, unknown>[] {
  const limited = rows.slice(0, Math.max(0, maxRows));
  return limited.map((row) => {
    const out: Record<string, unknown> = {};
    for (const [k, v] of Object.entries(row)) {
      if (v === null || v === undefined) {
        out[k] = v as null | undefined;
        continue;
      }
      if (typeof v === 'string') {
        out[k] = redactString(v, maxCharsPerValue);
      } else if (typeof v === 'number' || typeof v === 'boolean') {
        out[k] = v;
      } else if (v instanceof Date) {
        out[k] = v.toISOString();
      } else {
        // stringify and truncate complex values
        out[k] = compactValue(v, maxCharsPerValue);
      }
    }
    return out;
  });
}

function redactString(s: string, maxChars: number): string {
  // simple redactions for PII-ish patterns
  const emailRx = /([A-Z0-9._%+-]+)@([A-Z0-9.-]+\.[A-Z]{2,})/gi;
  const phoneRx = /\+?\d{1,3}[-.\s]?\(?\d{2,4}\)?[-.\s]?\d{2,4}[-.\s]?\d{2,6}/g;
  const ssnRx = /\b\d{3}-?\d{2}-?\d{4}\b/g;

  let out = s.replace(emailRx, (_, user, host) => `${user[0]}***@${host}`);
  out = out.replace(phoneRx, (m) => m.slice(0, 3) + '***' + m.slice(-2));
  out = out.replace(ssnRx, '***-**-****');

  if (out.length <= maxChars) return out;
  return out.slice(0, maxChars) + 'â€¦';
}

function buildHeuristicHints(columns: readonly FieldDiscoveryColumn[]): string[] {
  const hints: string[] = [];

  for (const col of columns) {
    const name = col.name.toLowerCase();
    const type = col.type.toLowerCase();

    if (/email/.test(name)) {
      hints.push(`Column "${col.name}" likely contains email addresses â†’ consider "PII" with Medium sensitivity and format validation.`);
    }
    if (/(phone|mobile|cell)/.test(name)) {
      hints.push(`Column "${col.name}" likely contains phone numbers â†’ consider "PII"; recommend E.164 normalization and validation.`);
    }
    if (/(ssn|social[_-]?security)/.test(name)) {
      hints.push(`Column "${col.name}" may be SSN â†’ consider "PII" with High sensitivity; encryption at rest, restricted access.`);
    }
    if (/(dob|date[_-]?of[_-]?birth|birth[_-]?date)/.test(name) || (type.includes('date') && /birth/.test(name))) {
      hints.push(`Column "${col.name}" looks like date of birth â†’ "PII" with Medium/High sensitivity depending on jurisdiction.`);
    }
    if (/(card|cc|credit[_-]?card)/.test(name)) {
      hints.push(`Column "${col.name}" may be card data â†’ classify "Financial" with High sensitivity; tokenize or avoid storing PAN.`);
    }
    if (/(amount|price|salary|income|revenue)/.test(name)) {
      hints.push(`Column "${col.name}" is financial metric â†’ classify "Financial" (usually Medium sensitivity), enforce numeric ranges.`);
    }
    if (/(patient|diagnosis|medical|icd|phi)/.test(name)) {
      hints.push(`Column "${col.name}" suggests PHI â†’ classify "PHI" with High/Critical sensitivity and HIPAA safeguards.`);
    }
    if (/(ip[_-]?address|ipv4|ipv6)/.test(name)) {
      hints.push(`Column "${col.name}" may store IPs â†’ treat as "PII" in some frameworks; consider anonymization/retention policy.`);
    }
    if (/password|secret|token|api[_-]?key/.test(name)) {
      hints.push(`Column "${col.name}" looks like a credential/secret â†’ do not store raw; hash/tokenize and restrict access.`);
    }
  }

  return Array.from(new Set(hints)).slice(0, 12); // cap to keep the prompt lean
}



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\prompts\naturalLanguage.ts
--------------------------------------------------------------------------------
export const NATURAL_LANGUAGE_SYSTEM_PROMPT = `You are an expert SQL analyst and data translator specializing in:
- Converting natural language to SQL queries
- Database schema understanding
- Query optimization and safety
- Business intelligence and analytics

Convert user queries into safe, efficient SQL statements with detailed explanations.`;

export const buildNaturalLanguagePrompt = (query: any): string => {
  const contextInfo = query.context ? `
Available Database Context:
- Schemas: ${query.context.schemas?.join(', ') || 'None specified'}
- Tables: ${query.context.tables?.join(', ') || 'None specified'}  
- Key Fields: ${query.context.fields?.join(', ') || 'None specified'}
` : '';

  return `Convert this natural language query to SQL:

User Query: "${query.query}"

${contextInfo}

Please provide a comprehensive JSON response:
{
  "sql": "Complete SELECT statement with proper formatting",
  "explanation": "Detailed step-by-step explanation of the query logic",
  "tables": ["table1", "table2"],
  "fields": ["field1", "field2", "field3"],
  "joinTypes": ["INNER", "LEFT", "RIGHT"],
  "aggregations": ["COUNT", "SUM", "AVG"],
  "filters": ["WHERE conditions applied"],
  "orderBy": ["Sorting criteria"],
  "confidence": 0.95,
  "warnings": ["Potential performance issues", "Security considerations"],
  "suggestions": ["Query optimization tips", "Alternative approaches"],
  "estimatedComplexity": "Simple|Medium|Complex",
  "estimatedExecutionTime": "Fast|Medium|Slow"
}

Important considerations:
1. Generate safe, read-only queries (SELECT only)
2. Include proper WHERE clauses to limit results
3. Use appropriate JOINs based on relationships
4. Consider performance implications
5. Flag potential security risks
6. Provide clear explanations for business users
7. Suggest LIMIT clauses for large datasets`;
};


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\prompts\qualityRules.ts
--------------------------------------------------------------------------------
export const QUALITY_RULES_SYSTEM_PROMPT = `You are a data quality expert specializing in:
- Data validation and quality rules
- Business rule implementation
- Data profiling and anomaly detection
- Quality metrics and monitoring

Generate specific, measurable, and implementable data quality rules.`;

export const buildQualityRulesPrompt = (fieldInfo: any): string => {
  return `Generate specific data quality rules for the following field:

Field Information:
- Name: ${fieldInfo.name}
- Data Type: ${fieldInfo.type}
- Classification: ${fieldInfo.classification}
- Sensitivity: ${fieldInfo.sensitivity}
- Business Context: ${fieldInfo.businessContext}
- Observed Patterns: ${JSON.stringify(fieldInfo.dataPatterns || [])}
- Sample Data: ${JSON.stringify(fieldInfo.sampleData || [])}

Please provide a JSON response with specific, actionable quality rules:
{
  "rules": [
    {
      "name": "Descriptive rule name",
      "type": "validation|format|range|completeness|uniqueness|consistency",
      "description": "Detailed description of what the rule checks",
      "implementation": "Specific implementation details or SQL/logic",
      "severity": "Critical|High|Medium|Low",
      "automated": true|false,
      "frequency": "Real-time|Daily|Weekly|Monthly"
    }
  ]
}

Focus on:
1. Specific, measurable criteria
2. Clear implementation guidance
3. Appropriate severity levels
4. Automation feasibility
5. Business impact consideration`;
};


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\routes\analysis.ts
--------------------------------------------------------------------------------
import { AnalysisController } from '@/controllers/AnalysisController';
import { authenticateToken } from '@/middleware/auth';
import { validateAnalysisRequest } from '@/middleware/validation';
import { Router } from 'express';

const router = Router();
const analysisController = new AnalysisController();

// All analysis routes require authentication
router.use(authenticateToken);

// Analysis endpoints
router.post('/schema', validateAnalysisRequest, analysisController.analyzeSchema);
router.post('/data-sample', analysisController.analyzeDataSample);
router.post('/quality-check', analysisController.performQualityCheck);

export default router;


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\routes\discovery.ts
--------------------------------------------------------------------------------
import { DiscoveryController } from '@/controllers/DiscoveryController';
import { authenticateToken } from '@/middleware/auth';
import { aiRateLimitMw, discoveryRateLimitMw } from '@/middleware/rateLimit';
import { validateDiscoveryRequest, validateNLQuery } from '@/middleware/validation';
import { NextFunction, Request, Response, Router } from 'express';

const router = Router();
const controller = new DiscoveryController();

const asyncHandler =
  <T extends (req: Request, res: Response, next: NextFunction) => Promise<any>>(fn: T) =>
  (req: Request, res: Response, next: NextFunction) =>
    Promise.resolve(fn.call(controller, req, res, next)).catch(next);

// In dev weâ€™ll allow requests without a token; see auth middleware below
router.use(authenticateToken);

router.post('/', discoveryRateLimitMw, validateDiscoveryRequest, asyncHandler(controller.startDiscovery));
router.post('/query', aiRateLimitMw, validateNLQuery, asyncHandler(controller.processNaturalLanguageQuery));
router.post('/quality-rules', aiRateLimitMw, asyncHandler(controller.generateQualityRules));
router.post('/explain-violation', aiRateLimitMw, asyncHandler(controller.explainViolation));

router.get('/', asyncHandler(controller.listDiscoverySessions));
router.get('/:sessionId', asyncHandler(controller.getDiscoveryStatus));
router.delete('/:sessionId', asyncHandler(controller.deleteDiscoverySession));

export default router;



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\routes\index.ts
--------------------------------------------------------------------------------
import { HealthController } from '@/controllers/HealthController';
import { Router } from 'express';
import analysisRoutes from './analysis';
import discoveryRoutes from './discovery';

const router = Router();
const healthController = new HealthController();

// Health check routes
router.get('/health', healthController.checkHealth);
router.get('/health/ready', healthController.checkReadiness);
router.get('/health/live', healthController.checkLiveness);

// Feature routes
router.use('/discovery', discoveryRoutes);
router.use('/analysis', analysisRoutes);

// API documentation endpoint
router.get('/docs', (req, res) => {
  void req;
  res.json({
    service: 'CWIC AI Service',
    version: process.env.APP_VERSION || '1.0.0',
    description: 'AI-powered data discovery and governance service',
    endpoints: {
      discovery: {
        'POST /api/discovery': 'Start field discovery session',
        'GET /api/discovery/:sessionId': 'Get discovery status and results',
        'GET /api/discovery': 'List discovery sessions',
        'DELETE /api/discovery/:sessionId': 'Delete discovery session',
        'POST /api/discovery/query': 'Process natural language query',
        'POST /api/discovery/quality-rules': 'Generate quality rules',
        'POST /api/discovery/explain-violation': 'Explain data quality violation'
      },
      analysis: {
        'POST /api/analysis/schema': 'Analyze database schema',
        'POST /api/analysis/data-sample': 'Analyze data samples',
        'POST /api/analysis/quality-check': 'Perform quality analysis'
      },
      health: {
        'GET /api/health': 'Service health status',
        'GET /api/health/ready': 'Readiness check',
        'GET /api/health/live': 'Liveness check'
      }
    }
  });
});

export default router;



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\scripts\cleanup.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\scripts\migrate.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\scripts\seed.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\server.ts
--------------------------------------------------------------------------------
// src/server.ts
import { logger } from '@utils/logger';
import App from './app';

const PORT = Number(process.env.PORT ?? 3003);     // << default 3003, not 8003
const HOST = process.env.HOST || '0.0.0.0';

function start() {
  try {
    const expressApp = new App().getApp();

    // Ensure a simple health endpoint exists here (even if App already has one)
    expressApp.get('/health', (_req, res) => {
      res.json({ ok: true, service: 'ai-service', pid: process.pid });
    });

    const server = expressApp.listen(PORT, HOST, () => {
      logger.info(`ðŸš€ AI Service listening on http://${HOST}:${PORT}`);
      logger.info(`ðŸ“ Health: http://${HOST}:${PORT}/health`);
      logger.info(`ðŸ§­ Docs:   http://${HOST}:${PORT}/api/docs`);
      logger.info(`ðŸŒ± NODE_ENV=${process.env.NODE_ENV || 'development'}`);
    });

    server.on('error', (err: any) => {
      logger.error('HTTP server error:', {
        message: err?.message,
        code: err?.code,
        stack: err?.stack,
      });
    });

    const shutdown = (signal: string) => {
      logger.warn(`Received ${signal}. Shutting down...`);
      server.close(() => {
        logger.info('HTTP server closed');
        process.exit(0);
      });
      setTimeout(() => process.exit(1), 5000).unref();
    };

    process.on('SIGINT', () => shutdown('SIGINT'));
    process.on('SIGTERM', () => shutdown('SIGTERM'));

    process.on('uncaughtException', (err) => {
      logger.error('Uncaught exception:', { message: err.message, stack: err.stack });
    });
    process.on('unhandledRejection', (reason: any) => {
      logger.error('Unhandled rejection:', { reason: reason?.message || String(reason), stack: reason?.stack });
    });
  } catch (err: any) {
    logger.error('Fatal startup error:', { message: err.message, stack: err.stack });
  }
}

start();



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\services\AIService.ts
--------------------------------------------------------------------------------
import { openai } from '@config/openai';
import { logger } from '@utils/logger';
import { CacheService } from './CacheService';

export interface FieldDiscoveryRequest {
  schema: string;
  tableName: string;
  columns: Array<{
    name: string;
    type: string;
    nullable: boolean;
    description?: string;
  }>;
  sampleData?: any[];
  context?: string;
}

export interface FieldDiscoveryResponse {
  fields: Array<{
    name: string;
    type: string;
    classification: 'PII' | 'PHI' | 'Financial' | 'General';
    sensitivity: 'High' | 'Medium' | 'Low';
    description: string;
    suggestedRules: string[];
    dataPatterns: string[];
    businessContext: string;
  }>;
  recommendations: {
    governance: string[];
    quality: string[];
    compliance: string[];
  };
  confidence: number;
  isAiGenerated: boolean; // Track if this came from AI or fallback
}

export interface NaturalLanguageQuery {
  query: string;
  context?: {
    schemas: string[];
    tables: string[];
    fields: string[];
  };
}

export interface QueryResult {
  sql: string;
  explanation: string;
  tables: string[];
  fields: string[];
  confidence: number;
  warnings: string[];
  isAiGenerated: boolean;
}

export class AIService {
  private cacheService: CacheService;

  constructor() {
    this.cacheService = new CacheService();
  }

  public async discoverFields(request: FieldDiscoveryRequest): Promise<FieldDiscoveryResponse> {
    try {
      const cacheKey = `field_discovery:${JSON.stringify(request)}`;
      const cached = await this.cacheService.get(cacheKey);
      
      if (cached) {
        logger.info('Returning cached field discovery result');
        return JSON.parse(cached);
      }

      // Check if OpenAI is available
      if (!openai.isAvailable()) {
        logger.warn('OpenAI not available - using fallback field discovery');
        return this.fallbackFieldDiscovery(request);
      }

      const prompt = this.buildFieldDiscoveryPrompt(request);
      
      const response = await openai.createChatCompletion({
        model: process.env.OPENAI_MODEL || 'gpt-4',
        messages: [
          {
            role: 'system',
            content: `You are an expert data governance analyst specializing in field classification, 
                     data sensitivity analysis, and compliance requirements. Analyze database fields 
                     and provide comprehensive governance recommendations.`
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: parseInt(process.env.OPENAI_MAX_TOKENS || '4000'),
        temperature: parseFloat(process.env.OPENAI_TEMPERATURE || '0.1'),
        response_format: { type: 'json_object' }
      });

      if (!response || !response.choices[0]?.message?.content) {
        logger.warn('No response from OpenAI - using fallback');
        return this.fallbackFieldDiscovery(request);
      }

      const result = JSON.parse(response.choices[0].message.content) as FieldDiscoveryResponse;
      result.isAiGenerated = true;
      
      // Cache the result for 1 hour
      await this.cacheService.set(cacheKey, JSON.stringify(result), 3600);
      
      logger.info('Field discovery completed successfully with AI', {
        schema: request.schema,
        table: request.tableName,
        fieldsAnalyzed: request.columns.length
      });

      return result;

    } catch (error) {
      logger.error('AI field discovery failed, using fallback:', error);
      return this.fallbackFieldDiscovery(request);
    }
  }

  public async processNaturalLanguageQuery(query: NaturalLanguageQuery): Promise<QueryResult> {
    try {
      const cacheKey = `nlq:${JSON.stringify(query)}`;
      const cached = await this.cacheService.get(cacheKey);
      
      if (cached) {
        logger.info('Returning cached natural language query result');
        return JSON.parse(cached);
      }

      // Check if OpenAI is available
      if (!openai.isAvailable()) {
        logger.warn('OpenAI not available - using fallback query processing');
        return this.fallbackNaturalLanguageQuery(query);
      }

      const prompt = this.buildNLQueryPrompt(query);
      
      const response = await openai.createChatCompletion({
        model: process.env.OPENAI_MODEL || 'gpt-4',
        messages: [
          {
            role: 'system',
            content: `You are an expert SQL analyst. Convert natural language queries into 
                     SQL statements with detailed explanations and safety warnings.`
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: parseInt(process.env.OPENAI_MAX_TOKENS || '4000'),
        temperature: parseFloat(process.env.OPENAI_TEMPERATURE || '0.1'),
        response_format: { type: 'json_object' }
      });

      if (!response || !response.choices[0]?.message?.content) {
        logger.warn('No response from OpenAI - using fallback');
        return this.fallbackNaturalLanguageQuery(query);
      }

      const result = JSON.parse(response.choices[0].message.content) as QueryResult;
      result.isAiGenerated = true;
      
      // Cache the result for 30 minutes
      await this.cacheService.set(cacheKey, JSON.stringify(result), 1800);
      
      logger.info('Natural language query processed successfully with AI');
      return result;

    } catch (error) {
      logger.error('AI query processing failed, using fallback:', error);
      return this.fallbackNaturalLanguageQuery(query);
    }
  }

  public async generateQualityRules(fieldInfo: any): Promise<string[]> {
    try {
      if (!openai.isAvailable()) {
        logger.warn('OpenAI not available - using fallback quality rules');
        return this.fallbackQualityRules(fieldInfo);
      }

      const prompt = `Generate data quality rules for the following field:
        Field Name: ${fieldInfo.name}
        Data Type: ${fieldInfo.type}
        Classification: ${fieldInfo.classification}
        Sample Data: ${JSON.stringify(fieldInfo.sampleData || [])}
        
        Return a JSON array of specific, actionable quality rules.`;

      const response = await openai.createChatCompletion({
        model: process.env.OPENAI_MODEL || 'gpt-4',
        messages: [
          {
            role: 'system',
            content: 'You are a data quality expert. Generate specific, measurable quality rules.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 1000,
        temperature: 0.1,
        response_format: { type: 'json_object' }
      });

      if (!response || !response.choices[0]?.message?.content) {
        return this.fallbackQualityRules(fieldInfo);
      }

      const result = JSON.parse(response.choices[0].message.content);
      return result.rules || this.fallbackQualityRules(fieldInfo);

    } catch (error) {
      logger.error('AI quality rule generation failed, using fallback:', error);
      return this.fallbackQualityRules(fieldInfo);
    }
  }

  public async explainViolation(violation: any): Promise<string> {
    try {
      if (!openai.isAvailable()) {
        return this.fallbackViolationExplanation(violation);
      }

      const prompt = `Explain this data quality violation in simple terms:
        Rule: ${violation.rule}
        Field: ${violation.field}
        Value: ${violation.value}
        Error: ${violation.error}
        
        Provide a clear explanation and suggested fix.`;

      const response = await openai.createChatCompletion({
        model: process.env.OPENAI_MODEL || 'gpt-4',
        messages: [
          {
            role: 'system',
            content: 'You are a helpful data analyst explaining quality issues.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 500,
        temperature: 0.3
      });

      if (!response || !response.choices[0]?.message?.content) {
        return this.fallbackViolationExplanation(violation);
      }

      return response.choices[0].message.content;

    } catch (error) {
      logger.error('AI violation explanation failed, using fallback:', error);
      return this.fallbackViolationExplanation(violation);
    }
  }

  // Fallback methods for when AI is not available
  private fallbackFieldDiscovery(request: FieldDiscoveryRequest): FieldDiscoveryResponse {
    const fields = request.columns.map(col => {
      const classification = this.classifyFieldBasic(col.name, col.type);
      const sensitivity = this.determineSensitivityBasic(classification);
      
      return {
        name: col.name,
        type: col.type,
        classification,
        sensitivity,
        description: `${col.type} field containing ${classification.toLowerCase()} data`,
        suggestedRules: this.getBasicQualityRules(col.type, classification),
        dataPatterns: this.detectBasicPatterns(col.name, col.type),
        businessContext: `Data field in ${request.tableName} table`
      };
    });

    return {
      fields,
      recommendations: {
        governance: ['Implement data classification policy', 'Set up access controls'],
        quality: ['Add data validation rules', 'Monitor data quality metrics'],
        compliance: ['Review compliance requirements', 'Implement audit logging']
      },
      confidence: 0.7, // Lower confidence for rule-based classification
      isAiGenerated: false
    };
  }

  private fallbackNaturalLanguageQuery(query: NaturalLanguageQuery): QueryResult {
    // Simple keyword-based SQL generation
    const lowerQuery = query.query.toLowerCase();
    let sql = 'SELECT ';
    
    if (lowerQuery.includes('count') || lowerQuery.includes('how many')) {
      sql += 'COUNT(*) ';
    } else {
      sql += '* ';
    }
    
    sql += 'FROM ';
    
    // Try to detect table names from context or query
    if (query.context?.tables && query.context.tables.length > 0) {
      sql += query.context.tables[0];
    } else if (lowerQuery.includes('user')) {
      sql += 'users';
    } else if (lowerQuery.includes('order')) {
      sql += 'orders';
    } else {
      sql += 'table_name';
    }

    return {
      sql,
      explanation: 'Basic SQL query generated using keyword matching (AI not available)',
      tables: query.context?.tables || ['table_name'],
      fields: query.context?.fields || ['*'],
      confidence: 0.3, // Low confidence for keyword-based generation
      warnings: ['AI service not available - this is a basic keyword-based query'],
      isAiGenerated: false
    };
  }

  private fallbackQualityRules(fieldInfo: any): string[] {
    const rules = [];
    
    if (fieldInfo.type?.includes('varchar') || fieldInfo.type?.includes('text')) {
      rules.push('Validate string length');
      rules.push('Check for null values');
    }
    
    if (fieldInfo.type?.includes('int') || fieldInfo.type?.includes('number')) {
      rules.push('Validate numeric range');
      rules.push('Check for negative values');
    }
    
    if (fieldInfo.name?.toLowerCase().includes('email')) {
      rules.push('Validate email format');
    }
    
    if (fieldInfo.name?.toLowerCase().includes('phone')) {
      rules.push('Validate phone number format');
    }
    
    return rules.length > 0 ? rules : ['Basic data validation required'];
  }

  private fallbackViolationExplanation(violation: any): string {
    return `Data quality issue detected in field "${violation.field}". The rule "${violation.rule}" failed because: ${violation.error}. Please review the data and apply appropriate corrections.`;
  }

  // Helper methods for basic classification
  private classifyFieldBasic(name: string, type: string): 'PII' | 'PHI' | 'Financial' | 'General' {
    const lowerName = name.toLowerCase();
    
    if (lowerName.includes('email') || lowerName.includes('phone') || lowerName.includes('name')) {
      return 'PII';
    }
    if (lowerName.includes('medical') || lowerName.includes('health')) {
      return 'PHI';
    }
    if (lowerName.includes('payment') || lowerName.includes('amount') || lowerName.includes('price')) {
      return 'Financial';
    }
    
    return 'General';
  }

  private determineSensitivityBasic(classification: string): 'High' | 'Medium' | 'Low' {
    switch (classification) {
      case 'PHI': return 'High';
      case 'PII': case 'Financial': return 'Medium';
      default: return 'Low';
    }
  }

  private getBasicQualityRules(type: string, classification: string): string[] {
    const rules = ['Not null validation'];
    
    if (type.includes('varchar')) {
      rules.push('String length validation');
    }
    
    if (classification === 'PII' || classification === 'PHI') {
      rules.push('Data encryption required');
    }
    
    return rules;
  }

  private detectBasicPatterns(name: string, type: string): string[] {
    const patterns = [];
    
    if (name.toLowerCase().includes('email')) {
      patterns.push('Email format');
    }
    if (name.toLowerCase().includes('phone')) {
      patterns.push('Phone number format');
    }
    if (type.includes('date') || type.includes('timestamp')) {
      patterns.push('Date format');
    }
    
    return patterns;
  }

  // Existing helper methods...
  private buildFieldDiscoveryPrompt(request: FieldDiscoveryRequest): string {
    return `Analyze the following database table and provide comprehensive field classification:

Schema: ${request.schema}
Table: ${request.tableName}
Context: ${request.context || 'Not provided'}

Columns:
${request.columns.map(col => 
  `- ${col.name} (${col.type}) ${col.nullable ? 'NULLABLE' : 'NOT NULL'} ${col.description ? '- ' + col.description : ''}`
).join('\n')}

${request.sampleData ? `Sample Data (first 5 rows):
${JSON.stringify(request.sampleData.slice(0, 5), null, 2)}` : ''}

Please provide a JSON response with comprehensive field analysis.`;
  }

  private buildNLQueryPrompt(query: NaturalLanguageQuery): string {
    const contextInfo = query.context ? `
Available Context:
- Schemas: ${query.context.schemas?.join(', ') || 'None'}
- Tables: ${query.context.tables?.join(', ') || 'None'}
- Fields: ${query.context.fields?.join(', ') || 'None'}
` : '';

    return `Convert this natural language query to SQL:
"${query.query}"

${contextInfo}

Please provide a JSON response with SQL and explanation.`;
  }
}


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\services\AnalysisService.ts
--------------------------------------------------------------------------------
// src/services/AnalysisService.ts
import { APIError } from '@/utils/errors';
import { logger } from '@/utils/logger';

// Import concrete classes but DO NOT rely on their exported types
// (your current module doesn't export DataProcessor/DataSample types)
import * as DataProcessorMod from '@/processors/DataProcessor';
import * as SchemaProcessorMod from '@/processors/SchemaProcessor';
import { AIService } from './AIService';

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Minimal contracts (kept small to avoid coupling & missing exports)
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

export type Severity = 'Low' | 'Medium' | 'High' | 'Critical';
export type RuleStatus = 'passed' | 'warning' | 'failed';

export interface DataSample {
  columnName: string;
  values: unknown[];
}

export interface QualityIssue {
  column: string;
  type:
    | 'null_values'
    | 'format_inconsistency'
    | 'out_of_range'
    | 'uniqueness'
    | 'referential'
    | 'custom';
  severity: Severity;
  count: number;
  sample?: unknown[];
  message?: string;
}

export interface IDataProcessor {
  analyzeSampleData(
    samples: DataSample[],
  ): Promise<{ analysis: Record<string, unknown>[]; qualityIssues: QualityIssue[] }>;
}

export interface SchemaInfo {
  name: string;
  // optional extra fields your processor might use
  [k: string]: unknown;
}

export interface ISchemaProcessor {
  processSchema(schema: SchemaInfo): Promise<unknown>;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Result types
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

export interface QualityRuleResult {
  ruleId: string;
  ruleName: string;
  status: RuleStatus;
  testedRecords: number;
  passedRecords: number;
  failedRecords: number;
  successRate: number; // 0..100
  aiExplanation?: string;
  recommendations?: string[];
}

export interface AnalysisSummary {
  totalColumns: number;
  qualityIssues: number;
  highSeverityIssues: number;
}

export interface DataSampleAnalysisResult {
  analysis: ReadonlyArray<Record<string, unknown>>;
  qualityIssues: ReadonlyArray<QualityIssue>;
  recommendations: ReadonlyArray<string>;
  summary: AnalysisSummary;
  analysisMetadata: {
    analyzedAt: string;
    analyzedBy: string;
    version: string;
  };
}

export interface SchemaAIInsights {
  overallAssessment: string;
  riskAreas: string[];
  complianceGaps: string[];
  optimizationOpportunities: string[];
}

export interface SchemaAnalysisResult {
  processed: unknown;
  aiInsights: SchemaAIInsights;
  analysisMetadata: {
    analyzedAt: string;
    analyzedBy: string;
    version: string;
  };
}

export interface QualityCheckResult {
  dataSourceId: string;
  results: ReadonlyArray<QualityRuleResult>;
  summary: {
    totalRules: number;
    passed: number;
    failed: number;
    warnings: number;
  };
  analysisMetadata: {
    analyzedAt: string;
    analyzedBy: string;
    version: string;
  };
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Utils
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const SERVICE_VERSION = '1.0';

function asApiError(err: unknown, fallback: string, status = 500): APIError {
  if (err instanceof APIError) return err;
  const msg = err instanceof Error ? err.message : fallback;
  return new APIError(msg, status, err);
}

async function withTimeout<T>(p: Promise<T>, ms: number, label: string): Promise<T> {
  let t: NodeJS.Timeout | undefined;
  try {
    const timeout = new Promise<never>((_, rej) =>
      (t = setTimeout(() => rej(new APIError(`${label} timed out after ${ms}ms`, 504)), ms)),
    );
    return await Promise.race([p, timeout]);
  } finally {
    if (t) clearTimeout(t);
  }
}

function isHigh(sev: Severity): boolean {
  return sev === 'High' || sev === 'Critical';
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Service
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

export class AnalysisService {
  private readonly schemaProcessor: ISchemaProcessor;
  private readonly dataProcessor: IDataProcessor;
  private readonly ai: AIService;
  private readonly aiTimeoutMs: number;

  constructor(deps?: {
    schemaProcessor?: ISchemaProcessor;
    dataProcessor?: IDataProcessor;
    aiService?: AIService;
    aiTimeoutMs?: number;
  }) {
    // Instantiate from modules if not injected
    const DefaultSchemaProcessor =
    ( SchemaProcessorMod as any).SchemaProcessor ?? (SchemaProcessorMod as any).default;
    const DefaultDataProcessor =
    (DataProcessorMod as any).DataProcessor ?? (DataProcessorMod as any).default;


    this.schemaProcessor = deps?.schemaProcessor ?? new DefaultSchemaProcessor();
    this.dataProcessor = deps?.dataProcessor ?? new DefaultDataProcessor();
    this.ai = deps?.aiService ?? new AIService();
    this.aiTimeoutMs = deps?.aiTimeoutMs ?? Number(process.env.AI_TIMEOUT_MS ?? 8000);
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Schema Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  public async analyzeSchema(schema: SchemaInfo, userId: string): Promise<SchemaAnalysisResult> {
    try {
      this.assertUser(userId);
      this.assertSchema(schema);

      logger.info('AnalysisService: schema analysis start', { userId, schema: schema.name });
      const processed = await this.schemaProcessor.processSchema(schema);

      const aiInsights = await this.safeSchemaInsights(processed);

      const res: SchemaAnalysisResult = {
        processed,
        aiInsights,
        analysisMetadata: {
          analyzedAt: new Date().toISOString(),
          analyzedBy: userId,
          version: SERVICE_VERSION,
        },
      };

      logger.info('AnalysisService: schema analysis complete', { userId, schema: schema.name });
      return res;
    } catch (err) {
      logger.error('AnalysisService: schema analysis error', { error: err });
      throw asApiError(err, 'Schema analysis failed', 500);
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Sample Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  public async analyzeDataSample(samples: DataSample[], userId: string): Promise<DataSampleAnalysisResult> {
    try {
      this.assertUser(userId);
      this.assertSamples(samples);

      logger.info('AnalysisService: data sample analysis start', { userId, samples: samples.length });

      const { analysis, qualityIssues } = await this.dataProcessor.analyzeSampleData(samples);

      const recommendations = await this.buildDataRecommendations(analysis, qualityIssues);

      const res: DataSampleAnalysisResult = {
        analysis,
        qualityIssues,
        recommendations,
        summary: {
          totalColumns: samples.length,
          qualityIssues: qualityIssues.length,
          highSeverityIssues: qualityIssues.filter((q) => isHigh(q.severity)).length,
        },
        analysisMetadata: {
          analyzedAt: new Date().toISOString(),
          analyzedBy: userId,
          version: SERVICE_VERSION,
        },
      };

      logger.info('AnalysisService: data sample analysis complete', {
        userId,
        samplesAnalyzed: samples.length,
        issuesFound: qualityIssues.length,
      });
      return res;
    } catch (err) {
      logger.error('AnalysisService: data sample analysis error', { error: err });
      throw asApiError(err, 'Data sample analysis failed', 500);
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Quality Check (mocked engine) â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  public async performQualityCheck(
    dataSourceId: string,
    rules: ReadonlyArray<Record<string, unknown>> | undefined,
    userId: string,
  ): Promise<QualityCheckResult> {
    try {
      this.assertUser(userId);
      if (!dataSourceId) throw new APIError('dataSourceId is required', 400);

      logger.info('AnalysisService: quality check start', {
        userId,
        dataSourceId,
        rules: rules?.length ?? 0,
      });

      const base = this.mockQualityResults(rules ?? []);
      const enhanced = await this.enhanceRuleResults(base);

      const summary = {
        totalRules: rules?.length ?? 0,
        passed: enhanced.filter((r) => r.status === 'passed').length,
        failed: enhanced.filter((r) => r.status === 'failed').length,
        warnings: enhanced.filter((r) => r.status === 'warning').length,
      };

      const res: QualityCheckResult = {
        dataSourceId,
        results: enhanced,
        summary,
        analysisMetadata: {
          analyzedAt: new Date().toISOString(),
          analyzedBy: userId,
          version: SERVICE_VERSION,
        },
      };

      logger.info('AnalysisService: quality check complete', {
        userId,
        dataSourceId,
        rulesChecked: rules?.length ?? 0,
      });
      return res;
    } catch (err) {
      logger.error('AnalysisService: quality check error', { error: err });
      throw asApiError(err, 'Quality check failed', 500);
    }
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   * Private: AI insights for schema
   * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async safeSchemaInsights(processed: unknown): Promise<SchemaAIInsights> {
    type TableGov = {
      sensitivity?: Severity | string;
      classification?: 'General' | 'PII' | 'PHI' | 'Financial' | string;
      complianceFrameworks?: string[];
      suggestedPolicies?: string[];
    };
    type MinimalProcessed = { name?: string; tables?: { governance?: TableGov }[] };

    const schema = (processed as MinimalProcessed) ?? {};
    const tables: { governance?: TableGov }[] = Array.isArray(schema.tables) ? [...schema.tables] : [];

    // Safety: ensure mutable arrays (avoid readonly assignment issues)
    const snapshot = tables.slice(0, 100).map((t) => ({
      sensitivity: (t.governance?.sensitivity ?? 'Low').toString(),
      classification: (t.governance?.classification ?? 'General').toString(),
      frameworks: Array.isArray(t.governance?.complianceFrameworks)
        ? [...t.governance!.complianceFrameworks!]
        : ([] as string[]),
      policyCount: Array.isArray(t.governance?.suggestedPolicies) ? t.governance!.suggestedPolicies!.length : 0,
    }));

    // If AI service exposes a summarization helper, call it. Otherwise, fall back.
    const hasSummarize =
      this.ai && (typeof (this.ai as any).safeSummarize === 'function' || typeof (this.ai as any).summarize === 'function');

    let overallAssessment = this.fallbackOverallAssessment(tables);
    if (hasSummarize) {
      try {
        const fn = (this.ai as any).safeSummarize ?? (this.ai as any).summarize;
        const out = await withTimeout(
          Promise.resolve(fn.call(this.ai, { topic: 'schema-assessment', payload: { tableCount: tables.length, snapshot } })),
          this.aiTimeoutMs,
          'AI schema assessment',
        );
        if (typeof out === 'string' && out.trim()) overallAssessment = out.trim();
      } catch {
        // keep fallback
      }
    }

    return {
      overallAssessment,
      riskAreas: this.deriveRiskAreas(tables),
      complianceGaps: this.deriveComplianceGaps(tables),
      optimizationOpportunities: this.deriveOptimizations(tables),
    };
  }

  private fallbackOverallAssessment(
    tables: ReadonlyArray<{ governance?: { sensitivity?: string; classification?: string; complianceFrameworks?: string[] } }>,
  ): string {
    const total = tables.length || 1;
    const sensitive = tables.filter((t) => (t.governance?.sensitivity ?? 'Low') !== 'Low').length;
    const frameworks = new Set<string>();
    for (const t of tables) {
      const list = t.governance?.complianceFrameworks ?? [];
      for (let i = 0; i < list.length; i++) frameworks.add(list[i] as string);
    }
    if (sensitive / total > 0.5) {
      return `High-risk schema: ${sensitive}/${total} tables contain non-low sensitivity data. ${frameworks.size} compliance frameworks detected. Prioritize governance.`;
    }
    if (sensitive > 0) {
      return `Moderate-risk schema: ${sensitive} sensitive tables. Frameworks: ${[...frameworks].join(', ') || 'None'}. Maintain standard governance.`;
    }
    return 'Low-risk schema: mostly general data. Basic governance practices are sufficient.';
  }

  private deriveRiskAreas(tables: ReadonlyArray<{ governance?: { sensitivity?: string; classification?: string } }>): string[] {
    const risks: string[] = [];
    const hi = tables.filter(
      (t) => (t.governance?.sensitivity ?? 'Low') === 'High' || (t.governance?.sensitivity ?? 'Low') === 'Critical',
    ).length;
    if (hi) risks.push(`${hi} table(s) contain highly sensitive data (strict access & encryption).`);

    const pii = tables.filter((t) => t.governance?.classification === 'PII').length;
    if (pii) risks.push(`${pii} table(s) contain PII; ensure privacy controls & minimization.`);

    const phi = tables.filter((t) => t.governance?.classification === 'PHI').length;
    if (phi) risks.push(`${phi} table(s) contain PHI; HIPAA safeguards required.`);

    return risks;
  }

  private deriveComplianceGaps(tables: ReadonlyArray<{ governance?: { complianceFrameworks?: string[] } }>): string[] {
    const frameworks = new Set<string>();
    for (const t of tables) {
      const list = t.governance?.complianceFrameworks ?? [];
      for (let i = 0; i < list.length; i++) frameworks.add(list[i] as string);
    }
    const gaps: string[] = [];
    if (frameworks.has('GDPR')) gaps.push('Validate GDPR data subject rights, retention, and DPIA for high-risk processing.');
    if (frameworks.has('HIPAA')) gaps.push('Perform HIPAA security risk analysis and enforce PHI audit controls.');
    if (frameworks.has('PCI-DSS')) gaps.push('Confirm segmentation & encryption for cardholder data.');
    return gaps;
  }

  private deriveOptimizations(
    tables: ReadonlyArray<{ governance?: { suggestedPolicies?: string[]; classification?: string } }>,
  ): string[] {
    const out: string[] = [];
    let noPolicy = 0;
    let general = 0;
    for (let i = 0; i < tables.length; i++) {
      const g = tables[i]?.governance;
      if (!g) continue;
      if (!Array.isArray(g.suggestedPolicies) || g.suggestedPolicies.length === 0) noPolicy++;
      if (g.classification === 'General') general++;
    }
    if (noPolicy) out.push(`${noPolicy} table(s) lack governance policies; define baseline classification and retention.`);
    if (tables.length && general / tables.length > 0.8) {
      out.push('Large share of tables classified as General; consider targeted reclassification review.');
    }
    out.push('Automate periodic discovery and policy drift detection.');
    out.push('Schedule quarterly compliance checks for sensitive domains.');
    return out;
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   * Private: Data recommendations
   * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private async buildDataRecommendations(
    analysis: ReadonlyArray<Record<string, unknown>>,
    issues: ReadonlyArray<QualityIssue>,
  ): Promise<string[]> {
    const recs = new Set<string>();
    const hi = issues.filter((q) => isHigh(q.severity)).length;
    if (hi) recs.add(`Address ${hi} high/critical quality issues immediately.`);

    if (issues.some((q) => q.type === 'null_values')) recs.add('Add NOT NULL/CHECK constraints or upstream validation.');
    if (issues.some((q) => q.type === 'format_inconsistency')) recs.add('Standardize formats with validation/canonicalization.');

    // Optional AI tip if method exists
    const hasSummarize =
      this.ai && (typeof (this.ai as any).safeSummarize === 'function' || typeof (this.ai as any).summarize === 'function');
    if (hasSummarize) {
      try {
        const fn = (this.ai as any).safeSummarize ?? (this.ai as any).summarize;
        const maybe = await withTimeout(
          Promise.resolve(fn.call(this.ai, { topic: 'data-quality-highlights', payload: { issues: issues.slice(0, 50) } })),
          Math.min(this.aiTimeoutMs, 3000),
          'AI data tip',
        );
        if (typeof maybe === 'string' && maybe.trim()) recs.add(maybe.trim());
      } catch {
        // ignore AI failure
      }
    }

    recs.add('Enable continuous monitoring & alerting for key quality dimensions.');
    recs.add('Publish a data quality dashboard with issue backlog & owners.');

    return Array.from(recs);
  }

  private mockQualityResults(_rules: ReadonlyArray<Record<string, unknown>>): QualityRuleResult[] {
    return [
      {
        ruleId: 'email_format',
        ruleName: 'Email Format Validation',
        status: 'passed',
        testedRecords: 1000,
        passedRecords: 998,
        failedRecords: 2,
        successRate: 99.8,
      },
      {
        ruleId: 'phone_format',
        ruleName: 'Phone Number Format',
        status: 'warning',
        testedRecords: 1000,
        passedRecords: 950,
        failedRecords: 50,
        successRate: 95.0,
      },
      {
        ruleId: 'null_check',
        ruleName: 'Required Field Validation',
        status: 'failed',
        testedRecords: 1000,
        passedRecords: 800,
        failedRecords: 200,
        successRate: 80.0,
      },
    ];
  }

  private async enhanceRuleResults(results: ReadonlyArray<QualityRuleResult>): Promise<QualityRuleResult[]> {
    const out: QualityRuleResult[] = [];
    const hasSummarize =
      this.ai && (typeof (this.ai as any).safeSummarize === 'function' || typeof (this.ai as any).summarize === 'function');
    for (let i = 0; i < results.length; i++) {
      const r = results[i];
      const baseExp = this.baseRuleExplanation(r);
      const baseRecs = this.baseRuleRecommendations(r);

      let aiNote: string | undefined;
      if (hasSummarize) {
        try {
          const fn = (this.ai as any).safeSummarize ?? (this.ai as any).summarize;
          const maybe = await withTimeout(
            Promise.resolve(fn.call(this.ai, { topic: 'quality-rule-explanation', payload: { id: r.ruleId, status: r.status, rate: r.successRate } })),
            Math.min(this.aiTimeoutMs, 3000),
            `AI note for ${r.ruleId}`,
          );
          if (typeof maybe === 'string' && maybe.trim()) aiNote = maybe.trim();
        } catch {
          // ignore
        }
      }

      out.push({
        ...r,
        aiExplanation: aiNote ?? baseExp,
        recommendations: Array.from(new Set([...(r.recommendations ?? []), ...baseRecs])),
      });
    }
    return out;
  }

  private baseRuleExplanation(r: QualityRuleResult): string {
    if (r.status === 'passed') return `Rule "${r.ruleName}" performing well (${r.successRate}% success).`;
    if (r.status === 'warning')
      return `Rule "${r.ruleName}" shows moderate issues (${r.failedRecords} failures). Investigate upstream validation.`;
    return `Rule "${r.ruleName}" failing (${r.failedRecords} failures). Immediate remediation required.`;
  }

  private baseRuleRecommendations(r: QualityRuleResult): string[] {
    const recs: string[] = [];
    if (r.status === 'failed') {
      recs.push('Identify root cause and add upstream validation.');
      recs.push('Cleanse/backfill existing bad records.');
      recs.push('Add tests & monitoring to prevent regressions.');
    } else if (r.status === 'warning') {
      recs.push('Track trend; tighten validation where safe.');
      recs.push('Add sampling-based alerts.');
    }
    return recs;
  }

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Guards â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

  private assertUser(userId: string): void {
    if (!userId || typeof userId !== 'string') throw new APIError('Invalid userId', 400);
  }

  private assertSchema(schema: unknown): asserts schema is SchemaInfo {
    if (!schema || typeof (schema as SchemaInfo).name !== 'string') {
      throw new APIError('Invalid schema payload', 400);
    }
  }

  private assertSamples(samples: unknown): asserts samples is DataSample[] {
    if (!Array.isArray(samples)) throw new APIError('samples must be an array', 400);
    if (samples.length === 0) throw new APIError('At least one sample is required', 400);
    for (let i = 0; i < samples.length; i++) {
      const s = samples[i] as Partial<DataSample>;
      if (!s || typeof s.columnName !== 'string' || !Array.isArray(s.values)) {
        throw new APIError(`Invalid sample at index ${i}`, 400);
      }
    }
  }
}



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\services\CacheService.ts
--------------------------------------------------------------------------------
// src/services/CacheService.ts
import { redis } from '@/config/redis';
import { logger } from '@/utils/logger';

function toSeconds(ms: number): number {
  return Math.max(1, Math.floor(ms / 1000));
}

export class CacheService {
  private readonly defaultTTLSeconds: number;

  constructor() {
    const raw = process.env.REDIS_TTL ?? '3600';
    const ttlNum = Number(raw);
    // If someone passed milliseconds (very large), normalize to seconds.
    this.defaultTTLSeconds = ttlNum > 7 * 24 * 3600 ? toSeconds(ttlNum) : (ttlNum || 3600);
  }

  /* ----------------------- Basic primitives ----------------------- */

  public async get(key: string): Promise<string | null> {
    try {
      return await redis.get(key);
    } catch (error) {
      logger.error('Cache GET error', { key, error });
      return null;
    }
  }

  public async getJSON<T = unknown>(key: string): Promise<T | null> {
    const v = await this.get(key);
    if (v == null) return null;
    try {
      return JSON.parse(v) as T;
    } catch {
      return null;
    }
  }

  /**
   * Set a value with TTL (seconds). Accepts string or any JSON-serializable value.
   * Works with both:
   *  - node-redis v4: client.set(key, value, { EX: ttl })
   *  - custom wrappers: redis.set(key, value, ttl)
   */
  public async set(key: string, value: unknown, ttlSeconds?: number): Promise<void> {
    const ttl = ttlSeconds ?? this.defaultTTLSeconds;
    const toStore = typeof value === 'string' ? value : JSON.stringify(value);

    try {
      // Prefer modern API via underlying client (node-redis v4)
      const client: any = redis.getClient?.() ?? null;
      if (client && typeof client.set === 'function') {
        try {
          await client.set(key, toStore, { EX: ttl });
          return;
        } catch {
          // fall through to wrapper style
        }
      }

      // Fallback to wrapper style: redis.set(key, value, ttl)
      const maybe = (redis as any).set;
      if (typeof maybe === 'function') {
        if (maybe.length >= 3) {
          await maybe.call(redis, key, toStore, ttl);
        } else {
          await maybe.call(redis, key, toStore);
          // If wrapper ignores TTL, try to set it via EXPIRE
          if (client && typeof client.expire === 'function') {
            await client.expire(key, ttl);
          }
        }
      }
    } catch (error) {
      logger.error('Cache SET error', { key, error });
      // don't throw â€” cache is best effort
    }
  }

  public async del(key: string): Promise<void> {
    try {
      await redis.del(key);
    } catch (error) {
      logger.error('Cache DEL error', { key, error });
    }
  }

  /**
   * Normalize EXISTS return to boolean whether itâ€™s number or boolean.
   */
  public async exists(key: string): Promise<boolean> {
    try {
      const raw = await (redis as any).exists?.(key);
      const n = typeof raw === 'number' ? raw : raw ? 1 : 0;
      return n > 0;
    } catch (error) {
      logger.error('Cache EXISTS error', { key, error });
      return false;
    }
  }

  /* ----------------------- Bulk helpers ----------------------- */

  /**
   * Flush keys by pattern safely. Uses scanIterator (node-redis v4),
   * falls back to SCAN or KEYS if needed.
   */
  public async flushPattern(pattern: string): Promise<void> {
    try {
      const client: any = redis.getClient?.() ?? null;
      if (!client) return;

      const batch: string[] = [];
      const flushBatch = async () => {
        if (batch.length) {
          await client.del(batch.splice(0, batch.length));
        }
      };

      if (typeof client.scanIterator === 'function') {
        // Best path: async iterator with MATCH/COUNT
        for await (const key of client.scanIterator({ MATCH: pattern, COUNT: 1000 })) {
          batch.push(key as string);
          if (batch.length >= 1000) await flushBatch();
        }
        await flushBatch();
        return;
      }

      if (typeof client.scan === 'function') {
        // Fallback: manual SCAN loop
        let cursor = '0';
        do {
          const [next, keys]: [string, string[]] = await client.scan(cursor, {
            MATCH: pattern,
            COUNT: 1000,
          });
          cursor = next;
          if (keys?.length) {
            batch.push(...keys);
            if (batch.length >= 1000) await flushBatch();
          }
        } while (cursor !== '0');
        await flushBatch();
        return;
      }

      if (typeof client.keys === 'function') {
        // Last resort: KEYS (blocking)
        const keys: string[] = await client.keys(pattern);
        if (keys.length) await client.del(keys);
      }
    } catch (error) {
      logger.error('Cache FLUSH PATTERN error', { pattern, error });
    }
  }

  /* ----------------------- Stats & diagnostics ----------------------- */

  public async getStats(): Promise<Record<string, string | number>> {
    try {
      const client: any = redis.getClient?.() ?? null;
      if (!client || typeof client.info !== 'function') return {};
      const info: string = await client.info('stats');
      return this.parseRedisInfo(info);
    } catch (error) {
      logger.error('Cache STATS error', { error });
      return {};
    }
  }

  private parseRedisInfo(info: string): Record<string, string | number> {
    const stats: Record<string, string | number> = {};
    const lines = info.split(/\r?\n/);

    for (const line of lines) {
      if (!line || line.startsWith('#')) continue; // skip comments / blanks
      const idx = line.indexOf(':');
      if (idx <= 0) continue;

      const key = line.slice(0, idx).trim();
      const raw = line.slice(idx + 1).trim();

      const n = Number(raw);
      stats[key] = Number.isFinite(n) && raw !== '' ? n : raw;
    }
    return stats;
    }
}

// Optional singleton export
export const cacheService = new CacheService();



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\services\DefaultDataProcessor.ts
--------------------------------------------------------------------------------
// src/services/DefaultDataProcessor.ts
import type { DataSample, IDataProcessor, QualityIssue } from '@/services/AnalysisService';

export class DefaultDataProcessor implements IDataProcessor {
  constructor(private readonly log: (msg: string, meta?: unknown) => void = () => {}) {}

  async analyzeSampleData(
    samples: DataSample[],
  ): Promise<{ analysis: Record<string, unknown>[]; qualityIssues: QualityIssue[] }> {
    this.log('DefaultDataProcessor.analyzeSampleData', { columns: samples.length });

    const analysis: Record<string, unknown>[] = [];
    const qualityIssues: QualityIssue[] = [];

    for (const s of samples) {
      const values = Array.isArray(s.values) ? s.values : [];
      const nonNull = values.filter(v => v !== null && v !== undefined);
      const nulls = values.length - nonNull.length;
      const uniqueCount = new Set(nonNull.map(v => JSON.stringify(v))).size;

      analysis.push({
        column: s.columnName,
        count: values.length,
        nulls,
        nullRate: values.length ? +(nulls / values.length * 100).toFixed(2) : 0,
        uniqueCount,
        sample: nonNull.slice(0, 5),
      });

      if (nulls > 0) {
        qualityIssues.push({
          column: s.columnName,
          type: 'null_values',
          severity: nulls / Math.max(values.length, 1) > 0.2 ? 'High' : 'Low',
          count: nulls,
          message: 'Column contains null/undefined values',
          sample: values.slice(0, 5),
        });
      }

      const lower = s.columnName.toLowerCase();
      if (lower.includes('email')) {
        const bad = nonNull.filter(v => typeof v === 'string' && !/^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(v));
        if (bad.length) {
          qualityIssues.push({
            column: s.columnName,
            type: 'format_inconsistency',
            severity: bad.length / Math.max(nonNull.length, 1) > 0.05 ? 'Medium' : 'Low',
            count: bad.length,
            message: 'Invalid email addresses detected',
            sample: bad.slice(0, 5),
          });
        }
      }
    }

    return { analysis, qualityIssues };
  }
}

export default DefaultDataProcessor;



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\services\DiscoveryService.ts
--------------------------------------------------------------------------------
// src/services/DiscoveryService.ts
import { db } from '@/config/database';
import { APIError } from '@/utils/errors';
import { logger } from '@/utils/logger';
import { v4 as uuidv4 } from 'uuid';
import { AIService, type FieldDiscoveryRequest } from './AIService';

export interface DiscoverySession {
  sessionId: string;
  userId: string;
  dataSourceId: string;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  progress: number;
  results?: unknown;
  error?: string;
  createdAt: Date;
  updatedAt: Date;
}

export interface StartDiscoveryRequest {
  userId: string;
  dataSourceId: string;
  schemas?: string[];
  tables?: string[];
  options?: {
    sampleSize?: number;
    includeData?: boolean;
    analysisDepth?: 'basic' | 'detailed' | 'comprehensive';
  };
}

// shape as returned from DB
type DbSessionRow = {
  session_id: string;
  user_id: string;
  data_source_id: string;
  status: DiscoverySession['status'];
  progress: number;
  results: unknown | null;
  error: string | null;
  created_at: Date;
  updated_at: Date;
};

// local helper type for metadata
type ColumnDef = { name: string; type: string; nullable: boolean; description?: string };
type TableMeta = { schema: string; name: string; columns: ColumnDef[] };

export class DiscoveryService {
  private readonly aiService = new AIService();

  public async startDiscovery(request: StartDiscoveryRequest): Promise<DiscoverySession> {
    try {
      const sessionId = uuidv4();

      // Create discovery session
      const session = await this.createSession({
        sessionId,
        userId: request.userId,
        dataSourceId: request.dataSourceId,
        status: 'pending',
        progress: 0,
      });

      // Kick off async processing
      setImmediate(() =>
        this.processDiscovery(session, request).catch((err) => {
          const msg = err instanceof Error ? err.message : String(err);
          logger.error('Discovery background task fatal error', { sessionId, error: msg });
        })
      );

      logger.info('Discovery session started', { sessionId, dataSourceId: request.dataSourceId });
      return session;
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : 'Unknown error';
      logger.error('Failed to start discovery', { error: msg });
      throw new APIError('Failed to start discovery', 500, err);
    }
  }

  public async getSession(sessionId: string): Promise<DiscoverySession | null> {
    try {
      const result = await db.query('SELECT * FROM discovery_sessions WHERE session_id = $1', [sessionId]);
      if (result.rows.length === 0) return null;
      return this.mapSessionFromDb(result.rows[0] as DbSessionRow);
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : 'Unknown error';
      logger.error('Failed to get discovery session', { sessionId, error: msg });
      throw new APIError('Failed to get session', 500, err);
    }
  }

  public async listSessions(userId: string, limit = 20, offset = 0): Promise<DiscoverySession[]> {
    try {
      const result = await db.query(
        `SELECT * FROM discovery_sessions 
         WHERE user_id = $1 
         ORDER BY created_at DESC 
         LIMIT $2 OFFSET $3`,
        [userId, limit, offset]
      );

      return (result.rows as DbSessionRow[]).map((row) => this.mapSessionFromDb(row));
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : 'Unknown error';
      logger.error('Failed to list discovery sessions', { userId, error: msg });
      throw new APIError('Failed to list sessions', 500, err);
    }
  }

  public async deleteSession(sessionId: string, userId: string): Promise<void> {
    try {
      const result = await db.query('DELETE FROM discovery_sessions WHERE session_id = $1 AND user_id = $2', [
        sessionId,
        userId,
      ]);

      if (result.rowCount === 0) {
        throw new APIError('Session not found', 404);
      }

      logger.info('Discovery session deleted', { sessionId });
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : 'Unknown error';
      logger.error('Failed to delete discovery session', { sessionId, error: msg });
      throw err instanceof APIError ? err : new APIError('Failed to delete session', 500, err);
    }
  }

  /* --------------------------- Internals --------------------------- */

  private async createSession(session: Partial<DiscoverySession>): Promise<DiscoverySession> {
    const result = await db.query(
      `INSERT INTO discovery_sessions 
       (session_id, user_id, data_source_id, status, progress, created_at, updated_at)
       VALUES ($1, $2, $3, $4, $5, NOW(), NOW())
       RETURNING *`,
      [session.sessionId, session.userId, session.dataSourceId, session.status, session.progress]
    );
    return this.mapSessionFromDb(result.rows[0] as DbSessionRow);
  }

  private async updateSession(sessionId: string, updates: Partial<DiscoverySession>): Promise<void> {
    const setClause: string[] = [];
    const values: unknown[] = [];
    let i = 1;

    for (const [key, value] of Object.entries(updates)) {
      if (key === 'sessionId') continue;
      // convert camelCase to snake_case
      const dbKey = key.replace(/[A-Z]/g, (m) => `_${m.toLowerCase()}`);
      setClause.push(`${dbKey} = $${i++}`);
      values.push(value);
    }

    if (setClause.length === 0) return;

    setClause.push(`updated_at = NOW()`);
    values.push(sessionId);

    await db.query(`UPDATE discovery_sessions SET ${setClause.join(', ')} WHERE session_id = $${i}`, values);
  }

  private async processDiscovery(session: DiscoverySession, request: StartDiscoveryRequest): Promise<void> {
    try {
      await this.updateSession(session.sessionId, { status: 'processing', progress: 10 });

      const dataSource = await this.getDataSource(request.dataSourceId);
      if (!dataSource) throw new Error('Data source not found');

      await this.updateSession(session.sessionId, { progress: 20 });

      // Retrieve metadata
      const metadata = await this.getSchemaMetadata(dataSource, request.schemas, request.tables);
      await this.updateSession(session.sessionId, { progress: 40 });

      const results: Array<{ schema: string; table: string; analysis: unknown }> = [];
      const totalTables = metadata.length || 1;

      for (let idx = 0; idx < metadata.length; idx++) {
        const table = metadata[idx];

        // Optional sample data
        let sampleData: unknown[] = [];
        if (request.options?.includeData) {
          sampleData = await this.getSampleData(
            dataSource,
            table.schema,
            table.name,
            request.options.sampleSize ?? 100
          );
        }

        // IMPORTANT: make columns mutable (clone), not readonly
        const columns: ColumnDef[] = table.columns.map((c) => ({ ...c }));

        const discoveryRequest: FieldDiscoveryRequest = {
          schema: table.schema,
          tableName: table.name,
          columns,            // mutable array now
          sampleData,         // always an array
          context: `Discovery session for ${String(dataSource.name ?? dataSource.id)}`,
        };

        const analysis = await this.aiService.discoverFields(discoveryRequest);

        results.push({ schema: table.schema, table: table.name, analysis });

        // progress: 40 â†’ 90 during table loop
        const progress = 40 + Math.floor(((idx + 1) / totalTables) * 50);
        await this.updateSession(session.sessionId, { progress });
      }

      await this.updateSession(session.sessionId, {
        status: 'completed',
        progress: 100,
        results: { tables: results, summary: this.generateSummary(results) },
      });

      logger.info('Discovery completed successfully', {
        sessionId: session.sessionId,
        tablesProcessed: results.length,
      });
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : 'Unknown error';
      logger.error('Discovery processing failed', { sessionId: session.sessionId, error: msg });

      await this.updateSession(session.sessionId, { status: 'failed', error: msg });
    }
  }

  private async getDataSource(dataSourceId: string): Promise<any | null> {
    const result = await db.query('SELECT * FROM data_sources WHERE id = $1', [dataSourceId]);
    return result.rows[0] ?? null;
  }

  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  private async getSchemaMetadata(
    _dataSource: any,
    _schemas?: string[],
    _tables?: string[]
  ): Promise<TableMeta[]> {
    // TODO: replace with real connector fetch
    return [
      {
        schema: 'public',
        name: 'users',
        columns: [
          { name: 'id', type: 'integer', nullable: false },
          { name: 'email', type: 'varchar', nullable: false },
          { name: 'first_name', type: 'varchar', nullable: true },
          { name: 'last_name', type: 'varchar', nullable: true },
          { name: 'phone', type: 'varchar', nullable: true },
          { name: 'created_at', type: 'timestamp', nullable: false },
        ],
      },
    ];
  }

  private async getSampleData(
    _dataSource: any,
    _schema: string,
    _table: string,
    _limit: number
  ): Promise<unknown[]> {
    // TODO: replace with actual sampling query
    return [
      { id: 1, email: 'john@example.com', first_name: 'John', last_name: 'Doe', phone: '+1234567890' },
      { id: 2, email: 'jane@example.com', first_name: 'Jane', last_name: 'Smith', phone: '+1987654321' },
    ];
  }

  private generateSummary(results: ReadonlyArray<{ analysis: any }>): {
    totalTables: number;
    totalFields: number;
    classifications: Record<string, number>;
    sensitivities: Record<string, number>;
    recommendations: string[];
  } {
    let totalFields = 0;
    const classifications: Record<string, number> = Object.create(null);
    const sensitivities: Record<string, number> = Object.create(null);

    for (const t of results) {
      const fields: ReadonlyArray<any> = Array.isArray(t.analysis?.fields) ? t.analysis.fields : [];
      totalFields += fields.length;

      for (const f of fields) {
        const cls = String(f.classification ?? 'Unknown');
        const sen = String(f.sensitivity ?? 'Unknown');
        classifications[cls] = (classifications[cls] ?? 0) + 1;
        sensitivities[sen] = (sensitivities[sen] ?? 0) + 1;
      }
    }

    const recommendations = results
      .flatMap((t) => {
        const gov: unknown = t.analysis?.recommendations?.governance;
        return Array.isArray(gov) ? gov : [];
      })
      .slice(0, 10);

    return {
      totalTables: results.length,
      totalFields,
      classifications,
      sensitivities,
      recommendations,
    };
  }

  private mapSessionFromDb(row: DbSessionRow): DiscoverySession {
    // Build with conditional spreads to satisfy exactOptionalPropertyTypes:
    const base: Omit<DiscoverySession, 'results' | 'error'> = {
      sessionId: row.session_id,
      userId: row.user_id,
      dataSourceId: row.data_source_id,
      status: row.status,
      progress: row.progress,
      createdAt: row.created_at,
      updatedAt: row.updated_at,
    };

    return {
      ...base,
      ...(row.results !== null ? { results: row.results } : {}),
      ...(row.error !== null ? { error: row.error } : {}),
    };
  }
}



--------------------------------------------------------------------------------
FILE: backend\ai-service\src\utils\errors.ts
--------------------------------------------------------------------------------
export class APIError extends Error {
  public statusCode: number;
  public isOperational: boolean;
  public details?: any;

  constructor(message: string, statusCode = 500, details?: any) {
    super(message);
    this.name = 'APIError';
    this.statusCode = statusCode;
    this.isOperational = true;
    this.details = details;

    // Ensure proper prototype chain
    Object.setPrototypeOf(this, APIError.prototype);

    // Capture stack trace
    Error.captureStackTrace(this, this.constructor);
  }
}

export class ValidationError extends APIError {
  constructor(message: string, details?: any) {
    super(message, 400, details);
    this.name = 'ValidationError';
  }
}

export class AuthenticationError extends APIError {
  constructor(message = 'Authentication required') {
    super(message, 401);
    this.name = 'AuthenticationError';
  }
}

export class AuthorizationError extends APIError {
  constructor(message = 'Insufficient permissions') {
    super(message, 403);
    this.name = 'AuthorizationError';
  }
}

export class NotFoundError extends APIError {
  constructor(resource = 'Resource') {
    super(`${resource} not found`, 404);
    this.name = 'NotFoundError';
  }
}

export class ConflictError extends APIError {
  constructor(message = 'Resource conflict') {
    super(message, 409);
    this.name = 'ConflictError';
  }
}

export class RateLimitError extends APIError {
  constructor(message = 'Rate limit exceeded') {
    super(message, 429);
    this.name = 'RateLimitError';
  }
}

export class ExternalServiceError extends APIError {
  constructor(service: string, message?: string) {
    super(message || `External service ${service} is unavailable`, 503);
    this.name = 'ExternalServiceError';
  }
}


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\utils\gracefulShutdown.ts
--------------------------------------------------------------------------------
// src/utils/gracefulShutdown.ts
import { Server } from 'http';
import { logger } from './logger';

interface ShutdownOptions {
  timeout?: number; // Timeout in milliseconds
  signals?: string[]; // Signals to listen for
  forceExitDelay?: number; // Delay before force exit
}

/**
 * Production-ready graceful shutdown handler
 */
export class GracefulShutdown {
  private server: Server;
  private isShuttingDown: boolean = false;
  private connections: Set<any> = new Set();
  private options: Required<ShutdownOptions>;
  private shutdownTimeout: NodeJS.Timeout | null = null;
  private forceExitTimeout: NodeJS.Timeout | null = null;

  constructor(server: Server, options: ShutdownOptions = {}) {
    this.server = server;
    this.options = {
      timeout: options.timeout || 30000, // 30 seconds default
      signals: options.signals || ['SIGTERM', 'SIGINT', 'SIGUSR2'],
      forceExitDelay: options.forceExitDelay || 5000 // 5 seconds before force exit
    };

    this.setupConnectionTracking();
    this.setupSignalHandlers();
  }

  /**
   * Track active connections
   */
  private setupConnectionTracking(): void {
    this.server.on('connection', (connection) => {
      this.connections.add(connection);
      
      connection.on('close', () => {
        this.connections.delete(connection);
      });

      // Handle connection errors
      connection.on('error', (error) => {
        logger.warn('Connection error during shutdown:', error);
        this.connections.delete(connection);
      });
    });
  }

  /**
   * Setup signal handlers for graceful shutdown
   */
  private setupSignalHandlers(): void {
    this.options.signals.forEach((signal) => {
      process.on(signal, () => {
        logger.info(`Received ${signal} signal, starting graceful shutdown...`);
        this.initiateShutdown(signal);
      });
    });

    // Handle uncaught exceptions during shutdown
    process.on('uncaughtException', (error) => {
      logger.error('Uncaught exception during shutdown:', error);
      if (this.isShuttingDown) {
        this.forceExit(1);
      }
    });

    // Handle unhandled promise rejections during shutdown
    process.on('unhandledRejection', (reason, promise) => {
      logger.error('Unhandled promise rejection during shutdown:', { reason, promise });
      if (this.isShuttingDown) {
        this.forceExit(1);
      }
    });
  }

  /**
   * Initiate graceful shutdown process
   */
  private async initiateShutdown(signal: string): Promise<void> {
    if (this.isShuttingDown) {
      logger.warn('Shutdown already in progress, ignoring signal');
      return;
    }

    this.isShuttingDown = true;
    const startTime = Date.now();

    logger.info('Starting graceful shutdown process...', {
      signal,
      activeConnections: this.connections.size,
      timeout: this.options.timeout
    });

    // Set timeout for forced shutdown
    this.shutdownTimeout = setTimeout(() => {
      logger.warn('Graceful shutdown timeout reached, forcing shutdown');
      this.forceShutdown();
    }, this.options.timeout);

    // Set force exit timeout
    this.forceExitTimeout = setTimeout(() => {
      logger.error('Force exit timeout reached, terminating process');
      this.forceExit(1);
    }, this.options.timeout + this.options.forceExitDelay);

    try {
      // Step 1: Stop accepting new connections
      await this.stopAcceptingConnections();

      // Step 2: Close existing connections gracefully
      await this.closeExistingConnections();

      // Step 3: Cleanup resources
      await this.cleanupResources();

      // Step 4: Exit gracefully
      const duration = Date.now() - startTime;
      logger.info(`Graceful shutdown completed successfully in ${duration}ms`);
      
      this.clearTimeouts();
      process.exit(0);

    } catch (error) {
      logger.error('Error during graceful shutdown:', error);
      this.forceExit(1);
    }
  }

  /**
   * Stop accepting new connections
   */
  private async stopAcceptingConnections(): Promise<void> {
    return new Promise((resolve, reject) => {
      if (!this.server.listening) {
        logger.info('Server not listening, skipping connection stop');
        return resolve();
      }

      logger.info('Stopping server from accepting new connections...');
      
      this.server.close((error) => {
        if (error) {
          logger.error('Error stopping server:', error);
          return reject(error);
        }
        
        logger.info('Server stopped accepting new connections');
        resolve();
      });
    });
  }

  /**
   * Close existing connections gracefully
   */
  private async closeExistingConnections(): Promise<void> {
    if (this.connections.size === 0) {
      logger.info('No active connections to close');
      return;
    }

    logger.info(`Closing ${this.connections.size} active connections...`);

    // Give connections time to finish naturally
    await this.waitForConnectionsToClose(5000);

    // Force close remaining connections
    if (this.connections.size > 0) {
      logger.warn(`Force closing ${this.connections.size} remaining connections`);
      this.connections.forEach((connection) => {
        try {
          connection.destroy();
        } catch (error) {
          logger.warn('Error destroying connection:', error);
        }
      });
      this.connections.clear();
    }

    logger.info('All connections closed');
  }

  /**
   * Wait for connections to close naturally
   */
  private async waitForConnectionsToClose(timeout: number): Promise<void> {
    const startTime = Date.now();
    
    while (this.connections.size > 0 && (Date.now() - startTime) < timeout) {
      await new Promise(resolve => setTimeout(resolve, 100));
    }
  }

  /**
   * Cleanup application resources
   */
  private async cleanupResources(): Promise<void> {
    logger.info('Cleaning up application resources...');

    try {
      // Import and close database connections
      const { db } = await import('@/config/database');
      if (db && typeof db.close === 'function') {
        await db.close();
        logger.info('Database connections closed');
      }
    } catch (error) {
      logger.warn('Error closing database connections:', error);
    }

    try {
      // Import and close Redis connections
      const { redis } = await import('@/config/redis');
      if (redis && typeof redis.close === 'function') {
        await redis.close();
        logger.info('Redis connections closed');
      }
    } catch (error) {
      logger.warn('Error closing Redis connections:', error);
    }

    // Clear any intervals or timeouts
    this.clearApplicationTimers();

    logger.info('Resource cleanup completed');
  }

  /**
   * Clear application timers and intervals
   */
  private clearApplicationTimers(): void {
    // Clear any global intervals or timeouts
    // This is a placeholder - you can add specific timer cleanup here
    logger.debug('Application timers cleared');
  }

  /**
   * Force shutdown when graceful shutdown fails
   */
  private forceShutdown(): void {
    logger.warn('Forcing immediate shutdown...');

    // Destroy all connections immediately
    this.connections.forEach((connection) => {
      try {
        connection.destroy();
      } catch (error) {
        logger.warn('Error destroying connection during force shutdown:', error);
      }
    });

    this.connections.clear();
    
    // Force close server
    try {
      this.server.close();
    } catch (error) {
      logger.warn('Error force closing server:', error);
    }

    this.clearTimeouts();
    this.forceExit(1);
  }

  /**
   * Force exit the process
   */
  private forceExit(code: number): void {
    logger.error(`Force exiting with code ${code}`);
    this.clearTimeouts();
    process.exit(code);
  }

  /**
   * Clear shutdown timeouts
   */
  private clearTimeouts(): void {
    if (this.shutdownTimeout) {
      clearTimeout(this.shutdownTimeout);
      this.shutdownTimeout = null;
    }

    if (this.forceExitTimeout) {
      clearTimeout(this.forceExitTimeout);
      this.forceExitTimeout = null;
    }
  }

  /**
   * Get shutdown status
   */
  public isShutdownInProgress(): boolean {
    return this.isShuttingDown;
  }

  /**
   * Get active connections count
   */
  public getActiveConnectionsCount(): number {
    return this.connections.size;
  }
}

/**
 * Simple graceful shutdown function (backwards compatible)
 */
export function gracefulShutdown(server: Server, options?: ShutdownOptions): GracefulShutdown {
  return new GracefulShutdown(server, options);
}

/**
 * Enhanced graceful shutdown with custom cleanup
 */
export function createGracefulShutdown(
  server: Server, 
  options: ShutdownOptions = {},
  customCleanup?: () => Promise<void>
): GracefulShutdown {
  const shutdown = new GracefulShutdown(server, options);

  // Add custom cleanup if provided
  if (customCleanup) {
    const originalCleanup = (shutdown as any).cleanupResources;
    (shutdown as any).cleanupResources = async function() {
      await originalCleanup.call(this);
      try {
        await customCleanup();
        logger.info('Custom cleanup completed');
      } catch (error) {
        logger.error('Error in custom cleanup:', error);
      }
    };
  }

  return shutdown;
}

// Export default
export default gracefulShutdown;


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\utils\logger.ts
--------------------------------------------------------------------------------
import winston from 'winston';
import DailyRotateFile from 'winston-daily-rotate-file';

const logFormat = winston.format.combine(
  winston.format.timestamp(),
  winston.format.errors({ stack: true }),
  winston.format.json()
);

const consoleFormat = winston.format.combine(
  winston.format.colorize(),
  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
  winston.format.printf(({ timestamp, level, message, ...meta }) => {
    return `${timestamp} [${level}]: ${message} ${Object.keys(meta).length ? JSON.stringify(meta, null, 2) : ''}`;
  })
);

// Create logger instance
export const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: logFormat,
  defaultMeta: { 
    service: 'ai-service',
    version: process.env.APP_VERSION || '1.0.0'
  },
  transports: [
    // Console transport
    new winston.transports.Console({
      format: process.env.NODE_ENV === 'production' ? logFormat : consoleFormat
    }),

    // File transport for errors
    new DailyRotateFile({
      filename: 'logs/error-%DATE%.log',
      datePattern: 'YYYY-MM-DD',
      level: 'error',
      maxSize: process.env.LOG_FILE_MAX_SIZE || '20m',
      maxFiles: process.env.LOG_FILE_MAX_FILES || '14d',
      zippedArchive: true
    }),

    // File transport for all logs
    new DailyRotateFile({
      filename: 'logs/combined-%DATE%.log',
      datePattern: 'YYYY-MM-DD',
      maxSize: process.env.LOG_FILE_MAX_SIZE || '20m',
      maxFiles: process.env.LOG_FILE_MAX_FILES || '14d',
      zippedArchive: true
    })
  ],

  // Handle exceptions and rejections
  exceptionHandlers: [
    new winston.transports.File({ filename: 'logs/exceptions.log' })
  ],
  rejectionHandlers: [
    new winston.transports.File({ filename: 'logs/rejections.log' })
  ]
});

// Create child logger for specific modules
export const createChildLogger = (module: string) => {
  return logger.child({ module });
};


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\utils\responses.ts
--------------------------------------------------------------------------------
export interface ApiResponse<T = any> {
  success: boolean;
  data?: T;
  message?: string;
  pagination?: {
    page: number;
    limit: number;
    total: number;
    totalPages: number;
  };
  meta?: {
    requestId?: string;
    timestamp: string;
    version: string;
  };
}

export const successResponse = <T>(
  data: T,
  message = 'Success',
  pagination?: any
): ApiResponse<T> => {
  return {
    success: true,
    data,
    message,
    ...(pagination && { pagination }),
    meta: {
      timestamp: new Date().toISOString(),
      version: process.env.APP_VERSION || '1.0.0'
    }
  };
};

export const errorResponse = (
  message: string,
  code = 500,
  details?: any
): ApiResponse => {
  return {
    success: false,
    message,
    ...(details && { data: details }),
    meta: {
      timestamp: new Date().toISOString(),
      version: process.env.APP_VERSION || '1.0.0'
    }
  };
};

export const paginatedResponse = <T>(
  data: T[],
  page: number,
  limit: number,
  total: number,
  message = 'Success'
): ApiResponse<T[]> => {
  return {
    success: true,
    data,
    message,
    pagination: {
      page,
      limit,
      total,
      totalPages: Math.ceil(total / limit)
    },
    meta: {
      timestamp: new Date().toISOString(),
      version: process.env.APP_VERSION || '1.0.0'
    }
  };
};


--------------------------------------------------------------------------------
FILE: backend\ai-service\src\utils\validator.ts
--------------------------------------------------------------------------------
// src/utils/validator.ts
import type { RequestHandler } from 'express';
import Joi, { Schema, ValidationOptions } from 'joi';

/**
 * Common primitives
 */
const uuid = Joi.string().guid({ version: ['uuidv4', 'uuidv5'] });

const email = Joi.string()
  .email({ tlds: { allow: false } })
  .max(254);

const password = Joi.string()
  .min(8)
  // at least one lowercase, one uppercase, one digit, one special
  .pattern(/^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[ !"#$%&'()*+,\-./:;<=>?@[\\\]^_`{|}~]).{8,}$/)
  .messages({
    'string.pattern.base':
      'Password must include upper & lower case letters, a number, and a special character.',
  });

/**
 * E.164-ish phone: optional leading +, then 7â€“15 digits (first digit cannot be 0)
 * If you want strictly E.164 (1â€“15 digits, no leading 0), use /^\+[1-9]\d{1,14}$/
 */
const phone = Joi.string()
  .pattern(/^\+?[1-9]\d{6,14}$/)
  .messages({
    'string.pattern.base': 'Phone must be a valid international number (e.g., +15551234567).',
  });

const nonEmptyString = Joi.string().trim().min(1);

const dateISO = Joi.date().iso();

/**
 * Reusable â€œfieldâ€ library
 */
export const Fields = {
  uuid,
  email,
  password,
  phone,
  nonEmptyString,
  dateISO,
  // add as needed:
  // intId: Joi.number().integer().positive(),
  // url: Joi.string().uri({ scheme: [/https?/] }),
};

/**
 * Example composed schemas you can import directly
 */
export const Schemas = {
  // Auth
  register: Joi.object({
    email: Fields.email.required(),
    password: Fields.password.required(),
    fullName: Fields.nonEmptyString.required(),
    phone: Fields.phone.optional(),
  }),

  login: Joi.object({
    email: Fields.email.required(),
    password: Joi.string().required(),
  }),

  // Users
  userCreate: Joi.object({
    email: Fields.email.required(),
    password: Fields.password.required(),
    firstName: Fields.nonEmptyString.required(),
    lastName: Fields.nonEmptyString.required(),
    phone: Fields.phone.optional(),
  }),

  userUpdate: Joi.object({
    email: Fields.email.optional(),
    firstName: Fields.nonEmptyString.optional(),
    lastName: Fields.nonEmptyString.optional(),
    phone: Fields.phone.optional(),
  }).min(1), // at least one field

  // Generic id param
  idParam: Joi.object({ id: Fields.uuid.required() }),

  // Pagination / filtering example
  paginationQuery: Joi.object({
    page: Joi.number().integer().min(1).default(1),
    pageSize: Joi.number().integer().min(1).max(200).default(25),
    search: Joi.string().trim().max(200).optional(),
    sortBy: Joi.string().trim().max(64).optional(),
    sortDir: Joi.string().valid('asc', 'desc').default('asc'),
  }),
} as const;

/**
 * Central validator helper (useful outside of Express too)
 */
export function validate<T>(
  value: unknown,
  schema: Schema,
  options: ValidationOptions = { abortEarly: false, stripUnknown: true }
): T {
  const { error, value: out } = schema.validate(value, options);
  if (error) {
    // Throw a structured error (you may map to your APIError)
    const details = error.details.map(d => d.message);
    throw new Error(`Validation failed: ${details.join('; ')}`);
  }
  return out as T;
}

/**
 * Express middlewares for validation
 */
type MiddlewareBuilder = (schema: Schema, options?: ValidationOptions) => RequestHandler;

export const validateBody: MiddlewareBuilder = (schema, options = { abortEarly: false, stripUnknown: true }) =>
  (req, _res, next) => {
    try {
      req.body = validate(req.body, schema, options);
      next();
    } catch (e) {
      next(e);
    }
  };

export const validateQuery: MiddlewareBuilder = (schema, options = { abortEarly: false, stripUnknown: true }) =>
  (req, _res, next) => {
    try {
      req.query = validate(req.query, schema, options);
      next();
    } catch (e) {
      next(e);
    }
  };

export const validateParams: MiddlewareBuilder = (schema, options = { abortEarly: false, stripUnknown: true }) =>
  (req, _res, next) => {
    try {
      req.params = validate(req.params, schema, options);
      next();
    } catch (e) {
      next(e);
    }
  };



--------------------------------------------------------------------------------
FILE: backend\ai-service\tests\analysis.test.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\tests\discovery.test.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\tests\health.test.ts
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\ai-service\tsconfig.json
--------------------------------------------------------------------------------
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "lib": ["ES2020"],
    "outDir": "./dist",
    "rootDir": "./src",

    "strict": true,
    "strictNullChecks": true,
    "noImplicitAny": true,
    "noImplicitReturns": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noImplicitThis": true,
    "alwaysStrict": true,
    "noImplicitOverride": true,

    "moduleResolution": "node",
    "baseUrl": ".",
    "paths": {
      "@/*": ["src/*"],
      "@config/*": ["src/config/*"],
      "@controllers/*": ["src/controllers/*"],
      "@services/*": ["src/services/*"],
      "@models/*": ["src/models/*"],
      "@middleware/*": ["src/middleware/*"],
      "@utils/*": ["src/utils/*"],
      "@types/*": ["src/types/*"]
    },

    "skipLibCheck": true,
    "exactOptionalPropertyTypes": false,

    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "resolveJsonModule": true,

    "declaration": true,
    "declarationMap": true,
    "sourceMap": true,
    "removeComments": true,

    "experimentalDecorators": true,
    "emitDecoratorMetadata": true
  },
  "ts-node": { "require": ["tsconfig-paths/register"] },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist", "tests"]
}



========================================================================================================================
  BACKEND SERVICE: PIPELINE-SERVICE
========================================================================================================================


--------------------------------------------------------------------------------
FILE: backend\pipeline-service\.env
--------------------------------------------------------------------------------
SERVICE_NAME=pipeline-service
PORT=3004
CORS_ORIGIN=http://localhost:5173



--------------------------------------------------------------------------------
FILE: backend\pipeline-service\Dockerfile
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\pipeline-service\Dockerfile.dev
--------------------------------------------------------------------------------
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3000
CMD [ "npm", "run", "dev" ]



--------------------------------------------------------------------------------
FILE: backend\pipeline-service\package.json
--------------------------------------------------------------------------------
{
    "name":  "cwic-pipeline-service",
    "version":  "1.0.0",
    "type":  "module",
    "main":  "dist/server.js",
    "scripts":  {
                    "dev":  "nodemon --watch src --ext ts --exec \"npx tsx src/server.ts\"",
                    "build":  "tsc",
                    "start":  "node dist/server.js",
                    "typecheck":  "tsc --noEmit"
                },
    "engines":  {
                    "node":  "\u003e=18"
                },
    "dependencies":  {
                         "cors":  "^2.8.5",
                         "dotenv":  "^16.4.5",
                         "express":  "^4.19.2",
                         "helmet":  "^7.1.0",
                         "morgan":  "^1.10.0"
                     },
    "devDependencies":  {
                            "@types/express":  "^4.17.21",
                            "@types/node":  "^20.12.7",
                            "nodemon":  "^3.1.0",
                            "ts-node":  "^10.9.2",
                            "typescript":  "^5.4.5"
                        }
}



--------------------------------------------------------------------------------
FILE: backend\pipeline-service\src\app.ts
--------------------------------------------------------------------------------
import cors from 'cors';
import 'dotenv/config';
import express from 'express';
import helmet from 'helmet';
import morgan from 'morgan';

export const app = express();

const corsOrigin = process.env.CORS_ORIGIN || 'http://localhost:5173';
app.use(helmet());
app.use(cors({ origin: corsOrigin }));
app.use(express.json());
app.use(morgan('dev'));

app.get('/health', (_req, res) => {
  res.json({ service: process.env.SERVICE_NAME || 'pipeline-service', status: 'ok' });
});

app.get('/', (_req, res) => {
  res.json({ service: process.env.SERVICE_NAME || 'pipeline-service', message: 'Service up and running' });
});



--------------------------------------------------------------------------------
FILE: backend\pipeline-service\src\server.ts
--------------------------------------------------------------------------------
import { app } from './app.js';

const fallback = Number(process.env.FALLBACK_PORT || 3004);
const port = Number(process.env.PORT || fallback);

app.listen(port, () => {
  console.log(`[${process.env.SERVICE_NAME || 'pipeline-service'}] listening on :${port}`);
});



--------------------------------------------------------------------------------
FILE: backend\pipeline-service\tsconfig.json
--------------------------------------------------------------------------------
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "esModuleInterop": true,
    "strict": true,
    "resolveJsonModule": true,
    "outDir": "dist",
    "skipLibCheck": true
  },
  "include": ["src"]
}



========================================================================================================================
  BACKEND SERVICE: NOTIFICATION-SERVICE
========================================================================================================================


--------------------------------------------------------------------------------
FILE: backend\notification-service\.env
--------------------------------------------------------------------------------
SERVICE_NAME=notification-service
PORT=3007
CORS_ORIGIN=http://localhost:5173



--------------------------------------------------------------------------------
FILE: backend\notification-service\Dockerfile
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\notification-service\Dockerfile.dev
--------------------------------------------------------------------------------
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3000
CMD [ "npm", "run", "dev" ]



--------------------------------------------------------------------------------
FILE: backend\notification-service\package.json
--------------------------------------------------------------------------------
{
    "name":  "cwic-notification-service",
    "version":  "1.0.0",
    "type":  "module",
    "main":  "dist/server.js",
    "scripts":  {
                    "dev":  "nodemon --watch src --ext ts --exec \"npx tsx src/server.ts\"",
                    "build":  "tsc",
                    "start":  "node dist/server.js",
                    "typecheck":  "tsc --noEmit"
                },
    "engines":  {
                    "node":  "\u003e=18"
                },
    "dependencies":  {
                         "cors":  "^2.8.5",
                         "dotenv":  "^16.4.5",
                         "express":  "^4.19.2",
                         "helmet":  "^7.1.0",
                         "morgan":  "^1.10.0"
                     },
    "devDependencies":  {
                            "@types/express":  "^4.17.21",
                            "@types/node":  "^20.12.7",
                            "nodemon":  "^3.1.0",
                            "ts-node":  "^10.9.2",
                            "typescript":  "^5.4.5"
                        }
}



--------------------------------------------------------------------------------
FILE: backend\notification-service\src\app.ts
--------------------------------------------------------------------------------
import cors from 'cors';
import 'dotenv/config';
import express from 'express';
import helmet from 'helmet';
import morgan from 'morgan';

export const app = express();

const corsOrigin = process.env.CORS_ORIGIN || 'http://localhost:5173';
app.use(helmet());
app.use(cors({ origin: corsOrigin }));
app.use(express.json());
app.use(morgan('dev'));

app.get('/health', (_req, res) => {
  res.json({ service: process.env.SERVICE_NAME || 'notification-service', status: 'ok' });
});

app.get('/', (_req, res) => {
  res.json({ service: process.env.SERVICE_NAME || 'notification-service', message: 'Service up and running' });
});



--------------------------------------------------------------------------------
FILE: backend\notification-service\src\server.ts
--------------------------------------------------------------------------------
import { app } from './app.js';

const fallback = Number(process.env.FALLBACK_PORT || 3007);
const port = Number(process.env.PORT || fallback);

app.listen(port, () => {
  console.log(`[${process.env.SERVICE_NAME || 'notification-service'}] listening on :${port}`);
});



--------------------------------------------------------------------------------
FILE: backend\notification-service\tsconfig.json
--------------------------------------------------------------------------------
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "esModuleInterop": true,
    "strict": true,
    "resolveJsonModule": true,
    "outDir": "dist",
    "skipLibCheck": true
  },
  "include": ["src"]
}



========================================================================================================================
  BACKEND SERVICE: INTEGRATION-SERVICE
========================================================================================================================


--------------------------------------------------------------------------------
FILE: backend\integration-service\.env
--------------------------------------------------------------------------------
SERVICE_NAME=integration-service
PORT=3006
CORS_ORIGIN=http://localhost:5173



--------------------------------------------------------------------------------
FILE: backend\integration-service\Dockerfile
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: backend\integration-service\Dockerfile.dev
--------------------------------------------------------------------------------
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3000
CMD [ "npm", "run", "dev" ]



--------------------------------------------------------------------------------
FILE: backend\integration-service\package.json
--------------------------------------------------------------------------------
{
    "name":  "cwic-integration-service",
    "version":  "1.0.0",
    "type":  "module",
    "main":  "dist/server.js",
    "scripts":  {
                    "dev":  "nodemon --watch src --ext ts --exec \"npx tsx src/server.ts\"",
                    "build":  "tsc",
                    "start":  "node dist/server.js",
                    "typecheck":  "tsc --noEmit"
                },
    "engines":  {
                    "node":  "\u003e=18"
                },
    "dependencies":  {
                         "cors":  "^2.8.5",
                         "dotenv":  "^16.4.5",
                         "express":  "^4.19.2",
                         "helmet":  "^7.1.0",
                         "morgan":  "^1.10.0"
                     },
    "devDependencies":  {
                            "@types/express":  "^4.17.21",
                            "@types/node":  "^20.12.7",
                            "nodemon":  "^3.1.0",
                            "ts-node":  "^10.9.2",
                            "typescript":  "^5.4.5"
                        }
}



--------------------------------------------------------------------------------
FILE: backend\integration-service\src\app.ts
--------------------------------------------------------------------------------
import cors from 'cors';
import 'dotenv/config';
import express from 'express';
import helmet from 'helmet';
import morgan from 'morgan';

export const app = express();

const corsOrigin = process.env.CORS_ORIGIN || 'http://localhost:5173';
app.use(helmet());
app.use(cors({ origin: corsOrigin }));
app.use(express.json());
app.use(morgan('dev'));

app.get('/health', (_req, res) => {
  res.json({ service: process.env.SERVICE_NAME || 'integration-service', status: 'ok' });
});

app.get('/', (_req, res) => {
  res.json({ service: process.env.SERVICE_NAME || 'integration-service', message: 'Service up and running' });
});



--------------------------------------------------------------------------------
FILE: backend\integration-service\src\server.ts
--------------------------------------------------------------------------------
import { app } from './app.js';

const fallback = Number(process.env.FALLBACK_PORT || 3006);
const port = Number(process.env.PORT || fallback);

app.listen(port, () => {
  console.log(`[${process.env.SERVICE_NAME || 'integration-service'}] listening on :${port}`);
});



--------------------------------------------------------------------------------
FILE: backend\integration-service\tsconfig.json
--------------------------------------------------------------------------------
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "esModuleInterop": true,
    "strict": true,
    "resolveJsonModule": true,
    "outDir": "dist",
    "skipLibCheck": true
  },
  "include": ["src"]
}



========================================================================================================================
  DATABASE SCRIPTS
========================================================================================================================


--------------------------------------------------------------------------------
FILE: database\init\02-auth.sql
--------------------------------------------------------------------------------
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS users (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  email CITEXT UNIQUE NOT NULL,
  password_hash TEXT NOT NULL,
  display_name TEXT NOT NULL,
  roles TEXT[] NOT NULL DEFAULT ARRAY['user'],
  is_verified BOOLEAN NOT NULL DEFAULT false,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE UNIQUE INDEX IF NOT EXISTS idx_users_email ON users (email);



--------------------------------------------------------------------------------
FILE: database\migrations\001_initial_schema.sql
--------------------------------------------------------------------------------
-- -- initial schema



--------------------------------------------------------------------------------
FILE: database\migrations\002_add_data_sources.sql
--------------------------------------------------------------------------------
-- -- add data sources



--------------------------------------------------------------------------------
FILE: database\migrations\003_add_quality_rules.sql
--------------------------------------------------------------------------------
-- -- add quality rules



--------------------------------------------------------------------------------
FILE: database\migrations\004_add_pipelines.sql
--------------------------------------------------------------------------------
-- -- add pipelines



--------------------------------------------------------------------------------
FILE: database\schemas\audit.sql
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: database\schemas\auth.sql
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: database\schemas\data_catalog.sql
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: database\schemas\pipelines.sql
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: database\schemas\quality.sql
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: database\seeds\data_sources.sql
--------------------------------------------------------------------------------
-- -- seed data sources



--------------------------------------------------------------------------------
FILE: database\seeds\pipelines.sql
--------------------------------------------------------------------------------
# Empty file


--------------------------------------------------------------------------------
FILE: database\seeds\quality_rules.sql
--------------------------------------------------------------------------------
-- -- seed quality rules



--------------------------------------------------------------------------------
FILE: database\seeds\users.sql
--------------------------------------------------------------------------------
-- -- seed users



========================================================================================================================
  BACKEND INFRASTRUCTURE
========================================================================================================================


--------------------------------------------------------------------------------
FILE: infrastructure\docker-compose\docker-compose.dev.yml
--------------------------------------------------------------------------------

version: '3.8'

services:
  # Frontend Development Server
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000
    depends_on:
      - api-gateway

  # API Gateway
  api-gateway:
    build:
      context: ./backend/api-gateway
      dockerfile: Dockerfile.dev
    ports:
      - "8000:8000"
    volumes:
      - ./backend/api-gateway:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - PORT=8000
      - AUTH_SERVICE_URL=http://auth-service:3001
      - DATA_SERVICE_URL=http://data-service:3002
      - AI_SERVICE_URL=http://ai-service:3003
      - PIPELINE_SERVICE_URL=http://pipeline-service:3004
    depends_on:
      - auth-service
      - data-service
      - ai-service
      - pipeline-service

  # Authentication Service
  auth-service:
    build:
      context: ./backend/auth-service
      dockerfile: Dockerfile.dev
    ports:
      - "3001:3001"
    volumes:
      - ./backend/auth-service:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - PORT=3001
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/cwic_auth
      - JWT_SECRET=your-super-secret-jwt-key
      - JWT_EXPIRES_IN=24h
    depends_on:
      - postgres
      - redis

  # Data Service
  data-service:
    build:
      context: ./backend/data-service
      dockerfile: Dockerfile.dev
    ports:
      - "3002:3002"
    volumes:
      - ./backend/data-service:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - PORT=3002
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/cwic_data
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis

  # AI Service
  ai-service:
    build:
      context: ./backend/ai-service
      dockerfile: Dockerfile.dev
    ports:
      - "3003:3003"
    volumes:
      - ./backend/ai-service:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - PORT=3003
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
    depends_on:
      - redis

  # Pipeline Service
  pipeline-service:
    build:
      context: ./backend/pipeline-service
      dockerfile: Dockerfile.dev
    ports:
      - "3004:3004"
    volumes:
      - ./backend/pipeline-service:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - PORT=3004
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/cwic_pipelines
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - AZURE_DEVOPS_TOKEN=${AZURE_DEVOPS_TOKEN}
    depends_on:
      - postgres

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_MULTIPLE_DATABASES=cwic_auth,cwic_data,cwic_pipelines,cwic_notifications
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/scripts/create-multiple-databases.sh:/docker-entrypoint-initdb.d/create-multiple-databases.sh
      - ./database/migrations:/docker-entrypoint-initdb.d/migrations

  # Redis Cache
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  # MinIO for Object Storage (S3 Compatible)
  minio:
    image: minio/minio:latest
    restart: unless-stopped
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin123
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"

  # Elasticsearch for Search
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0
    restart: unless-stopped
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  # Kafka for Event Streaming
  kafka:
    image: confluentinc/cp-kafka:latest
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    depends_on:
      - zookeeper

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000

volumes:
  postgres_data:
  redis_data:
  minio_data:
  elasticsearch_data:


--------------------------------------------------------------------------------
FILE: infrastructure\docker-compose\docker-compose.override.yml
--------------------------------------------------------------------------------




--------------------------------------------------------------------------------
FILE: infrastructure\docker-compose\docker-compose.prod.yml
--------------------------------------------------------------------------------




--------------------------------------------------------------------------------
FILE: infrastructure\docker-compose\docker-compose.staging.yml
--------------------------------------------------------------------------------




--------------------------------------------------------------------------------
FILE: infrastructure\helm-charts\cwic-platform\Chart.yaml
--------------------------------------------------------------------------------
apiVersion: v2
name: cwic-platform
version: 0.1.0
type: application



--------------------------------------------------------------------------------
FILE: infrastructure\helm-charts\cwic-platform\values.yaml
--------------------------------------------------------------------------------
# values placeholder



========================================================================================================================
  BACKEND DIRECTORY STRUCTURE
========================================================================================================================

BACKEND SERVICES STRUCTURE:
  [DIR] backend\ai-service
  [DIR] backend\ai-service\src
  [DIR] backend\ai-service\src\config
  [DIR] backend\ai-service\src\controllers
  [DIR] backend\ai-service\src\interfaces
  [DIR] backend\ai-service\src\jobs
  [DIR] backend\ai-service\src\middleware
  [DIR] backend\ai-service\src\models
  [DIR] backend\ai-service\src\processors
  [DIR] backend\ai-service\src\prompts
  [DIR] backend\ai-service\src\routes
  [DIR] backend\ai-service\src\scripts
  [DIR] backend\ai-service\src\services
  [DIR] backend\ai-service\src\utils
  [DIR] backend\ai-service\tests
  [DIR] backend\api-gateway
  [DIR] backend\api-gateway\src
  [DIR] backend\api-gateway\src\config
  [DIR] backend\api-gateway\src\controllers
  [DIR] backend\api-gateway\src\middleware
  [DIR] backend\api-gateway\src\models
  [DIR] backend\api-gateway\src\proxy
  [DIR] backend\api-gateway\src\routes
  [DIR] backend\api-gateway\src\services
  [DIR] backend\api-gateway\src\utils
  [DIR] backend\api-gateway\tests
  [DIR] backend\auth-service
  [DIR] backend\auth-service\src
  [DIR] backend\auth-service\src\config
  [DIR] backend\auth-service\src\controllers
  [DIR] backend\auth-service\src\middleware
  [DIR] backend\auth-service\src\models
  [DIR] backend\auth-service\src\routes
  [DIR] backend\auth-service\src\services
  [DIR] backend\auth-service\src\utils
  [DIR] backend\auth-service\tests
  [DIR] backend\data-service
  [DIR] backend\data-service\migrations
  [DIR] backend\data-service\src
  [DIR] backend\data-service\src\config
  [DIR] backend\data-service\src\controllers
  [DIR] backend\data-service\src\db
  [DIR] backend\data-service\src\middleware
  [DIR] backend\data-service\src\models
  [DIR] backend\data-service\src\repositories
  [DIR] backend\data-service\src\routes
  [DIR] backend\data-service\src\services
  [DIR] backend\data-service\src\services\connectors
  [DIR] backend\data-service\src\utils
  [DIR] backend\data-service\tests
  [DIR] backend\integration-service
  [DIR] backend\integration-service\src
  [DIR] backend\integration-service\src\adapters
  [DIR] backend\integration-service\src\adapters\azure-devops
  [DIR] backend\integration-service\src\adapters\git
  [DIR] backend\integration-service\src\adapters\jira
  [DIR] backend\integration-service\src\adapters\servicenow
  [DIR] backend\integration-service\src\config
  [DIR] backend\integration-service\src\controllers
  [DIR] backend\integration-service\src\middleware
  [DIR] backend\integration-service\src\models
  [DIR] backend\integration-service\src\routes
  [DIR] backend\integration-service\src\services
  [DIR] backend\integration-service\src\utils
  [DIR] backend\integration-service\tests
  [DIR] backend\notification-service
  [DIR] backend\notification-service\src
  [DIR] backend\notification-service\src\config
  [DIR] backend\notification-service\src\controllers
  [DIR] backend\notification-service\src\middleware
  [DIR] backend\notification-service\src\models
  [DIR] backend\notification-service\src\routes
  [DIR] backend\notification-service\src\services
  [DIR] backend\notification-service\src\utils
  [DIR] backend\notification-service\tests
  [DIR] backend\pipeline-service
  [DIR] backend\pipeline-service\src
  [DIR] backend\pipeline-service\src\config
  [DIR] backend\pipeline-service\src\controllers
  [DIR] backend\pipeline-service\src\middleware
  [DIR] backend\pipeline-service\src\models
  [DIR] backend\pipeline-service\src\routes
  [DIR] backend\pipeline-service\src\services
  [DIR] backend\pipeline-service\src\utils
  [DIR] backend\pipeline-service\tests
  [DIR] backend\shared
  [DIR] backend\shared\config
  [DIR] backend\shared\constants
  [DIR] backend\shared\middleware
  [DIR] backend\shared\types
  [DIR] backend\shared\utils

BACKEND FILE TYPES INCLUDED:
  - *.ts
  - *.js
  - *.json
  - *.env*
  - *.md
  - *.txt
  - Dockerfile*
  - *.yml
  - *.yaml

EXCLUDED DIRECTORIES:
  - node_modules
  - .git
  - dist
  - build
  - coverage
  - .nyc_output
  - logs
  - tmp
  - temp
