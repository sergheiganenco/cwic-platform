-- Migration 011: Enhanced Quality System with 6 Dimensions
-- Based on industry best practices from Monte Carlo, Soda, Great Expectations

-- ============================================================================
-- 1. DROP OLD TABLES (if they exist)
-- ============================================================================
DROP TABLE IF EXISTS quality_scan_schedules CASCADE;
DROP TABLE IF EXISTS quality_issues CASCADE;
DROP TABLE IF EXISTS data_profiles CASCADE;
DROP TABLE IF EXISTS quality_results CASCADE;
DROP TABLE IF EXISTS quality_rules CASCADE;

-- ============================================================================
-- 2. QUALITY RULES (Enhanced with 6 Dimensions)
-- ============================================================================
CREATE TABLE quality_rules (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  description TEXT,

  -- Core classification
  dimension VARCHAR(50) NOT NULL CHECK (dimension IN (
    'completeness',   -- null rates, missing values
    'accuracy',       -- outliers, range validation
    'consistency',    -- referential integrity
    'validity',       -- format, type, regex
    'freshness',      -- data recency
    'uniqueness'      -- duplicates, PK validation
  )),
  severity VARCHAR(20) NOT NULL DEFAULT 'medium' CHECK (severity IN ('low', 'medium', 'high', 'critical')),

  -- Scope
  asset_id UUID REFERENCES catalog_assets(id) ON DELETE CASCADE,
  data_source_id UUID REFERENCES data_sources(id) ON DELETE CASCADE,
  column_name VARCHAR(255),  -- NULL means table-level rule

  -- Rule definition
  rule_type VARCHAR(50) NOT NULL CHECK (rule_type IN (
    'threshold',      -- metric > threshold
    'sql',            -- custom SQL query
    'ai_anomaly',     -- ML-based detection
    'pattern',        -- regex/pattern matching
    'comparison',     -- cross-table comparison
    'freshness_check' -- time-based checks
  )),
  rule_config JSONB NOT NULL,  -- rule-specific configuration

  -- Thresholds (for threshold-based rules)
  threshold_config JSONB,  -- { "operator": ">=", "value": 95, "unit": "percent" }

  -- Automation
  enabled BOOLEAN DEFAULT true,
  auto_generated BOOLEAN DEFAULT false,  -- generated by auto-profiling
  ml_model_id VARCHAR(255),  -- for AI-based rules
  schedule_cron VARCHAR(100),  -- cron expression for scheduled runs

  -- Metadata
  created_by VARCHAR(255),
  created_at TIMESTAMP DEFAULT now(),
  updated_at TIMESTAMP DEFAULT now(),
  last_run_at TIMESTAMP,

  -- Performance
  avg_execution_time_ms INTEGER,
  run_count INTEGER DEFAULT 0,

  -- Tags for organization
  tags TEXT[]
);

CREATE INDEX idx_quality_rules_asset ON quality_rules(asset_id) WHERE enabled = true;
CREATE INDEX idx_quality_rules_datasource ON quality_rules(data_source_id) WHERE enabled = true;
CREATE INDEX idx_quality_rules_dimension ON quality_rules(dimension);
CREATE INDEX idx_quality_rules_enabled ON quality_rules(enabled);

-- ============================================================================
-- 3. DATA PROFILES (Auto-profiling results)
-- ============================================================================
CREATE TABLE data_profiles (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  asset_id UUID NOT NULL REFERENCES catalog_assets(id) ON DELETE CASCADE,
  data_source_id UUID REFERENCES data_sources(id),

  -- Basic stats
  profile_date TIMESTAMP DEFAULT now(),
  row_count BIGINT,
  column_count INTEGER,
  size_bytes BIGINT,

  -- Column-level profiles
  profile_data JSONB NOT NULL,
  /* Example structure:
  {
    "columns": [
      {
        "name": "user_id",
        "dataType": "integer",
        "nullRate": 0.02,
        "uniqueRate": 0.98,
        "min": 1,
        "max": 50000,
        "avg": 25000,
        "stdDev": 14433,
        "topValues": [{"value": 100, "count": 50}],
        "anomalies": ["high_null_rate_spike"],
        "suggestedRules": ["uniqueness_check", "not_null_check"]
      }
    ],
    "relationships": [
      {"column": "user_id", "references": "users.id", "orphanRate": 0.001}
    ]
  }
  */

  -- Computed quality score (0-100)
  quality_score DECIMAL(5,2),

  -- Dimension scores
  completeness_score DECIMAL(5,2),
  accuracy_score DECIMAL(5,2),
  consistency_score DECIMAL(5,2),
  validity_score DECIMAL(5,2),
  freshness_score DECIMAL(5,2),
  uniqueness_score DECIMAL(5,2),

  created_at TIMESTAMP DEFAULT now()
);

CREATE INDEX idx_data_profiles_asset ON data_profiles(asset_id);
CREATE INDEX idx_data_profiles_date ON data_profiles(profile_date DESC);
CREATE INDEX idx_data_profiles_score ON data_profiles(quality_score);

-- ============================================================================
-- 4. QUALITY RESULTS (Execution results)
-- ============================================================================
CREATE TABLE quality_results (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  rule_id UUID NOT NULL REFERENCES quality_rules(id) ON DELETE CASCADE,
  asset_id UUID REFERENCES catalog_assets(id) ON DELETE SET NULL,
  data_source_id UUID REFERENCES data_sources(id) ON DELETE SET NULL,

  -- Execution info
  run_at TIMESTAMP DEFAULT now(),
  status VARCHAR(20) NOT NULL CHECK (status IN ('passed', 'failed', 'warning', 'error', 'timeout', 'skipped')),

  -- Metrics
  metric_value DECIMAL(15,4),  -- actual measured value
  threshold_value DECIMAL(15,4),  -- expected threshold
  rows_checked BIGINT,
  rows_failed BIGINT,
  execution_time_ms INTEGER,

  -- Error handling
  error_message TEXT,
  error_stack TEXT,

  -- Sample failures (for debugging)
  sample_failures JSONB,
  /* Example:
  [
    {"row_id": "abc-123", "column": "email", "value": "invalid", "reason": "Invalid email format"},
    ...
  ]
  */

  -- AI-based anomaly score (0-1, higher = more anomalous)
  anomaly_score DECIMAL(5,4),

  -- Context
  context JSONB,  -- additional execution context

  created_at TIMESTAMP DEFAULT now()
);

CREATE INDEX idx_quality_results_rule ON quality_results(rule_id);
CREATE INDEX idx_quality_results_asset ON quality_results(asset_id);
CREATE INDEX idx_quality_results_datasource ON quality_results(data_source_id);
CREATE INDEX idx_quality_results_run_at ON quality_results(run_at DESC);
CREATE INDEX idx_quality_results_status ON quality_results(status);

-- ============================================================================
-- 5. QUALITY ISSUES (Violations with tracking)
-- ============================================================================
CREATE TABLE quality_issues (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  result_id UUID REFERENCES quality_results(id) ON DELETE CASCADE,
  rule_id UUID NOT NULL REFERENCES quality_rules(id) ON DELETE CASCADE,
  asset_id UUID REFERENCES catalog_assets(id) ON DELETE SET NULL,
  data_source_id UUID REFERENCES data_sources(id) ON DELETE SET NULL,

  -- Classification
  severity VARCHAR(20) NOT NULL CHECK (severity IN ('low', 'medium', 'high', 'critical')),
  dimension VARCHAR(50) NOT NULL,

  -- Status tracking
  status VARCHAR(50) DEFAULT 'open' CHECK (status IN (
    'open',
    'acknowledged',
    'in_progress',
    'resolved',
    'false_positive',
    'wont_fix'
  )),

  -- Description
  title VARCHAR(500) NOT NULL,
  description TEXT,

  -- Impact assessment
  impact_score INTEGER CHECK (impact_score >= 0 AND impact_score <= 100),
  affected_rows BIGINT,
  affected_columns TEXT[],

  -- Debugging data
  sample_data JSONB,  -- sample rows showing the issue

  -- AI-generated insights
  root_cause TEXT,  -- AI-generated explanation
  remediation_plan TEXT,  -- AI-suggested fix
  similar_issues UUID[],  -- related issue IDs

  -- Workflow
  assigned_to VARCHAR(255),
  resolved_at TIMESTAMP,
  resolution_notes TEXT,

  -- Timestamps
  first_seen_at TIMESTAMP DEFAULT now(),
  last_seen_at TIMESTAMP DEFAULT now(),
  created_at TIMESTAMP DEFAULT now(),
  updated_at TIMESTAMP DEFAULT now(),

  -- Recurrence tracking
  occurrence_count INTEGER DEFAULT 1
);

CREATE INDEX idx_quality_issues_rule ON quality_issues(rule_id);
CREATE INDEX idx_quality_issues_asset ON quality_issues(asset_id);
CREATE INDEX idx_quality_issues_status ON quality_issues(status);
CREATE INDEX idx_quality_issues_severity ON quality_issues(severity);
CREATE INDEX idx_quality_issues_first_seen ON quality_issues(first_seen_at DESC);

-- ============================================================================
-- 6. SCAN SCHEDULES (Automated scanning)
-- ============================================================================
CREATE TABLE quality_scan_schedules (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  description TEXT,

  -- Scope
  data_source_id UUID REFERENCES data_sources(id) ON DELETE CASCADE,
  asset_ids UUID[],  -- specific assets, NULL = all
  rule_ids UUID[],   -- specific rules, NULL = all enabled rules

  -- Schedule
  cron_expression VARCHAR(100) NOT NULL,
  timezone VARCHAR(50) DEFAULT 'UTC',
  enabled BOOLEAN DEFAULT true,

  -- Tracking
  last_run_at TIMESTAMP,
  next_run_at TIMESTAMP,
  last_status VARCHAR(50),
  last_duration_ms INTEGER,

  -- Configuration
  config JSONB,  -- { "timeout": 300000, "parallelism": 5 }

  -- Notifications
  notify_on_failure BOOLEAN DEFAULT true,
  notification_channels JSONB,  -- { "email": ["user@example.com"], "slack": ["#alerts"] }

  created_by VARCHAR(255),
  created_at TIMESTAMP DEFAULT now(),
  updated_at TIMESTAMP DEFAULT now()
);

CREATE INDEX idx_scan_schedules_datasource ON quality_scan_schedules(data_source_id);
CREATE INDEX idx_scan_schedules_enabled ON quality_scan_schedules(enabled);
CREATE INDEX idx_scan_schedules_next_run ON quality_scan_schedules(next_run_at) WHERE enabled = true;

-- ============================================================================
-- 7. VIEWS FOR REPORTING
-- ============================================================================

-- Quality summary by asset
CREATE OR REPLACE VIEW v_asset_quality_summary AS
SELECT
  a.id AS asset_id,
  a.name AS asset_name,
  a.type AS asset_type,
  a.data_source_id,
  COUNT(DISTINCT qr.id) AS rule_count,
  COUNT(DISTINCT CASE WHEN qr.enabled THEN qr.id END) AS active_rule_count,
  COUNT(DISTINCT res.id) FILTER (WHERE res.run_at >= now() - interval '7 days') AS recent_checks,
  COUNT(DISTINCT res.id) FILTER (WHERE res.status = 'passed' AND res.run_at >= now() - interval '7 days') AS passed_checks,
  COUNT(DISTINCT res.id) FILTER (WHERE res.status = 'failed' AND res.run_at >= now() - interval '7 days') AS failed_checks,
  ROUND(
    CASE
      WHEN COUNT(DISTINCT res.id) FILTER (WHERE res.run_at >= now() - interval '7 days') > 0
      THEN (COUNT(DISTINCT res.id) FILTER (WHERE res.status = 'passed' AND res.run_at >= now() - interval '7 days')::DECIMAL /
            COUNT(DISTINCT res.id) FILTER (WHERE res.run_at >= now() - interval '7 days')) * 100
      ELSE NULL
    END, 2
  ) AS pass_rate,
  COUNT(DISTINCT qi.id) FILTER (WHERE qi.status = 'open') AS open_issues,
  MAX(res.run_at) AS last_check_at,
  p.quality_score AS latest_quality_score
FROM catalog_assets a
LEFT JOIN quality_rules qr ON qr.asset_id = a.id
LEFT JOIN quality_results res ON res.rule_id = qr.id
LEFT JOIN quality_issues qi ON qi.asset_id = a.id
LEFT JOIN LATERAL (
  SELECT quality_score
  FROM data_profiles
  WHERE asset_id = a.id
  ORDER BY profile_date DESC
  LIMIT 1
) p ON TRUE
GROUP BY a.id, a.name, a.type, a.data_source_id, p.quality_score;

-- Quality trends
CREATE OR REPLACE VIEW v_quality_trends AS
SELECT
  DATE_TRUNC('hour', run_at) AS time_bucket,
  data_source_id,
  COUNT(*) AS total_checks,
  COUNT(*) FILTER (WHERE status = 'passed') AS passed,
  COUNT(*) FILTER (WHERE status = 'failed') AS failed,
  COUNT(*) FILTER (WHERE status = 'error') AS errors,
  ROUND(AVG(execution_time_ms), 2) AS avg_execution_ms,
  ROUND(AVG(CASE WHEN status = 'passed' THEN 100 ELSE 0 END), 2) AS pass_rate
FROM quality_results
WHERE run_at >= now() - interval '30 days'
GROUP BY time_bucket, data_source_id
ORDER BY time_bucket DESC;

-- ============================================================================
-- 8. FUNCTIONS FOR QUALITY SCORING
-- ============================================================================

-- Calculate quality score for an asset
CREATE OR REPLACE FUNCTION calculate_asset_quality_score(p_asset_id UUID)
RETURNS DECIMAL(5,2) AS $$
DECLARE
  v_score DECIMAL(5,2);
BEGIN
  SELECT
    ROUND(
      COALESCE(AVG(
        CASE
          WHEN status = 'passed' THEN 100
          WHEN status = 'warning' THEN 75
          WHEN status = 'failed' THEN 0
          ELSE 50
        END
      ), 50), 2
    )
  INTO v_score
  FROM quality_results
  WHERE asset_id = p_asset_id
    AND run_at >= now() - interval '7 days';

  RETURN COALESCE(v_score, 0);
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- 9. TRIGGERS
-- ============================================================================

-- Update quality_rules.updated_at on change
CREATE OR REPLACE FUNCTION update_quality_rule_timestamp()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = now();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_quality_rules_updated
BEFORE UPDATE ON quality_rules
FOR EACH ROW
EXECUTE FUNCTION update_quality_rule_timestamp();

-- Update quality_issues.updated_at on change
CREATE TRIGGER trg_quality_issues_updated
BEFORE UPDATE ON quality_issues
FOR EACH ROW
EXECUTE FUNCTION update_quality_rule_timestamp();

-- ============================================================================
-- 10. SEED DATA - Sample Rules
-- ============================================================================

-- We'll add seed data through the application, but here's a template
-- INSERT INTO quality_rules (name, description, dimension, severity, rule_type, rule_config, enabled)
-- VALUES
-- ('Null Check Example', 'Ensures critical columns are not null', 'completeness', 'high', 'threshold',
--  '{"column": "email", "metric": "null_rate", "operator": "<", "value": 0.01}'::jsonb, true);

-- ============================================================================
-- MIGRATION COMPLETE
-- ============================================================================
